[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software for 432",
    "section": "",
    "text": "The course makes heavy use of the R statistical programming language, and several related tools, most especially the RStudio development environment. Every bit of this software is free to use, and open-source.\nYou will need access to a computer to do your work for this class, not just an iPad or other tablet, but an actual computer. You do not need a state of the art machine, nor should you need any special hardware to run things for this course."
  },
  {
    "objectID": "software.html#updating-your-r-packages",
    "href": "software.html#updating-your-r-packages",
    "title": "Software for 432",
    "section": "Updating Your R Packages",
    "text": "Updating Your R Packages\nAbout twice a month, it’s a good idea to update your R packages. To do so, follow steps 3 and 4 above."
  },
  {
    "objectID": "software.html#the-meta-packages",
    "href": "software.html#the-meta-packages",
    "title": "Software for 432",
    "section": "The Meta-Packages",
    "text": "The Meta-Packages\n\nInstalling the tidyverse meta-package installs the packages listed at https://www.tidyverse.org/.\nInstalling the easystats meta-package installs the packages in the easystats framework.\nInstalling the tidymodels meta-package installs the packages listed at https://www.tidymodels.org/packages/."
  },
  {
    "objectID": "software.html#note-a-windows-issue-with-rtools",
    "href": "software.html#note-a-windows-issue-with-rtools",
    "title": "Software for 432",
    "section": "Note: A Windows Issue with RTools",
    "text": "Note: A Windows Issue with RTools\nIf you are using Windows, and get messages during installation that the latest version of RTools needs to be installed, you can usually just ignore them. If you don’t want to ignore them, go here to download and install RTools for Windows."
  },
  {
    "objectID": "software.html#installing-a-single-package",
    "href": "software.html#installing-a-single-package",
    "title": "Software for 432",
    "section": "Installing a Single Package",
    "text": "Installing a Single Package\nIf you want to install a single package, you can do so by finding the word Packages on the right side of your RStudio screen.\n\nClick on the Packages tab to start installing the packages you’ll need.\nClick Install, which will bring up a dialog box, where you can type in the names of the packages that you need. These should be separated by a space or comma. Be sure to leave the Install dependencies box checked.\n\n\nA popup box may appear, asking “Do you want to install from sources the packages which need compilation?” to which I usually answer No. A Yes response leads to a slower installation, but can solve problems if you still have them after updating.\nThis may take a few minutes. As long as you’re seeing activity in the Console window, things are progressing.\nEventually, you’ll get a message that “The downloaded source packages are in …” with a directory name. That’s the sign that the updating is done."
  },
  {
    "objectID": "quiz1.html",
    "href": "quiz1.html",
    "title": "Quiz 1",
    "section": "",
    "text": "Quiz 1 covers materials from Classes 1-14 of the course, as well as Jeff Leek’s How to be a modern scientist, and other sources assigned during classes 1-14.\nAll Quiz 1 elements are now available.\nQuiz 1 is due on Wednesday 2025-03-05 at Noon."
  },
  {
    "objectID": "quiz1.html#how-do-i-get-help",
    "href": "quiz1.html#how-do-i-get-help",
    "title": "Quiz 1",
    "section": "How do I get help?",
    "text": "How do I get help?\nThis is an open book, open notes quiz. You are welcome to consult the materials provided on the course website and that we’ve been reading in the class, but you are not allowed to discuss the questions on this quiz with anyone other than Professor Love and the teaching assistants. You will complete a short affirmation that you have obeyed these rules as part of submitting the Quiz.\nIf you need clarification on a Quiz question, you have exactly two ways of getting help:\n\nYou can ask your question via email to Thomas dot Love at case dot edu, or\nYou can ask your question of Dr. Love directly in class 15 on 2025-03-04.\n\nDuring the Quiz period (2024-02-27 through 2024-03-05) we will not answer questions about Quiz 1 except through the email and class session above.\n\nPlease check your email and the Quiz 1 links page before submitting the final version of your Quiz, to see if Dr. Love has posted any changes there.\nDr. Love promises to respond to all questions received before 9 AM on 2024-03-05 (three hours before the Quiz is due) in a timely fashion."
  },
  {
    "objectID": "projA.html",
    "href": "projA.html",
    "title": "Project A",
    "section": "",
    "text": "In Project A, you will be analyzing, presenting and discussing a pair of regression models, specifically a linear regression and a logistic regression, describing a data set (available to the public) that you identify.\nThere are two main deliverables:\n\nThe Project A Plan, due when the Calendar says it is.\nThe Project A Portfolio and Presentation, due when the Calendar indicates.\n\nAll Project A work is submitted via Canvas.\n\n\nIf you decide to use some sort of AI to help you with any part of the Project, we ask that you place a note to that effect, describing what you used and how you used it, as a separate section called “Use of AI”. This should appear just before your section containing the Session Information. Thank you.\n\n\n\nOn our 432-data page, you will find a pair of Quarto templates:\n\nthis one is for the Project A Plan, and\nthis one is for the Project A Portfolio.\n\nPlease use these templates in preparing your work. They will make completing and grading Project A much easier.\n\n\n\nYou can choose either to work alone, or with one other person, to complete Project A. If you work in a group for Project A, you may be asked to work alone for Project B later in the term.\n\nYou will need to identify your Project A partner prior to the submission of your Project A Plan.\nIf you are working with a partner, all work must be submitted by exactly one of you to Canvas while the non-reporting partner submits a one-page note to Canvas indicating the members of the partnership and that the partner will submit the work.\n\n\n\n\nSanity checks are an important part of your programming, but they don’t belong in your final plan, portfolio or presentation. Neither do false starts, and explorations that don’t lead anywhere.\n\n\n\nA demonstration Project A, built by Professor Love, is available here, where you can view the HTML and download the Quarto code.\n\nProfessor Love has posted the .qmd, .html and data files for the Project A demonstration project to our Shared Google Drive.\nThe Demonstration for Project A shows the minimum requirements for a low B grade on the project. The main things that are missing in the Demonstration are careful interpretations and explanations of some of the ideas, results and code. You need to include those pieces in order to move from a low B to some sort of A grade.\nIn addition, the Demonstration does not include the optional extra sections 8.9 and 9.7 described below."
  },
  {
    "objectID": "projA.html#use-of-ai",
    "href": "projA.html#use-of-ai",
    "title": "Project A",
    "section": "",
    "text": "If you decide to use some sort of AI to help you with any part of the Project, we ask that you place a note to that effect, describing what you used and how you used it, as a separate section called “Use of AI”. This should appear just before your section containing the Session Information. Thank you."
  },
  {
    "objectID": "projA.html#templates",
    "href": "projA.html#templates",
    "title": "Project A",
    "section": "",
    "text": "On our 432-data page, you will find a pair of Quarto templates:\n\nthis one is for the Project A Plan, and\nthis one is for the Project A Portfolio.\n\nPlease use these templates in preparing your work. They will make completing and grading Project A much easier."
  },
  {
    "objectID": "projA.html#working-with-a-partner",
    "href": "projA.html#working-with-a-partner",
    "title": "Project A",
    "section": "",
    "text": "You can choose either to work alone, or with one other person, to complete Project A. If you work in a group for Project A, you may be asked to work alone for Project B later in the term.\n\nYou will need to identify your Project A partner prior to the submission of your Project A Plan.\nIf you are working with a partner, all work must be submitted by exactly one of you to Canvas while the non-reporting partner submits a one-page note to Canvas indicating the members of the partnership and that the partner will submit the work."
  },
  {
    "objectID": "projA.html#sanity-checks-and-false-starts",
    "href": "projA.html#sanity-checks-and-false-starts",
    "title": "Project A",
    "section": "",
    "text": "Sanity checks are an important part of your programming, but they don’t belong in your final plan, portfolio or presentation. Neither do false starts, and explorations that don’t lead anywhere."
  },
  {
    "objectID": "projA.html#demonstration-project",
    "href": "projA.html#demonstration-project",
    "title": "Project A",
    "section": "",
    "text": "A demonstration Project A, built by Professor Love, is available here, where you can view the HTML and download the Quarto code.\n\nProfessor Love has posted the .qmd, .html and data files for the Project A demonstration project to our Shared Google Drive.\nThe Demonstration for Project A shows the minimum requirements for a low B grade on the project. The main things that are missing in the Demonstration are careful interpretations and explanations of some of the ideas, results and code. You need to include those pieces in order to move from a low B to some sort of A grade.\nIn addition, the Demonstration does not include the optional extra sections 8.9 and 9.7 described below."
  },
  {
    "objectID": "projA.html#what-makes-a-data-set-acceptable",
    "href": "projA.html#what-makes-a-data-set-acceptable",
    "title": "Project A",
    "section": "What Makes a Data Set Acceptable?",
    "text": "What Makes a Data Set Acceptable?\n\nShared with the World. The data must be available to you, and shared with me and everyone else in the world (without any identifying information) as a well-tidied file by the time you submit your Project A Plan. If the data is from another source, the source (web or other) must be completely identified to me. Ongoing projects that require anyone’s approval to share data are not appropriate for Project A.\n\nYou should have the data in R by February 1, so that you will have sufficient time to complete the other elements of this Plan. Any data you cannot have by that time is a bad choice.\nFor Project A, you may not use any data set used in the 431 or 432 teaching materials.\nFor Project A, do not use data from NHANES or from County Health Rankings.\n\nYou will need to use meaningfully different data sets in 432 Projects A and B.\nIn submitting your Project A Plan, you will need to be able to write “I am certain that it is completely appropriate for these data to be shared with anyone, without any conditions. There are no concerns about privacy or security.” So be sure that’s true before you pick a data set.\n\nSize.\n\nA minimum of 150 complete observations are required on each variable. It is fine if there are some missing values, as well, so long as there are at least 150 rows with complete observations on all variables you intend to use in each model.\nThe maximum data set size is 2000 observations, so if you have something larger than that, you’ll need to select a random subset of the available information as part of your data tidying process.\n\nOutcomes. The columns must include one quantitative outcome and one binary categorical outcome.\n\nWe prefer distinct outcomes, but if necessary, the binary outcome can be generated from the quantitative outcome (as an example, your quantitative outcome could be resting heart rate in beats per minute, and your binary outcome could be whether the resting heart rate is below 70 beats per minute.)\n\nInputs. You will need at least four regression inputs (predictors) for each of your two models.\n\nAt least one of the four must be quantitative (a variable is not quantitative for this purpose unless it has more than 10 different, ordered, observed values), and at least one must be multi-categorical (with between 3 and 6 categories, each containing a minimum of 30 subjects) for each model.\nYour other inputs can represent binary, multi-categorical or quantitative data.\nYou can examine different candidate predictors for each outcome, or use the same ones in both your linear and logistic regression models.\nIf you are considering a predictor for either your linear or logistic regression model which has 20% or more missing values among the observations where you have complete data on the relevant outcome, then either (a) look elsewhere for a more informative predictor, or (b) change your sampling strategy to require complete cases on that variable, as well, if possible.\n\nDepending on your sample size, you can study more than the minimum number of regression inputs. See specifications below for your linear and logistic models.\n\n\nYour data need not be related to health, or medicine, or biology.\n\nNo hierarchical data\nIn each project this semester, we will require you to study cross-sectional data, where rows indicate subjects and columns indicate variables gathered to describe those subjects at a particular moment in time or space. Do not use “nested” data in Project A.\n\nOne example of hierarchical (nested) data would be a model of patient results that contains measures not just for individual patients but also measures for the providers within which patients are grouped, and also for health systems in which providers are grouped. That wouldn’t work for this project.\nAnother example of hierarchical (nested) data would be a model of individual people’s outcomes where the covariates are gathered at the state or county level, as well as at the level of individuals, and again, that doesn’t work for this project.\nThe singular exception to the “no hierarchical data” rule is that it will usually be acceptable for all inputs to be collected at a single (baseline) time point and both outcomes to be collected at a single future point in time. For example, you could predict systolic blood pressure in 2022 (or whether or not a subject’s systolic blood pressure in 2022 was below 140), based on a set of input variables (likely including systolic blood pressure) all gathered in 2021.\n\n\n\nJoining multiple data sets\nDr. Love will be pleased with a data collection effort that appropriately puts together at least two different data bases, should that be appropriate.\nWhat we have in mind are the following scenarios:\n\nMultiple data sets describing different variables for the same subjects that can be linked, so that you can build a combined data set with the same subjects but pulling together multiple sources of data, as, for example, the County Health Rankings do each year.\nMultiple data sets describing different subjects but the same variables, such as different years of a survey, combining, for instance, multiple iterations of NHANES so as to increase the available sample size.\n\nIf your project research questions and available data lead to one of these approaches, great. If not, don’t force it.\nThis may require you to learn something about the various joining commands, like left_join and inner_join that are highlighted in the Combine Data Sets section on the Data Transformation Cheat Sheet from Posit.\n\nWe heartily recommend Garret Grolemund’s YouTube materials on Data Wrangling, for instance this Introduction to Data Manipulation which is about combining multiple data sets.\nAnother great resource for combining data sets (and most of your other R questions) is the second edition of R for Data Science, by Wickham and Grolemund."
  },
  {
    "objectID": "projA.html#good-data-sets-to-use",
    "href": "projA.html#good-data-sets-to-use",
    "title": "Project A",
    "section": "Good Data Sets To Use",
    "text": "Good Data Sets To Use\nSome sources of data we’d like to see people use include:\n\nCDC WONDER data, which could (at the county level) be combined with data from County Health Rankings 2022 to do something interesting.\nThe data sets described in the American Statistical Association’s Data Challenge Expo for 2022, which include five very interesting data sets selected from the Urban Institute Data Catalog\nA data set from the Tidy Tuesday archive or from the Data is Plural archive might be a good candidate.\nThe Health and Retirement Study\nThe General Social Survey although the problem there is a lack of quantitative variables.\nThe many many public use data sets available at ICSPR\nThe 500 Cities and PLACES data portal, most probably focusing on the County-level data.\nNational Center on Health Statistics which includes NHANES (not a good choice for this project) but also other data sets.\nBehavioral Risk Factor Surveillance System\n\n\nFor examples of using public microdata from surveys, I recommend the book “Analyze Survey Data for Free” which has, for example, information on MEPS, the Health and Retirement Study, the General Social Survey, NHANES, the National Immunization Survey and many others.\nWhile data on COVID-19 would be permitted for 432 projects, most of the available data is longitudinal and thus unsuitable for Project A.\nWe are not interested in people using NHANES data in Project A, or County Health Rankings data, unless (as indicated above) those County Health Rankings are combined with meaningful additional data sets.\nKaggle Public Datasets may be permitted, but we discourage this. We will only accept those with really useful variables, no hierarchical structure and strong descriptions of how the data were gathered (which is at best 5% of what’s available on Kaggle). Don’t select a Kaggle data set without running it by us on Campuswire (see below) to see if we’re willing to let you use it.\nYou will not be permitted to use data from a textbook or other educational resource for learning statistics, data science or related methods (online or otherwise).\n\nExamples of such repositories (that are sadly out-of-bounds for this project and for 432 Project B) include the Cleveland Clinic Statistical Education dataset repository, the Vanderbilt University Biostatistics Datasets, or the UCI Machine Learning Repository.\n\nIt’s not a great idea to type “regression data sets” into Google - rarely does that lead to an interesting project."
  },
  {
    "objectID": "projA.html#running-a-data-set-past-us-for-project-a",
    "href": "projA.html#running-a-data-set-past-us-for-project-a",
    "title": "Project A",
    "section": "Running a Data Set Past Us for Project A",
    "text": "Running a Data Set Past Us for Project A\nTo get Professor Love and the TAs to “sign off” on a data set as appropriate for your Project A Plan, you need to tell us the following four things in a (private or public - your choice) note on Campuswire in the Project A folder. Please do this if you’re not sure your data set is appropriate.\n\nthe data source, as described here, along with a URL so we can access the data\na description of who the subjects are and how they were selected, as described here - it helps if you also tell us how many subjects are in the data.\nwhat quantitative outcome you plan to use in your linear regression model, including its units of measurement and the minimum, mean and maximum values available in your data\nwhat binary outcome you plan to use in your logistic regression model, specifying both of the mutually exclusive and collectively exhaustive categories and how many subjects fall into each of those two categories.\n\nAlso, we ask that you not ask us to pick between two “options” - submit the one you’d rather do. If it’s a problem, we’ll let you know, and you can then change to another option if necessary."
  },
  {
    "objectID": "projA.html#project-a-plan-contents",
    "href": "projA.html#project-a-plan-contents",
    "title": "Project A",
    "section": "Project A Plan Contents",
    "text": "Project A Plan Contents\nPlease use the Quarto template we provided for the Project A Plan in preparing your work. The list of HTML “themes” that are available in Quarto by changing the “theme” option in the start of your document can be found here and we encourage you to pick something you think looks nice.\n\nTitle and Authors\nYour project should have a meaningful title (not containing the words “432” or “Project” or “Proposal” or “Plan”) but rather something describing your actual data and plans.\nPlease keep the main title to no more than 80 characters, including spaces. You can add a subtitle if you like, but the main title should stand on its own. Feel free to focus on one of your two research questions (rather than both) if that’s what’s needed to keep to the 80-character limit.\n\n\nR Packages and Setup\nYou’ll load necessary packages at the start in an unnumbered section of your work, following the template.\n\nDo not source in an R script or package unless you actually need something it provides.\nDo not load core elements of tidyverse or easystats separately. instead just load the meta-packages, and do so last.\nUse #| message: false as part of your Quarto code chunk where the packages are listed (as we have done in the template) to eliminate HTML messages about when packages were built or how objects were masked.\n\n\n\n1. Data Source\nProvide complete information on the source of the data: how did you get it, how was it gathered, by whom, in what setting, for what purpose, and using what sampling strategy.\nThis small section should include a clear link (with all necessary details) to the URL which we can use to obtain the raw data freely.\n\n\n2. The Subjects\nA description (one or two sentences should be sufficient) of who or what the subjects (rows) are in your data set, and how they were sampled from the population of interest.\n\n\n3. Loading and Tidying the Data\n\n3.1 Loading the Raw Data\nProvide code to ingest the raw data. Ideally, this should use tidyverse-friendly code and a direct link to the URL where the data are housed online.\n\n\n3.2 Cleaning the Data (involves several subsections)\nTidy and clean up the data to meet all necessary requirements for your modeling work. This will require multiple sub-sections as you tackle different tasks for different sets of variables. Use the tidyverse for data management whenever possible. Some of things you need to do here…\n\nEliminate all variables that are not going to be used (either as identifiers, outcomes or inputs) in your planned analyses.\nChange variable names to more meaningful ones, although it’s helpful to keep them at 10 characters or less. Use clean_names() from the janitor package to clean up and standardize the presentation of variable names.\nI want to discourage you from using data set names, variable names and especially category level names that are long (more than 8-10 characters) if you can avoid it. You want to be clear, certainly, but long names are (a) harder to type and (b) harder to see in plots and tables.\n\nMore than 8 characters in a category level’s name will make a lot of plotting very irritating down the line, especially in something like a nomogram or prediction plot.\nDon’t use spaces in the names of variables or the names of categories - separate words with underscores.\n\nSample the data as needed to ensure that you meet the data set size specifications (no more than 2000 rows, for instance.)\nConvert all variables to appropriate types (factors, etc.) as needed, and complete appropriate checks of the values for all variables.\n\nNever use 1 and 2 as the levels of a binary variable, like sex = 1 for M and 2 for F, or anything like that. Always use 1 and 0, or actual names like “M” and “F” as the levels.\nBe sure that if you have a multi-categorical variable with a natural order of levels (like Low, Medium, High, or Excellent, Very Good, Good, Fair, Poor or Strongly agree, Agree, Neither Agree nor Disagree, Disagree, Strongly Disagree) be sure that the data_codebook() results you show in Section 5 show that order. If you need to fix this, the place to fix it is here in Section 3.\n\nIf you have prospective inputs (predictors) that are multi-categorical with more than 6 categories, collapse them to six or fewer categories in a rational way at this stage.\nEnsure that all categorical variables have at least 30 observations at each level, collapsing or removing levels as needed to accomplish this end.\nIf you are using a cutpoint to split a quantitative measure into categories, be sure to include in the variable description part of Section 5 exactly what that cutoff (or set of cutoffs’) value was (for example, “values above the mean” isn’t sufficient, “values above 45.67 (the mean of the data)” is a sufficient response.)\nYour tidied data set should be arranged with a row (subject) identifier at the far left. That identifier should have a unique value for each row, and should be saved as a character variable in R.\nWe expect your final tibble to have some missing values. Do not impute or delete these, but do be sure they are correctly identified as missing values by R.\nDo not list the entire tibble or print out large strands of R output (like summaries of the entire tibble) anywhere in your document, except where you are required by these instructions to do so.\nZap away any variable labels in section 3 using zap_label() from the haven package or a similar alternative. I’m not a fan of labels in R data sets for variables, as they make the results of many plots, tables and things like data_codebook() much harder to read.\n\n\n\n\n4. The Tidy Tibble\n\n4.1 Listing the Tibble\nIn this section, you should start by listing the tibble you created in Section 3, with all variables correctly imported (via your code) as the types of variables (factor/integer/numeric, etc.) that you need for modeling.\n\nThis should be a listing, not a glimpse or anything else. Just type in the name of your tibble.\nThe resulting list should be limited to the first 10 rows of your data.\n\n\n\n4.2 Size and Identifiers\nWrite a sentence specifying the number of rows and the number of columns in your tibble, and this should match the R output.\nThen identify the name of the variable that provides the identifier for each row in your tidy tibble, and demonstrate that each row has a unique value for that variable, and that the variable is represented as a character in R.\n\nOne way to do this is to run the n_distinct function from the dplyr package on this particular variable.\nDo not present summary or descriptive results on every variable in the whole tibble here - you’ll do that in Section 5.\n\n\n\n4.3 Save the tidied Tibble as an .Rds file\nNow, save the tidied data set as an .Rds file (using write_rds or the equivalent), which you’ll need to submit to us. The tibble should have the same name as the data file you submit to Canvas.\n\n\n\n5. The Code Book\n\n5.1 Defining the Variables\nIn this section, provide a beautiful codebook which tells us (at a minimum) the following information for each variable in the tibble you printed and saved in Section 4.\n\nThe name of the variable in your tibble.\nThe role of the variable in your planned analyses (options include identifier, outcome, or input)\nThe type of variable for each outcome or input (options are categorical, in which case tell us how many categories, or quantitative)\nA short description of the meaning of the variable.\n\nThis should include the units of measurement if the variable is quantitative, and a list of the possible values if the variable is categorical.\n\n\nAll variables in your tidy data set, and in your codebook in Section 5 should fall into one of four roles. Don’t include other things in your tidy data set. - identifiers: variables that identify the subjects in your data, and these should be labeled as Identifier in the variable descriptions part of Section 5. - outcomes for either of your models, and these should be labeled as Outcome (linear) or Outcome (logistic) in the variable descriptions part of Section 5. - predictors for either of your models, and these should be labeled as either Predictor or Input in the variable descriptions part of Section 5. - other variables that you needed to use to create something in the previous three groups. These should not be labeled as predictors or outcomes in Section 5.\nAs an example, here’s a part of a simple table:\n\n\n\n\n\n\n\n\n\nVariable\nRole\nType\nDescription\n\n\n\n\nsubjectID\nidentifier\n-\ncharacter code for subjects\n\n\nsysbp\noutcome (linear)\nquant\nMost Recent Systolic Blood Pressure, in mm Hg\n\n\nstatin\ninput\n2-cat\nHas a current statin prescription? (Yes or No)\n\n\n\n\n\n5.2 Numerical Description\nHere, run the data_codebook() command on your entire tibble. Be sure that the results match up with what you’ve described in defining the variables, and that the same variables appear, in the same order, in the codebook and in these results.\nYou are welcome to include other summaries as well, but data_codebook() is required.\n\n\n\n6. Linear Regression Plans\n\n6.1 My First Research Question\nBegin this section by specifying a question that you hope to answer with the linear model you are proposing. A research question relates clearly to the data you have and your modeling plans, and, like all questions, it ends with a question mark. Eventually, you will need to answer this question in your portfolio.\nJeff Leek in his book “How to be a Modern Scientist” has some excellent advice in his section on “How Do You Start a Paper.” In particular, you want to identify research questions that:\n\nare concrete, (and for which you can find useful data), and that\nsolve a real problem, and that\ngive you an opportunity to do something new,\nthat you will feel ownership of, and\nthat you want to work on.\n\nWe recommend you use the FINER criteria (or, if relevant, the PICOT criteria) to help you refine a vague notion of a research question into something appropriate for this project.\n\nFINER = Feasible, Interesting, Novel, Ethical and Relevant.\nPICOT is often used in medical studies assessing or evaluating patients and includes Patient (or Problem), Intervention (or Indicator), Comparison group, Outcomes and Time.\n\nThe Wikipedia entry on research questions provides some helpful links to these ideas.\n\n\n6.2 My Quantitative Outcome\n\nIf necessary, this section should begin by filtering the data to the observations with complete data on the quantitative outcome (for the linear model.) This might be necessary if some of the rows in your tibble have complete data on one outcome (binary) but not the other (quantitative). Obviously, if your data are already complete on this outcome, there’s no need to re-filter.\n\nThis subsection tells us what you will use your linear regression model to explain or predict.\n\nTell us the name in the tibble of the linear regression outcome you will use (this should the quantitative outcome you identified in your Codebook) and state why you are interested in predicting this variable.\nProvide a count of the number of rows in your data with complete information on this outcome.\nProvide a nicely labeled graphical summary of the distribution of your outcome to supplement the numerical description you provided in Section 5.2.\nComment briefly on the characteristics of the outcome’s distribution. Is your outcome skewed or symmetric, is it discrete or fairly continuous, is there a natural transformation to consider?\nDemonstrate that the variable you have selected meets the standard for a quantitative variable used in this Project, specifically that it has more than 10 different, ordered, observed values.\nAs part of section 6.2, I want to see the following three things for your linear outcome, in each case, restricting the data to the observations with complete data on that outcome.\n\nplots - at least a histogram and Normal Q-Q plot of the linear outcome, built using ggplot2 and patchwork.\nnumerical summaries - the results of both describe() (from the Hmisc package) and favstats() (from the mosaic package) for the linear outcome.\na tabyl() (from the janitor package) of the most common values of your outcome, along with the fraction of all cases with complete data on that outcome that have that particular value, so that you can verify that no value occurs in more than 10% of your complete observations.\n\n\n\n\n6.3 My Planned Predictors (Linear Model)\nNow, tell us precisely which four (or more) candidate predictors (inputs) you intend to use for your linear regression model.\n\nPlease use the variable names that appear in your code book and tibble.\nDemonstrate to us that you have at least one input which is quantitative, specifically that it has more than 10 different, ordered, observed values.\nDemonstrate to us that you have at least one categorical input which has between 3 and 6 categories, that will be used as a factor in your modeling, and that has at least 30 observations in each level of the factor. If necessary, you can create such a predictor from a quantitative one, but if you are doing this, remember that only the multi-categorical version of the predictor should be included in your models.\nDemonstrate that the total number of candidate predictors you suggest is no more than \\(4 + (N_1 - 100)/100\\), rounding down, where \\(N_1\\) is the number of rows with complete outcome data in your tibble.\n\nIn section 6.3.1, you should briefly specify your guesses as to the expected direction of relationships between your outcome and your predictors. Use the word association instead of correlation, basically always, unless you are referring specifically to a correlation coefficient.\nIn section 6.3.2, I then want to see a missingness summary including miss_var_summary() and miss_case_table() (from the naniar package) across all variables in the codebook that play a role in your planned linear regression model after filtering to the cases with complete data on your linear outcome. I’m hoping that you’ll have complete data for all predictors on more than 60% of your observations, and that you won’t be missing more than 20% of any individual predictor.\n\n\n\n7 Logistic Regression Plans\n\n7.1 My Second Research Question\nBegin this section by specifying a question that you hope to answer with the logistic model you are proposing. A research question relates clearly to the data you have and your modeling plans, and, like all questions, it ends with a question mark. Eventually, you will need to answer this question in your portfolio.\nSee section 6.1 for some more suggestions about improving your research questions.\n\n\n7.2 My Binary Outcome\nThis subsection should begin by filtering the data to the observations with complete data on the binary outcome (for the logistic model.) This might be necessary if some of the rows in your tibble have complete data on one outcome (quantitative) but not the other (binary). Obviously, if your data are already complete on this outcome, there’s no need to re-filter.\nThis subsection tells us what you will use your logistic regression model to explain or predict.\n\nTell us the name in the tibble of the logistic regression outcome you will use (this should be the binary (2-category) outcome you identified in your codebook) and state why you are interested in predicting this variable.\nIf your logistic regression outcome cannot be expressed in the form of a yes/no question, coded as 1 = yes and 0 = no, and if the name of that variable doesn’t tell us what 1 means, then adjust your setup accordingly until this is true.\n\nFor example, don’t use “Active / Inactive” for a status variable, instead use active = 1 or 0 for the same information.\nThis is because if you use a factor in R for your outcome, the logistic regression model will not necessarily choose the result (Yes instead of No, 1 instead of 0) that you’re looking for unless you actually use 0 and 1 or No and Yes for the levels, and 0 and 1 have fewer characters.\nIf your outcome was “High / Low” it will choose Low because it is the second one alphabetically!\nAlso, if you use 1 and 0 as your levels, the prediction process I have described in slide set 8 and in the support1000 example will work, every time. If you do something else, it might not.\n\nAs part of section 7.2, I want to see a a tabyl() (again from the janitor package) of the values of your binary outcome, after restricting the data to the observations with complete data on your primary outcome. This will provide a count of the number of rows in your data with each of the two possible values of this outcome.\n\n\n\n7.3 My Planned Predictors (Logistic Model)\nNow, tell us precisely which four (or more) candidate predictors you intend to use for your logistic regression model.\n\nIf you are using some of the same predictors as in your linear regression model, there’s no need to repeat yourself. Simply tell us which variables you’ll use again, and then provide descriptions for any new predictors that did not appear in your plans for the linear model.\nDemonstrate that the total number of candidate predictors you suggest for your logistic regression model is no more than \\(4 + (N_2 - 100)/100\\) predictors, rounded down, where \\(N_2\\) is the number of subjects in the smaller of your two outcome groups.\n\nIn section 7.3.1, you should briefly specify your guesses as to the expected direction of relationships between your outcome and your predictors. Use the word association instead of correlation, basically always, unless you are referring specifically to a correlation coefficient.\nIn a new section 7.3.2, I want to see a missingness summary including miss_var_summary() and miss_case_table() (from the naniar package) across all variables in the codebook that play a role in your planned logistic regression model after filtering to the cases with complete data on your binary outcome. I’m hoping that you’ll have complete data for all predictors on more than 60% of your observations, and that you won’t be missing more than 20% of any individual predictor.\n\n\n\n8 Affirmation\nNext you need to affirm that the data set meets all of the project requirements, especially that the data can be shared freely over the internet, and that there is no protected information of any kind involved.\nThe text we want to see here is\n\nI am certain that it is completely appropriate for these data to be shared with anyone, without any conditions. There are no concerns about privacy or security.\n\nIf you are unsure as to whether this is true, select a different data set.\n\n\n9 References\nReferences (you’ll need one to describe the source of your data, at least) go here.\n\n\n10 Session Information\nPlease provide session information by running xfun::session_info().\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "projA.html#my-best-piece-of-advice",
    "href": "projA.html#my-best-piece-of-advice",
    "title": "Project A",
    "section": "My Best Piece of Advice",
    "text": "My Best Piece of Advice\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look at the resulting HTML output, closely."
  },
  {
    "objectID": "projA.html#most-important-piece-of-advice",
    "href": "projA.html#most-important-piece-of-advice",
    "title": "Project A",
    "section": "Most Important Piece of Advice",
    "text": "Most Important Piece of Advice\nAs mentioned above, it is crucial to review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look at the resulting HTML output, closely."
  },
  {
    "objectID": "projA.html#your-audience",
    "href": "projA.html#your-audience",
    "title": "Project A",
    "section": "Your Audience",
    "text": "Your Audience\nYour audience for this presentation includes Professor Love, the TAs and your fellow students. Prepare your presentation with that audience in mind. What will they need to know to understand what you’ve done, and get excited about it?"
  },
  {
    "objectID": "projA.html#outline-of-the-presentation",
    "href": "projA.html#outline-of-the-presentation",
    "title": "Project A",
    "section": "Outline of the Presentation",
    "text": "Outline of the Presentation\nYour presentation should include fewer than 10 slides, since you only have four minutes.\nYour “most important finding” is just going to be one of many potentially interesting findings in your Project. Your job in the presentation is not to prove to me that you did a lot of work - I’ll see that in the portfolio.\nInstead, your job in the presentation is to interest your audience in something you found that is (at least relatively) important. You are expected to help us understand the following things related to your most important finding, based on either a linear or logistic model.\nYou will not be developing any new material for the slides (just restating and rearranging things you’ve already done and perhaps constructing a short narrative to help us retain your key findings) once you have the portfolio. As a result, we encourage you to complete the portfolio first.\nWe suggest you develop about 8 slides. This should include…\n\nA title slide\nA couple of slides to describe the Subjects, Outcome and Predictors\n\nMake sure we understand who the subjects are, how they were selected, what the outcome is and why we should care about it, and what predictors are involved in the model you’ll show.\n\nSeveral slides showing meaningful statistical findings (What should we learn from your model?)\n\nWhat does the model (don’t show us details of multiple models in the presentation) say about the relationship between the outcome and the predictors?\nYou’re only showing us one model (of models A, B, Y and Z) in the presentation.\nHow well does this model fit the data you have, and how well might it fit in new data?\n\nA couple of slides discussing next steps\n\nIt is unlikely that you’ll have a model that is truly satisfactory all on its own, so what could be done to improve it that you cannot already do with the data you have? What other data could be collected, how could the measures be refined, could you design a study to get to a more convincing answer?\n\n\nMake sure that you introduce yourself when you start to speak, over your title slide if you are working alone. We’re happy to see your face during the presentation, but this isn’t mandatory. If you are working with a partner, each of you should introduce yourself at the beginning, and let me know who’s speaking first.\n(Essentially) every word and every image/table/chart in your slides can and should come directly from the materials contained in your HTML portfolio.\n\nThe development of the presentation is mostly about selecting useful information to present and then arranging it in a way that sticks for your audience.\nYour presentation should include no R code but instead will provide nicely formatted figures and tables along with text. Some figures don’t work well on slides, like nomograms, without a lot of work. Pick something that is both useful and easy to see.\nEach slide should have a title, indicating the message you want us to get from the slide (don’t use generic titles like “Results” or “Table 1”).\nYou’ll have to cut out around 95% of your portfolio to create your slides, and you should follow your instincts regarding your audience (Professor Love, the TAs and your fellow students are your audience.)\nDeveloping the presentation is where you have to make decisions about what’s most important to show an audience to get them interested in your work. That’s a critically important skill."
  },
  {
    "objectID": "projA.html#using-splines-and-other-complex-predictors",
    "href": "projA.html#using-splines-and-other-complex-predictors",
    "title": "Project A",
    "section": "Using Splines and other Complex Predictors",
    "text": "Using Splines and other Complex Predictors\n\nHow should I describe a restricted cubic spline that I’ve fit in a model? Do I write out that equation with the variablename’ and variablename’’ in it, or … ?\n\nTell us how many knots were involved and show a graph that depicts the impact of the spline.\n\nNo one explains splines without a graph.\n\nMake a graph and use it is excellent advice for many aspects of your presentation. Sensible graphs to accomplish this task in a multivariate regression model include the ggplot with Predict combination, the plot(summary()) approach, and/or a nomogram."
  },
  {
    "objectID": "projA.html#variable-selection",
    "href": "projA.html#variable-selection",
    "title": "Project A",
    "section": "Variable Selection",
    "text": "Variable Selection\n\nWhat is the best way to select an appropriate set of predictors?\n\nIt depends in part on the kind of question you are trying to answer. For most projects, I recommend a question that is explicitly about prediction, rather than either (a) trying to explain a phenomenon in existing data without reference to external prediction or (b) trying to make some sort of causal inference, which requires methods beyond the scope of this class.\nWhat I would always try to do is start with a question I want to answer, which should motivate specific predictors. A combination of logic, theory and prior empirical evidence is always preferable. A scan of the literature is always useful. A conceptual model of the relationship which makes predictions about what “should” happen under current understanding can be very helpful. I strongly urge you to pick a project where you have some prior understanding of how the data will behave and where you can express that pre-modeling belief as part of your presentation of your work.\nWhat I would definitely not do is scan a list of correlations in the current data to see which ones look promising, and then forget that I did that when it came time to evaluate the models I developed. It’s fine to go on a fishing expedition here, but then you have to severely temper your claims, and in particular you have to give up on drawing any substantial conclusions about causation or explanation and focus instead on a question about prediction, and (of course) validation of your model becomes essential."
  },
  {
    "objectID": "nhanes_codebook.html",
    "href": "nhanes_codebook.html",
    "title": "NHANES Codebook",
    "section": "",
    "text": "library(janitor)\nlibrary(naniar)\nlibrary(easystats)\nlibrary(tidyverse)\n\n\nnh_1500 &lt;- read_rds(\"https://github.com/THOMASELOVE/432-data/raw/refs/heads/master/data/nh_1500.Rds\")\nnh_3143 &lt;- read_rds(\"https://github.com/THOMASELOVE/432-data/raw/refs/heads/master/data/nh_3143.Rds\")"
  },
  {
    "objectID": "nhanes_codebook.html#r-packages-and-setup",
    "href": "nhanes_codebook.html#r-packages-and-setup",
    "title": "NHANES Codebook",
    "section": "",
    "text": "library(janitor)\nlibrary(naniar)\nlibrary(easystats)\nlibrary(tidyverse)\n\n\nnh_1500 &lt;- read_rds(\"https://github.com/THOMASELOVE/432-data/raw/refs/heads/master/data/nh_1500.Rds\")\nnh_3143 &lt;- read_rds(\"https://github.com/THOMASELOVE/432-data/raw/refs/heads/master/data/nh_3143.Rds\")"
  },
  {
    "objectID": "nhanes_codebook.html#the-nh_1500-and-nh_3143-data",
    "href": "nhanes_codebook.html#the-nh_1500-and-nh_3143-data",
    "title": "NHANES Codebook",
    "section": "The nh_1500 and nh_3143 data",
    "text": "The nh_1500 and nh_3143 data\nThese data are drawn from NHANES in the 2013-14, 2015-16 and 2017-18 cycles. I’ve removed observations with missing data, and adjusted the names and some details for the selected variables listed below.\nThe data set I generated from this work yielded 1500 + 3143 = 4643 observations, which I have partitioned into one sample called nh_1500 and another called nh_3143. Each of the 4643 observations appears in exactly one of the two files. Each file contains the following set of 30 variables."
  },
  {
    "objectID": "nhanes_codebook.html#variable-descriptions",
    "href": "nhanes_codebook.html#variable-descriptions",
    "title": "NHANES Codebook",
    "section": "Variable Descriptions",
    "text": "Variable Descriptions\n\n\n\n\n\n\n\n\n\nMy Variable\nNHANES Name\nDescription\nSource\n\n\n\n\nSEQN\nSEQN\nSubject ID code\nall files\n\n\nsex\nRIAGENDR\nsex (Male or Female)\nDEMO\n\n\nage\nRIDAGEYR\nage in years\nDEMO\n\n\nrace_eth\nRIDRETH1\nrace/ethnicity (4 levels)\nDEMO\n\n\neduc\nDMDEDUC2\neducational attainment (4 levels)\nDEMO\n\n\nmarital\nDMDMARTL\nmarital status (4 levels)\nDEMO\n\n\nhh_size\nDMDHHSIZ\nhousehold size (1-6 people)\nDEMO\n\n\ninc_pov\nINDFMPIR\nincome divided by poverty level (capped at 5)\nDEMO\n\n\nwtint2yr\nWTINT2YR\nsampling weight for interview questions\nDEMO\n\n\nwtmec2yr\nWTMEC2YR\nsampling weight for medical examination\nDEMO\n\n\nweight\nBMXWT\nweight in kilograms\nExam: BMX\n\n\nheight\nBMXHT\nstanding height in centimeters\nExam: BMX\n\n\nwaist\nBMXWAIST\nwaist circumference in centimeters\nExam: BMX\n\n\npulse\nBPXPLS\n60-second heart rate (30-sec rate * 2)\nExam: BPX\n\n\nsbp1\nBPXSY1\n(First) Systolic BP, in mm Hg\nExam: BPX\n\n\ndbp1\nBPXDI1\n(First) Diastolic BP, in mm Hg\nExam: BPX\n\n\nsbp3\nBPXSY3\n(Third) Systolic BP, in mm Hg\nExam: BPX\n\n\ndbp3\nBPXDI3\n(Third) Diastolic BP, in mm Hg\nExam: BPX\n\n\nbun\nLBXSBU\nSerum Blood Urea Nitrogen (mg/dl)\nLab: BIOPRO\n\n\nscr\nLBXSCR\nSerum Creatinine (mg/dl)\nLab: BIOPRO\n\n\nglucose\nLBXSGL\nSerum Glucose (mg/dl)\nLab: BIOPRO\n\n\niron\nLBXSIR\nSerum Iron (ug/dl)\nLab: BIOPRO\n\n\nbili\nLBXSTB\nSerum Bilirubin (mg/dl)\nLab: BIOPRO\n\n\nuric\nLBXSUA\nUric acid (mg/dl)\nLab: BIOPRO\n\n\nwbc\nLBXWBCSI\nWhite blood cell count (1000 cells/uL)\nLab: CBC_J\n\n\nrbc\nLBXRBCSI\nRed blood cell count (1000 cells/uL)\nLab: CBC_J\n\n\nplatelet\nLBXPLTSI\nPlatelet count (1000 cells/uL)\nLab: CBC_J\n\n\nhealth\nHSD010\nSelf-reported overall health (E, VG, G, F, P)\nQues.: HSQ\n\n\nsmk100\nSMQ020\nSmoked at least 100 cigarettes in your life? (Yes/No)\nQues.: SMQ\n\n\nlimited\nPFQ051\nLimited in the kind or amount of work you can do by a physical/mental/emotional problem? (Yes/No)\nQues.: PFQ"
  },
  {
    "objectID": "nhanes_codebook.html#numerical-summaries-for-nh_1500",
    "href": "nhanes_codebook.html#numerical-summaries-for-nh_1500",
    "title": "NHANES Codebook",
    "section": "Numerical Summaries for nh_1500",
    "text": "Numerical Summaries for nh_1500\n\ndim(nh_1500)\n\n[1] 1500   30\n\ndata_codebook(nh_1500)\n\nnh_1500 (1500 rows and 30 variables, 30 shown)\n\nID | Name     | Type        | Missings |            Values |            N\n---+----------+-------------+----------+-------------------+-------------\n1  | SEQN     | character   | 0 (0.0%) |            100055 |    1 ( 0.1%)\n   |          |             |          |            100056 |    1 ( 0.1%)\n   |          |             |          |            100105 |    1 ( 0.1%)\n   |          |             |          |            100115 |    1 ( 0.1%)\n   |          |             |          |            100123 |    1 ( 0.1%)\n   |          |             |          |            100137 |    1 ( 0.1%)\n   |          |             |          |            100172 |    1 ( 0.1%)\n   |          |             |          |            100180 |    1 ( 0.1%)\n   |          |             |          |            100190 |    1 ( 0.1%)\n   |          |             |          |            100212 |    1 ( 0.1%)\n   |          |             |          |             (...) |             \n---+----------+-------------+----------+-------------------+-------------\n2  | sex      | categorical | 0 (0.0%) |              Male |  732 (48.8%)\n   |          |             |          |            Female |  768 (51.2%)\n---+----------+-------------+----------+-------------------+-------------\n3  | age      | numeric     | 0 (0.0%) |          [30, 59] |         1500\n---+----------+-------------+----------+-------------------+-------------\n4  | race_eth | categorical | 0 (0.0%) |          NH_White |  645 (43.0%)\n   |          |             |          |          NH_Black |  334 (22.3%)\n   |          |             |          |          NH_Other |  296 (19.7%)\n   |          |             |          |          Hispanic |  225 (15.0%)\n---+----------+-------------+----------+-------------------+-------------\n5  | educ     | categorical | 0 (0.0%) |          Non_Grad |  167 (11.1%)\n   |          |             |          |           HS_Grad |  302 (20.1%)\n   |          |             |          |         Some_Coll |  545 (36.3%)\n   |          |             |          |         Coll_Grad |  486 (32.4%)\n---+----------+-------------+----------+-------------------+-------------\n6  | marital  | categorical | 0 (0.0%) |           Married |  885 (59.0%)\n   |          |             |          |            Former |  274 (18.3%)\n   |          |             |          |             Never |  236 (15.7%)\n   |          |             |          |           Partner |  105 ( 7.0%)\n---+----------+-------------+----------+-------------------+-------------\n7  | hh_size  | numeric     | 0 (0.0%) |            [1, 6] |         1500\n---+----------+-------------+----------+-------------------+-------------\n8  | inc_pov  | numeric     | 0 (0.0%) |         [0.13, 5] |         1500\n---+----------+-------------+----------+-------------------+-------------\n9  | wtint2yr | numeric     | 0 (0.0%) | [5019.48, 407301] |         1500\n---+----------+-------------+----------+-------------------+-------------\n10 | wtmec2yr | numeric     | 0 (0.0%) | [5374.38, 407801] |         1500\n---+----------+-------------+----------+-------------------+-------------\n11 | weight   | numeric     | 0 (0.0%) |       [32.8, 185] |         1500\n---+----------+-------------+----------+-------------------+-------------\n12 | height   | numeric     | 0 (0.0%) |    [141.9, 199.4] |         1500\n---+----------+-------------+----------+-------------------+-------------\n13 | waist    | numeric     | 0 (0.0%) |       [55.5, 165] |         1500\n---+----------+-------------+----------+-------------------+-------------\n14 | pulse    | numeric     | 0 (0.0%) |         [40, 140] |         1500\n---+----------+-------------+----------+-------------------+-------------\n15 | sbp1     | numeric     | 0 (0.0%) |         [86, 218] |         1500\n---+----------+-------------+----------+-------------------+-------------\n16 | dbp1     | numeric     | 0 (0.0%) |         [44, 120] |         1500\n---+----------+-------------+----------+-------------------+-------------\n17 | sbp3     | numeric     | 0 (0.0%) |         [82, 214] |         1500\n---+----------+-------------+----------+-------------------+-------------\n18 | dbp3     | numeric     | 0 (0.0%) |         [26, 114] |         1500\n---+----------+-------------+----------+-------------------+-------------\n19 | bun      | numeric     | 0 (0.0%) |           [3, 73] |         1500\n---+----------+-------------+----------+-------------------+-------------\n20 | scr      | numeric     | 0 (0.0%) |     [0.35, 12.74] |         1500\n---+----------+-------------+----------+-------------------+-------------\n21 | glucose  | numeric     | 0 (0.0%) |         [55, 501] |         1500\n---+----------+-------------+----------+-------------------+-------------\n22 | iron     | numeric     | 0 (0.0%) |          [9, 325] |         1500\n---+----------+-------------+----------+-------------------+-------------\n23 | bili     | numeric     | 0 (0.0%) |          [0, 7.1] |         1500\n---+----------+-------------+----------+-------------------+-------------\n24 | uric     | numeric     | 0 (0.0%) |       [0.8, 12.4] |         1500\n---+----------+-------------+----------+-------------------+-------------\n25 | wbc      | numeric     | 0 (0.0%) |       [2.5, 22.8] |         1500\n---+----------+-------------+----------+-------------------+-------------\n26 | rbc      | numeric     | 0 (0.0%) |      [2.89, 6.75] |         1500\n---+----------+-------------+----------+-------------------+-------------\n27 | platelet | numeric     | 0 (0.0%) |         [54, 583] |         1500\n---+----------+-------------+----------+-------------------+-------------\n28 | health   | categorical | 0 (0.0%) |         Excellent |  138 ( 9.2%)\n   |          |             |          |            V_Good |  404 (26.9%)\n   |          |             |          |              Good |  646 (43.1%)\n   |          |             |          |              Fair |  264 (17.6%)\n   |          |             |          |              Poor |   48 ( 3.2%)\n---+----------+-------------+----------+-------------------+-------------\n29 | smk100   | categorical | 0 (0.0%) |               Yes |  671 (44.7%)\n   |          |             |          |                No |  829 (55.3%)\n---+----------+-------------+----------+-------------------+-------------\n30 | limited  | categorical | 0 (0.0%) |               Yes |  286 (19.1%)\n   |          |             |          |                No | 1214 (80.9%)\n-------------------------------------------------------------------------"
  },
  {
    "objectID": "nhanes_codebook.html#numerical-summaries-for-nh_3143",
    "href": "nhanes_codebook.html#numerical-summaries-for-nh_3143",
    "title": "NHANES Codebook",
    "section": "Numerical Summaries for nh_3143",
    "text": "Numerical Summaries for nh_3143\n\ndim(nh_3143)\n\n[1] 3143   30\n\ndata_codebook(nh_3143)\n\nnh_3143 (3143 rows and 30 variables, 30 shown)\n\nID | Name     | Type        | Missings |            Values |            N\n---+----------+-------------+----------+-------------------+-------------\n1  | SEQN     | character   | 0 (0.0%) |            100001 |    1 ( 0.0%)\n   |          |             |          |            100016 |    1 ( 0.0%)\n   |          |             |          |            100024 |    1 ( 0.0%)\n   |          |             |          |            100025 |    1 ( 0.0%)\n   |          |             |          |            100037 |    1 ( 0.0%)\n   |          |             |          |            100046 |    1 ( 0.0%)\n   |          |             |          |            100072 |    1 ( 0.0%)\n   |          |             |          |            100087 |    1 ( 0.0%)\n   |          |             |          |            100099 |    1 ( 0.0%)\n   |          |             |          |            100100 |    1 ( 0.0%)\n   |          |             |          |             (...) |             \n---+----------+-------------+----------+-------------------+-------------\n2  | sex      | categorical | 0 (0.0%) |              Male | 1504 (47.9%)\n   |          |             |          |            Female | 1639 (52.1%)\n---+----------+-------------+----------+-------------------+-------------\n3  | age      | numeric     | 0 (0.0%) |          [30, 59] |         3143\n---+----------+-------------+----------+-------------------+-------------\n4  | race_eth | categorical | 0 (0.0%) |          NH_White | 1367 (43.5%)\n   |          |             |          |          NH_Black |  685 (21.8%)\n   |          |             |          |          NH_Other |  627 (19.9%)\n   |          |             |          |          Hispanic |  464 (14.8%)\n---+----------+-------------+----------+-------------------+-------------\n5  | educ     | categorical | 0 (0.0%) |          Non_Grad |  366 (11.6%)\n   |          |             |          |           HS_Grad |  658 (20.9%)\n   |          |             |          |         Some_Coll | 1078 (34.3%)\n   |          |             |          |         Coll_Grad | 1041 (33.1%)\n---+----------+-------------+----------+-------------------+-------------\n6  | marital  | categorical | 0 (0.0%) |           Married | 1825 (58.1%)\n   |          |             |          |            Former |  580 (18.5%)\n   |          |             |          |             Never |  460 (14.6%)\n   |          |             |          |           Partner |  278 ( 8.8%)\n---+----------+-------------+----------+-------------------+-------------\n7  | hh_size  | numeric     | 0 (0.0%) |            [1, 6] |         3143\n---+----------+-------------+----------+-------------------+-------------\n8  | inc_pov  | numeric     | 0 (0.0%) |         [0.11, 5] |         3143\n---+----------+-------------+----------+-------------------+-------------\n9  | wtint2yr | numeric     | 0 (0.0%) | [5356.64, 433085] |         3143\n---+----------+-------------+----------+-------------------+-------------\n10 | wtmec2yr | numeric     | 0 (0.0%) | [5454.01, 419057] |         3143\n---+----------+-------------+----------+-------------------+-------------\n11 | weight   | numeric     | 0 (0.0%) |     [40.3, 207.2] |         3143\n---+----------+-------------+----------+-------------------+-------------\n12 | height   | numeric     | 0 (0.0%) |    [140.1, 202.7] |         3143\n---+----------+-------------+----------+-------------------+-------------\n13 | waist    | numeric     | 0 (0.0%) |     [63.2, 177.9] |         3143\n---+----------+-------------+----------+-------------------+-------------\n14 | pulse    | numeric     | 0 (0.0%) |         [42, 120] |         3143\n---+----------+-------------+----------+-------------------+-------------\n15 | sbp1     | numeric     | 0 (0.0%) |         [74, 236] |         3143\n---+----------+-------------+----------+-------------------+-------------\n16 | dbp1     | numeric     | 0 (0.0%) |         [34, 124] |         3143\n---+----------+-------------+----------+-------------------+-------------\n17 | sbp3     | numeric     | 0 (0.0%) |         [74, 218] |         3143\n---+----------+-------------+----------+-------------------+-------------\n18 | dbp3     | numeric     | 0 (0.0%) |         [30, 126] |         3143\n---+----------+-------------+----------+-------------------+-------------\n19 | bun      | numeric     | 0 (0.0%) |           [2, 73] |         3143\n---+----------+-------------+----------+-------------------+-------------\n20 | scr      | numeric     | 0 (0.0%) |      [0.3, 16.64] |         3143\n---+----------+-------------+----------+-------------------+-------------\n21 | glucose  | numeric     | 0 (0.0%) |         [19, 521] |         3143\n---+----------+-------------+----------+-------------------+-------------\n22 | iron     | numeric     | 0 (0.0%) |          [7, 476] |         3143\n---+----------+-------------+----------+-------------------+-------------\n23 | bili     | numeric     | 0 (0.0%) |          [0, 3.3] |         3143\n---+----------+-------------+----------+-------------------+-------------\n24 | uric     | numeric     | 0 (0.0%) |         [0.7, 18] |         3143\n---+----------+-------------+----------+-------------------+-------------\n25 | wbc      | numeric     | 0 (0.0%) |       [2.3, 31.4] |         3143\n---+----------+-------------+----------+-------------------+-------------\n26 | rbc      | numeric     | 0 (0.0%) |      [2.79, 7.14] |         3143\n---+----------+-------------+----------+-------------------+-------------\n27 | platelet | numeric     | 0 (0.0%) |         [36, 777] |         3143\n---+----------+-------------+----------+-------------------+-------------\n28 | health   | categorical | 0 (0.0%) |         Excellent |  337 (10.7%)\n   |          |             |          |            V_Good |  886 (28.2%)\n   |          |             |          |              Good | 1299 (41.3%)\n   |          |             |          |              Fair |  529 (16.8%)\n   |          |             |          |              Poor |   92 ( 2.9%)\n---+----------+-------------+----------+-------------------+-------------\n29 | smk100   | categorical | 0 (0.0%) |               Yes | 1351 (43.0%)\n   |          |             |          |                No | 1792 (57.0%)\n---+----------+-------------+----------+-------------------+-------------\n30 | limited  | categorical | 0 (0.0%) |               Yes |  563 (17.9%)\n   |          |             |          |                No | 2580 (82.1%)\n-------------------------------------------------------------------------"
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "Important\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7."
  },
  {
    "objectID": "lab6.html#option-a",
    "href": "lab6.html#option-a",
    "title": "Lab 6",
    "section": "Option A",
    "text": "Option A\nIf you do not already have a professional website describing your work life to the world, then build one, ideally using Quarto. For help, refer to the instructions from this past fall’s 431 at https://github.com/THOMASELOVE/431-labs-2024/tree/main/lab7.\nWe want you to build a professional looking website that will be useful to you. The content is up to you, and what you’d like to share but it’s worth considering using this opportunity to craft your online presence. This is also an opportunity to learn how to use Quarto for the purpose.\n\nThe most common tool people who’ve completed this have mentioned to me is the tutorial by Marvin Schmitt at https://www.marvinschmitt.com/blog/website-tutorial-quarto/, but numerous additional resources for doing this are available here.\n\nThe key thing is to commit to making this a part of your life going forward, however you most effectively do that.\nSample websites built by students in last fall’s 431 class in this class are available to help you get an idea of what you can do.\nOnce you’ve built a site you’re proud of, post a Public Note to the Lab 6 folder in Campuswire celebrating your achievement and containing a link to your page for your fellow students to be inspired by."
  },
  {
    "objectID": "lab6.html#option-b",
    "href": "lab6.html#option-b",
    "title": "Lab 6",
    "section": "Option B",
    "text": "Option B\nIf you already have a personal website that you built for 431 or for some other reason, then improve it by adding a blog post dated in 2025 that discusses in a meaningful way something important that you learned, either from reading How To Be A Modern Scientist by Jeff Leek, or from reading some part of the supplement entitled Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05 from 2019 in The American Statistician. A substantial discussion of something useful or meaningful to you is what we’re looking for.\n\n\n\n\n\n\nNote\n\n\n\nA student asked about the length of the blog post. I’m looking for something involving several paragraphs, perhaps a good minimum length would be 10 sentences.\n\n\nThen post a Public Note to the Lab 6 folder in Campuswire celebrating your achievement and containing a link to your page for your fellow students to be inspired by, and a link to the specific blog post you’ve built now, and also adding a short description of anything else (besides adding the blog post) that you have improved about your site in 2025."
  },
  {
    "objectID": "lab6.html#deadline",
    "href": "lab6.html#deadline",
    "title": "Lab 6",
    "section": "Deadline",
    "text": "Deadline\nTo receive full credit, you must complete the lab (by making your post to Campuswire as described above) no later than the deadline on the Course Calendar\n\nYou can complete Lab 6 at any time prior to that date."
  },
  {
    "objectID": "lab6.html#why-do-this",
    "href": "lab6.html#why-do-this",
    "title": "Lab 6",
    "section": "Why do this?",
    "text": "Why do this?\nTimely completion of Lab 6 will improve your course grade and will earn you some of Dr. Love’s respect. Also, no one under the age of 50 exists these days in a professional capacity without a website.\n\nIf you’re looking for Dr. Love to write a letter of recommendation for you in the fullness of time, he’s going to want to see your website."
  },
  {
    "objectID": "lab6.html#impact-of-lab-6-on-your-grade",
    "href": "lab6.html#impact-of-lab-6-on-your-grade",
    "title": "Lab 6",
    "section": "Impact of Lab 6 on your grade",
    "text": "Impact of Lab 6 on your grade\nLab 6 is due when the Calendar says it is, and is worth up to 30 points. Late submissions can receive up to 15 points, if they are between 1 hour and 7 days late. We will not grade work received more than 7 days after the deadline.\nYour homework (labs) grade is made up of the top 4 scores you had on Labs 1-5 plus your score on Lab 7, which is out of a total possible score of 250 points (each lab is worth 50), PLUS your score on Lab 6, all divided by 2.65. There are slight adjustments for people who were excused from Labs during the term.\n\nThis means that the best possible score you can get on the labs is 250 (for Labs 1-5 and 7 combined) + 30 (for Lab 6) = 280, which will then be divided by 2.65 to yield a score of 105.7.\nIf you don’t do Lab 6, then your maximum Lab score would be 250 / 2.65, which is 94.3."
  },
  {
    "objectID": "lab6.html#spring-2025-lab-6-submissions",
    "href": "lab6.html#spring-2025-lab-6-submissions",
    "title": "Lab 6",
    "section": "Spring 2025 Lab 6 Submissions",
    "text": "Spring 2025 Lab 6 Submissions\nthat Dr. Love has approved will be posted to our web site as the semester moves on."
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Course Calendar.\n\nWe charge a 5 point penalty for a lab that is 1-48 hours late.\nWe do not grade work that is more than 48 hours late.\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7. If you decide to skip a lab, please submit a note to Canvas by the deadline saying that you are skipping the lab.\n\n\n\n\nYou should be able to modify any of the first three Lab templates available on our 432-data page to help you do this Lab. Feel encouraged to try a different HTML theme if you like, maybe yeti or spacelab or materia.\n\n\n\n\n\n\nTip\n\n\n\nIn my answer sketch for Lab 4, I used the following R packages:\n\njanitor, naniar\nbroom, car, caret, gt, mice, readxl, rms\neasystats and tidyverse\n\nin case that is useful for you to know.\n\n\n\n\n\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output.\n\n\n\n\nThe hbp_3024.xlsx Excel file (first introduced in Lab 2) is available for download on the 432 data page. Be sure that you see 8 variables with missing values when you impute the data.\nA detailed description of each variable in the hbp_3024 data is available here."
  },
  {
    "objectID": "lab4.html#template",
    "href": "lab4.html#template",
    "title": "Lab 4",
    "section": "",
    "text": "You should be able to modify any of the first three Lab templates available on our 432-data page to help you do this Lab. Feel encouraged to try a different HTML theme if you like, maybe yeti or spacelab or materia.\n\n\n\n\n\n\nTip\n\n\n\nIn my answer sketch for Lab 4, I used the following R packages:\n\njanitor, naniar\nbroom, car, caret, gt, mice, readxl, rms\neasystats and tidyverse\n\nin case that is useful for you to know."
  },
  {
    "objectID": "lab4.html#our-best-advice",
    "href": "lab4.html#our-best-advice",
    "title": "Lab 4",
    "section": "",
    "text": "Review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "lab4.html#the-data",
    "href": "lab4.html#the-data",
    "title": "Lab 4",
    "section": "",
    "text": "The hbp_3024.xlsx Excel file (first introduced in Lab 2) is available for download on the 432 data page. Be sure that you see 8 variables with missing values when you impute the data.\nA detailed description of each variable in the hbp_3024 data is available here."
  },
  {
    "objectID": "lab4.html#be-sure-to-include-session-information",
    "href": "lab4.html#be-sure-to-include-session-information",
    "title": "Lab 4",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "lab4.html#after-the-lab",
    "href": "lab4.html#after-the-lab",
    "title": "Lab 4",
    "section": "After the Lab",
    "text": "After the Lab\n\nWe will post an answer sketch to our Shared Google Drive 48 hours after the Lab is due.\nWe will post grades to our Grading Roster on our Shared Google Drive one week after the Lab is due.\nSee the Lab Appeal Policy in our Syllabus if you are interested in having your Lab grade reviewed, and use the Lab Regrade Request form specified there to complete the task. Thank you."
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Course Calendar.\n\nWe charge a 5 point penalty for a lab that is 1-48 hours late.\nWe do not grade work that is more than 48 hours late.\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7. If you decide to skip a lab, please submit a note to Canvas by the deadline saying that you are skipping the lab.\n\n\n\n\nThere is a Lab 2 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 2, as it will make things easier for you and for the people grading your work.\n\nIn the Lab 2 template, we use the zephyr theme. If you’d like to use a different theme, the available list is here.\n\n\n\n\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output.\n\n\n\n\nThe hbp3024.xlsx file is available for download on the 432 data page.\nA detailed description of each variable in the hbp3024 data is available here."
  },
  {
    "objectID": "lab2.html#template",
    "href": "lab2.html#template",
    "title": "Lab 2",
    "section": "",
    "text": "There is a Lab 2 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 2, as it will make things easier for you and for the people grading your work.\n\nIn the Lab 2 template, we use the zephyr theme. If you’d like to use a different theme, the available list is here."
  },
  {
    "objectID": "lab2.html#our-best-advice",
    "href": "lab2.html#our-best-advice",
    "title": "Lab 2",
    "section": "",
    "text": "Review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "lab2.html#the-data",
    "href": "lab2.html#the-data",
    "title": "Lab 2",
    "section": "",
    "text": "The hbp3024.xlsx file is available for download on the 432 data page.\nA detailed description of each variable in the hbp3024 data is available here."
  },
  {
    "objectID": "lab2.html#be-sure-to-include-session-information",
    "href": "lab2.html#be-sure-to-include-session-information",
    "title": "Lab 2",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "lab2.html#after-the-lab",
    "href": "lab2.html#after-the-lab",
    "title": "Lab 2",
    "section": "After the Lab",
    "text": "After the Lab\n\nWe will post an answer sketch to our Shared Google Drive 48 hours after the Lab is due.\nWe will post grades to our Grading Roster on our Shared Google Drive one week after the Lab is due.\nSee the Lab Appeal Policy in our Syllabus if you are interested in having your Lab grade reviewed, and use the Lab Regrade Request form specified there to complete the task. Thank you."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "432 Main Page for Spring 2025",
    "section": "",
    "text": "PQHS/CRSP/MPHP 432 is the second half (431 is the first half) of a two-semester course taught by Professor Thomas Love in the Department of Population and Quantitative Health Sciences at Case Western Reserve University.\nIf you have any questions before class begins, contact Dr. Love at Thomas dot Love at case dot edu. Once the class starts, visit the Contact Us page to learn more about how to get help from us."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "432 Main Page for Spring 2025",
    "section": "",
    "text": "PQHS/CRSP/MPHP 432 is the second half (431 is the first half) of a two-semester course taught by Professor Thomas Love in the Department of Population and Quantitative Health Sciences at Case Western Reserve University.\nIf you have any questions before class begins, contact Dr. Love at Thomas dot Love at case dot edu. Once the class starts, visit the Contact Us page to learn more about how to get help from us."
  },
  {
    "objectID": "index.html#this-years-class-will-be-unusual.",
    "href": "index.html#this-years-class-will-be-unusual.",
    "title": "432 Main Page for Spring 2025",
    "section": "This year’s class will be unusual.",
    "text": "This year’s class will be unusual.\nThanks to Dr. Love’s recovery from a broken leg and ankle surgery, this year’s 432 class will be held remotely (via Zoom) before Spring Break, and then (mostly) in person starting March 18. See the Course Calendar for more details."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "432 Main Page for Spring 2025",
    "section": "Getting Started",
    "text": "Getting Started\nYour first six steps in this course are to:\n\nRegister and enroll via the CWRU Student Information System (SIS).\n\nSection 1 of PQHS 432, CRSP 432 and MPHP 432 are identical.\n\nReview this course website, and be sure to visit the Course Calendar, and skim through the Course Syllabus.\nYou need to buy Jeff Leek’s How to be a Modern Scientist, available electronically through https://leanpub.com/modernscientist. The suggested cost is $10, but you can pay what you want.\nPlease fill out the Welcome to 432 Survey at https://bit.ly/432-2025-welcome-survey to help us get to know you a little better. Note that you must log into Google via your CWRU account in order to access the survey, which should take about 10 minutes to complete.\nGet started installing or updating the software you’ll need for the course.\nAttend our first class session via Zoom starting at 1 PM on 2024-01-14. Zoom information will be sent to all registered students via their CWRU email."
  },
  {
    "objectID": "index.html#everything-will-appear-here.",
    "href": "index.html#everything-will-appear-here.",
    "title": "432 Main Page for Spring 2025",
    "section": "Everything will appear here.",
    "text": "Everything will appear here.\nEverything that Professor Love will provide to help you with the course will be linked through this website. The menu bar links to just about everything you’ll need this semester, and its elements will go live as they become available. Things you’ll find here include…\n\nThe Course Calendar, which includes links to class materials, assignments and other information, and which is the final word on all deadlines for the course.\nThe Course Syllabus for 432.\nProfessor Love’s Course Notes for 432.\nDetails on Assignments, including Projects, Quizzes, and Labs.\nInformation on installing R and RStudio software, installing key R packages, and downloading data, code and templates you’ll need for this class\nKey Links to other systems that will help us deliver 432, including:\n\nthe Canvas system (CWRU log-in required) we use to communicate information about class recordings, and for turning in assignments\nour Shared Google Drive (CWRU log-in required) where we provide answer sketches for class assignments, an in-semester grading roster, and feedback on work\nthe Campuswire discussion board for the 432 course\n\nInformation on places where you can get help and contact us once the course begins.\nAdditional Sources and references are here, including things to read or watch to supplement our main work"
  },
  {
    "objectID": "index.html#accessing-the-431-materials",
    "href": "index.html#accessing-the-431-materials",
    "title": "432 Main Page for Spring 2025",
    "section": "Accessing the 431 Materials",
    "text": "Accessing the 431 Materials\nNote that the 431 class materials from Fall 2024 are here, and will remain there until 2025-06-01."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "If you have a question before class begins, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "contact.html#before-class-begins",
    "href": "contact.html#before-class-begins",
    "title": "Contact Us",
    "section": "",
    "text": "If you have a question before class begins, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "contact.html#once-class-starts",
    "href": "contact.html#once-class-starts",
    "title": "Contact Us",
    "section": "Once Class Starts",
    "text": "Once Class Starts\nIf you’ve spent 15 minutes working on something and are stuck, don’t keep working on it. ASK FOR HELP. To get help in this course, you have three main options:\n\nUse our Campuswire discussion board to ask (and answer) questions about the course. This is available 24 hours a day, 7 days a week. The system is highly catered to getting you help fast and efficiently from classmates, the TAs, and myself. Rather than emailing questions to the teaching staff, I encourage you to post your questions on Campuswire. If you’re new to Campuswire, you can sign up via the email you received from Dr. Love in mid-January at your CWRU email.\nAttend TA office hours to get one-on-one help or share a computer problem. TA office hours are held via Zoom, and will begin on Friday 2025-01-17, and continue through Tuesday 2025-04-22, except for Martin Luther King Day (Monday 2025-01-20) and Spring Break (hours are cancelled from 2025-03-08 through 2025-03-14.) Here is the schedule…\n\n\n\n\nDay\nTime(s)\n\n\n\n\nFridays\n12:15 to 1:45 PM\n\n\nSundays\n6 to 7:30 PM and 8:30 to 10 PM\n\n\nMondays\n11:30 AM to 1 PM and 7:30 to 9 PM and 9 to 10:30 PM\n\n\nTuesdays\n11:15 AM to 12:45 PM and 6 to 7:30 PM and 9 to 10:30 PM\n\n\n\nZoom information to join these sessions is posted to our Shared Google Drive.\n\nEmail Professor Love at Thomas dot Love at case dot edu with any individual concerns related to the course that you do not feel comfortable asking elsewhere, or if you have any special concerns or needs that you want him to know about, or to set up an appointment with him for a Zoom meeting.\n\nProfessor Love will also hold “drop in” hours for as long as necessary after each class. Just let him know that you want to talk with him at the end of class."
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar for 432: Spring 2025",
    "section": "",
    "text": "Class\nDate\nLocation\nDescription\n\n\n\n\nClass 15\n03-04\nZoom\nTime-to-Event Data, part 1  Ask Me Anything: Quiz 1\n\n\nQuiz 1\n03-05\nGoogle\nQuiz 1 due at noon (covers classes 1-14)\n\n\nClass 16\n03-06\nZoom\nQuiz 1 results, Time-to-Event Data, part 2  Ask Me Anything: Project A\n\n\n–\n–\n–\nSpring Break - no 432 class or office hours 03-08 through 03-14\n\n\nClass 17\n03-18\nIn Person\nRegression on Count Outcomes, 1\n\n\nProject A\n03-19\nCanvas\nProject A Portfolio due to Canvas at noon\n\n\nClass 18\n03-20\nIn Person\nRegression on Count Outcomes, 2\n\n\nClass 19\n03-25\nIn Person\nRegression on Multi-Categorical Outcomes, 1\n\n\nLab 6\n03-26\nCampuswire\nLab 6 submitted via Campuswire at noon\n\n\nClass 20\n03-27\nIn Person\nRegression on Multi-Categorical Outcomes, 2"
  },
  {
    "objectID": "calendar.html#march-2025",
    "href": "calendar.html#march-2025",
    "title": "Calendar for 432: Spring 2025",
    "section": "",
    "text": "Class\nDate\nLocation\nDescription\n\n\n\n\nClass 15\n03-04\nZoom\nTime-to-Event Data, part 1  Ask Me Anything: Quiz 1\n\n\nQuiz 1\n03-05\nGoogle\nQuiz 1 due at noon (covers classes 1-14)\n\n\nClass 16\n03-06\nZoom\nQuiz 1 results, Time-to-Event Data, part 2  Ask Me Anything: Project A\n\n\n–\n–\n–\nSpring Break - no 432 class or office hours 03-08 through 03-14\n\n\nClass 17\n03-18\nIn Person\nRegression on Count Outcomes, 1\n\n\nProject A\n03-19\nCanvas\nProject A Portfolio due to Canvas at noon\n\n\nClass 18\n03-20\nIn Person\nRegression on Count Outcomes, 2\n\n\nClass 19\n03-25\nIn Person\nRegression on Multi-Categorical Outcomes, 1\n\n\nLab 6\n03-26\nCampuswire\nLab 6 submitted via Campuswire at noon\n\n\nClass 20\n03-27\nIn Person\nRegression on Multi-Categorical Outcomes, 2"
  },
  {
    "objectID": "calendar.html#april-2025",
    "href": "calendar.html#april-2025",
    "title": "Calendar for 432: Spring 2025",
    "section": "April 2025",
    "text": "April 2025\n\n\n\nClass\nDate\nLocation\nDescription\n\n\n\n\nClass 21\n04-01\nIn Person\nRegression on Multi-Categorical Outcomes, 3\n\n\nProject B\n04-02\nGoogle\nProject B Proposal Form due at noon\n\n\nClass 22\n04-03\nIn Person\nTime to Event (Survival) Outcomes, 1\n\n\nClass 23\n04-08\nIn Person\nTime to Event (Survival) Outcomes, 2\n\n\nLab 7\n04-09\nCanvas\nLab 7 due to Canvas at noon\n\n\nClass 24\n04-10\nIn Person\nK-Means Clustering, PCA\n\n\nClass 25\n04-15\nZoom\nProject B: Ask Me Anything\n\n\nClass 26\n04-17\nIn Person\nIntroduction to Hierarchical Models\n\n\n–\n04-17\n3 - 6 PM\nIn Person Project B Presentations\n\n\nQuiz 2\n04-18\nQuiz 2\nQuiz 2 made available at noon to all students\n\n\n–\n04-18\n10 AM - 2 PM\nZoom Project B Presentations\n\n\n–\n04-21\n8:30AM - Noon,  1 - 6:30PM\nZoom Project B Presentations\n\n\n–\n04-22\n8:30AM - Noon,  1 - 6:30PM\nZoom Project B Presentations\n\n\n–\n04-22\n–\nNo 432 class today, Last Day for TA office hours\n\n\n–\n04-23\n8:30 - 11 AM,  12:30 - 2 PM\nZoom Project B Presentations\n\n\nClass 27\n04-24\nIn Person\nSemester Wrap-up, Where To Go From Here\n\n\nQuiz 2\n04-25\nGoogle\nQuiz 2 due at noon (covers classes 1-24)\n\n\nProject B\n04-30\nCanvas\nProject B Portfolio due to Canvas at noon\n\n\n\nCourse Evaluations Please visit https://webapps.case.edu/courseevals/ after you present your project, and complete the course evaluations before they close in early May. Thanks."
  },
  {
    "objectID": "calendar.html#january-2025",
    "href": "calendar.html#january-2025",
    "title": "Calendar for 432: Spring 2025",
    "section": "January 2025",
    "text": "January 2025\n\n\n\nClass\nDate\nLocation\nDescription\n\n\n\n\nClass 1\n01-14\nZoom\nIntroduction and Logistics\n\n\nSurvey\n01-15\nGoogle\nComplete Welcome to 432 Survey by noon\n\n\nClass 2\n01-16\nZoom\nGetting Up to Speed\n\n\n–\n01-17\nZoom\nTA office hours begin\n\n\n–\n01-20\nZoom\nMartin Luther King Jr Holiday - no office hours\n\n\nClass 3\n01-21\nZoom\nLinear Regression, 1\n\n\nLab 1\n01-22\nCanvas\nLab 1 due to Canvas at noon\n\n\nClass 4\n01-23\nZoom\nLogistic Regression, 1\n\n\nClass 5\n01-28\nZoom\nLinear and Logistic Regression, 2\n\n\nLab 2\n01-29\nCanvas\nLab 2 due to Canvas at noon\n\n\nClass 6\n01-30\nZoom\nLinear and Logistic Regression, 3"
  },
  {
    "objectID": "calendar.html#february-2025",
    "href": "calendar.html#february-2025",
    "title": "Calendar for 432: Spring 2025",
    "section": "February 2025",
    "text": "February 2025\n\n\n\nClass\nDate\nLocation\nDescription\n\n\n\n\nReading\n02-03\n–\nBefore Class 7, read all of How to be a Modern Scientist\n\n\nClass 7\n02-04\nZoom\nLinear and Logistic Regression, 4\n\n\nProject A\n02-05\nCanvas\nProject A Plan due to Canvas at noon\n\n\nClass 8\n02-06\nZoom\nLogistic Regression and the movies, 1\n\n\nClass 9\n02-11\nZoom\nLogistic Regression and the movies, 2\n\n\nLab 3\n02-12\nCanvas\nLab 3 due to Canvas at noon\n\n\nClass 10\n02-13\nZoom\nLinear Regression and happiness, 1\n\n\nClass 11\n02-18\nZoom\nLinear Regression and happiness, 2\n\n\nLab 4\n02-19\nCanvas\nLab 4 due to Canvas at noon\n\n\nClass 12\n02-20\nZoom\nBuilding Table 1, Using Survey Weights\n\n\nClass 13\n02-25\nZoom\nPower and Sample Size (The Fundamentals)\n\n\nLab 5\n02-26\nCanvas\nLab 5 due to Canvas at noon\n\n\nClass 14\n02-27\nZoom\nPower and Sample Size, Retrospective Design\n\n\nQuiz 1\n02-28\nQuiz 1\nQuiz 1 made available at noon to all students"
  },
  {
    "objectID": "calendar.html#ta-office-hours",
    "href": "calendar.html#ta-office-hours",
    "title": "Calendar for 432: Spring 2025",
    "section": "TA Office Hours",
    "text": "TA Office Hours\nAttend TA office hours to get one-on-one help or share a computer problem. TA office hours are held via Zoom, and will begin on Friday 2025-01-17, and continue through Tuesday 2025-04-22, except for Martin Luther King Day (Monday 2025-01-20) and Spring Break (hours are cancelled from 2025-03-08 through 2025-03-14.) Here is the schedule…\n\n\n\nDay\nTime(s)\n\n\n\n\nFridays\n12:15 to 1:45 PM\n\n\nSundays\n6 to 7:30 PM and 8:30 to 10 PM\n\n\nMondays\n11:30 AM to 1 PM and 7:30 to 9 PM and 9 to 10:30 PM\n\n\nTuesdays\n11:15 AM to 12:45 PM and 6 to 7:30 PM and 9 to 10:30 PM\n\n\n\nZoom information to join these sessions is posted to our Shared Google Drive."
  },
  {
    "objectID": "432_projectA_demo.html",
    "href": "432_projectA_demo.html",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "",
    "text": "This is a demonstration Project A for 432. It includes everything you must include in sections 1, 2, 4-9, and 11-13, although there are numerous places where a better project would say more about the results shown here than I have. I also have only performed single imputation, rather than multiple imputation, in this demonstration project, and I’ve only given some guidance regarding the discussion, leaving it up to you.\nI decided to share this with students in the class in an effort to ensure that students could meet the minimum standards for the project (to obtain a solid B) more easily."
  },
  {
    "objectID": "432_projectA_demo.html#what-is-this",
    "href": "432_projectA_demo.html#what-is-this",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "",
    "text": "This is a demonstration Project A for 432. It includes everything you must include in sections 1, 2, 4-9, and 11-13, although there are numerous places where a better project would say more about the results shown here than I have. I also have only performed single imputation, rather than multiple imputation, in this demonstration project, and I’ve only given some guidance regarding the discussion, leaving it up to you.\nI decided to share this with students in the class in an effort to ensure that students could meet the minimum standards for the project (to obtain a solid B) more easily."
  },
  {
    "objectID": "432_projectA_demo.html#r-packages-and-setup",
    "href": "432_projectA_demo.html#r-packages-and-setup",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "R Packages and Setup",
    "text": "R Packages and Setup\n\n\nCode\nknitr::opts_chunk$set(comment = NA) \n\nlibrary(janitor) \nlibrary(naniar)\n\nlibrary(broom)\nlibrary(car)\nlibrary(caret)\nlibrary(GGally)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(knitr)\nlibrary(mice)\nlibrary(mosaic)\nlibrary(patchwork)\nlibrary(ROCR)\nlibrary(rsample)\nlibrary(rms)\n\nlibrary(easystats)\nlibrary(tidyverse) \n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "432_projectA_demo.html#sec-orig",
    "href": "432_projectA_demo.html#sec-orig",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "3.1 How the nh_demo file was built",
    "text": "3.1 How the nh_demo file was built\nI included the following items, from both 2017-18 and 2015-16 NHANES.\nFrom the DEMO files: DEMO_J for 2017-18 and DEMO_I for 2015-16\n\nSEQN = Subject identifying code\nRIDSTATR = Interview/Examination Status (categorical)\n\n1 = Interview only\n2 = Interview and MEC examination (MEC = mobile examination center)\n\nRIDAGEYR = Age in years (quantitative, topcoded at 80)\n\nAll subjects ages 80 and over are coded as 80\n\nRIAGENDR = Sex (categorical)\n\n1 = Male, 2 = Female\n\nRIDRETH3 = Race/Ethnicity (categorical)\n\n1 = Mexican-American,\n2 = Other Hispanic (1 and 2 are often combined)\n3 = Non-Hispanic White\n4 = Non-Hispanic Black\n6 = Non-Hispanic Asian\n7 = Other Race including Multi-Racial\nNote that categories 1 and 2 are often combined, and sometimes we leave out category 7, or combine it with 6.\n\n\nFrom the Body Measures (BMX) files: BMX_J for 2017-18 and BMX_I for 2015-16 (BMX is part of the Examination data)\n\nBMXWAIST = Waist Circumference in cm (quantitative)\n\nFrom the Cholesterol - High-Density Lipoprotein (HDL) files: HDL_J for 2017-18 and HDL_I for 2015-16 (HDL is part of the Lab data)\n\nLBDHDD = Direct HDL cholesterol in mg/dl (quantitative)\n\nFrom the Current Health Status (HSQ) file HSQ_J for 2017-18 and HSQ_I for 2015-16 (HSQ is part of the Questionnaire data)\n\nHSD010 = Self-reported general health (categorical)\n\n1 = Excellent\n2 = Very good\n3 = Good\n4 = Fair\n5 = Poor\n7 = Refused\n9 = Don’t Know"
  },
  {
    "objectID": "432_projectA_demo.html#ingesting-and-cleaning-the-nhanes-data",
    "href": "432_projectA_demo.html#ingesting-and-cleaning-the-nhanes-data",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "3.2 Ingesting and Cleaning the NHANES data",
    "text": "3.2 Ingesting and Cleaning the NHANES data\nI used functions from the nhanesA package to pull in the data from each necessary database using nhanes() and translated = FALSE, then used zap_label() to delete the labels, and converted to tibbles. Next, I used inner_join() to merge data within each NHANES cycle.\nNext, I selected the 8 variables we need from the merged cycle-specific files, and converted each of the categorical variables to factors except the SEQN (subject code.) Then I added an indicator of the CYCLE (2015-16 or 2017-18) in which the data were drawn, and combined it all into one big tibble with full_join().\nThen I decided to restrict our work here to adult subjects between the ages of 40 and 79, in part to avoid the fact that NHANES classifies everyone over age 80 as RIDAGEYR = 80. So that involved filtering to those with RIDAGEYR &gt; 39 and RIDAGEYR &lt; 80.\nWe have three quantitative variables: age (RIDAGEYR), waist circumference (BMXWAIST) and HDL Cholesterol (LBDHDD), which I renamed to AGE, WAIST and HDL, respectively. The ranges (minimum and maximum) for Age, Waist Circumference and HDL Cholesterol all seem fairly plausible to me. Perhaps you know better, and please do use that knowledge. We do have several hundred missing values in the waist circumference and HDL cholesterol values that we will need to deal with after we sample the data.\nIn terms of categorical variables, I started by checking to see that everyone has RIDSTATR status 2, meaning they completed both the NHANES questionnaire and the NHANES examination. Since they do, I then made sure our CYCLE variable works to indicate the NHANES reporting CYCLE for each subject by running tabyl(CYCLE, RIDSTATR).\nNext, I changed RIAGENDR to SEX with values “M” and “F” as a factor. Another option would have been to use a 1/0 numeric variable to represent, for example FEMALE = 1 if the subject was reported as female and 0 if the subject was reported as male.\nNext, I created RACE_ETH to replace RIDRETH3.\n\nI recoded the levels of the RIDRETH3 variable to use short, understandable group names, using mutate() and fct_recode(), and\nCollapsed the first two categories (Mexican-American and Other Hispanic) into a single category, using fct_recode(), and\nChanged the variable name to RACE_ETH using mutate() and\nSorted the resulting factor in order of their counts using fct_infreq().\n\nThe most common RACE_ETH turns out to be Non-Hispanic White, followed by Hispanic, then Non-Hispanic Black, Non-Hispanic Asian and finally Other Race (including Multi-Racial.) We won’t collapse any further for now.\nNext, I changed HSD010 to SROH, as follows.\n\nAgain, I recoded the levels of this categorical variable with fct_recode()\nI also renamed the variable SROH (which is an abbreviation for self-reported overall health) with mutate(). Always explain your abbreviations.\nI also converted the values 7 (Refused) and 9 (Don’t Know) to missing values with na_if(), then I used droplevels() to drop all of the now-unused levels in the factors I’m using."
  },
  {
    "objectID": "432_projectA_demo.html#sec-hdlrisk",
    "href": "432_projectA_demo.html#sec-hdlrisk",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "3.3 Our outcomes",
    "text": "3.3 Our outcomes\nOur outcomes are each based on HDL cholesterol level. So I filtered the data for complete cases on that variable.\nNext, I created a binary outcome for logistic regression, which is an indicator (taking the values 0 and 1) for an HDL cholesterol value that is “at risk” according to these standards published on the Mayo Clinic site, specifically, an adult male is considered to be at risk if their HDL &lt; 40 mg/dl and an adult female is considered to be at risk if their HDL &lt; 50 mg/dl.\nSince we have complete data in nh_demo_full on HDL and SEX, I used that data to create the new variable HDL_RISK as shown below, then checked to see that this had worked properly:\n\n\nCode\n## this code chunk is not evaluated here - just showing you what I would do\n\nnh_demo_full &lt;- nh_demo_full |&gt;\n  mutate(HDL_RISK = case_when( \n    SEX == \"M\" & HDL &lt; 40 ~ 1,\n    SEX == \"F\" & HDL &lt; 50 ~ 1,\n    .default = 0))\n\n## check to see if this worked properly\nnh_demo_full |&gt; group_by(HDL_RISK, SEX) |&gt; \n  summarise(n = n(), min(HDL), max(HDL))"
  },
  {
    "objectID": "432_projectA_demo.html#arranging-the-tibble-and-sampling-the-data",
    "href": "432_projectA_demo.html#arranging-the-tibble-and-sampling-the-data",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "3.4 Arranging the Tibble and Sampling the Data",
    "text": "3.4 Arranging the Tibble and Sampling the Data\n\nWe don’t need RIDSTATR any more because we’ve checked and see that all its values are 2, as needed.\nWe’ve renamed some variables and replaced others with better versions.\n\nFinally, I selected 999 observations from our working file, called the nh_demo_full tibble for use in our subsequent analyses, and this sample is called nh_demo. I have provided you with the nh_demo.Rds file, which I’m loading below.\n\n\nCode\nnh_demo &lt;- read_rds(\"data/nh_demo.Rds\")"
  },
  {
    "objectID": "432_projectA_demo.html#listing-the-tibble",
    "href": "432_projectA_demo.html#listing-the-tibble",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "4.1 Listing the Tibble",
    "text": "4.1 Listing the Tibble\n\n\n\n\n\n\nNote\n\n\n\nIf I ask you to print/list a tibble, part of the reason is to ensure that it actually is a tibble. A tibble will, as below, only print the first 10 rows.\n\n\n\n\nCode\nnh_demo\n\n\n# A tibble: 999 × 9\n   SEQN    HDL HDL_RISK   AGE RACE_ETH WAIST SROH      SEX   CYCLE  \n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt; &lt;chr&gt;  \n 1 84842    48        0    50 Hispanic 101.  Good      M     2015-16\n 2 92202   111        0    65 NH_Black  77   Fair      F     2015-16\n 3 93010    46        1    63 NH_White  NA   Poor      F     2015-16\n 4 93974    39        1    59 NH_White 111.  Good      M     2017-18\n 5 96205    56        0    52 NH_Black  98   Good      M     2017-18\n 6 95571    42        1    54 NH_White 104.  Good      F     2017-18\n 7 99969    54        0    69 NH_Black 115.  Fair      M     2017-18\n 8 89494    36        1    42 NH_Asian  98.1 &lt;NA&gt;      F     2015-16\n 9 83887   101        0    51 NH_Black  74   Excellent F     2015-16\n10 96908    48        1    73 Other    102   Good      F     2017-18\n# ℹ 989 more rows"
  },
  {
    "objectID": "432_projectA_demo.html#size-and-identifiers",
    "href": "432_projectA_demo.html#size-and-identifiers",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "4.2 Size and Identifiers",
    "text": "4.2 Size and Identifiers\nOur analytic tibble, called nh_demo, has 999 rows (observations) on 9 columns (variables.) Our indicator variable is the SEQN, and we have a unique SEQN for each row in our data set, as the code below demonstrates.\n\n\nCode\nidentical(nrow(nh_demo), n_distinct(nh_demo$SEQN))\n\n\n[1] TRUE"
  },
  {
    "objectID": "432_projectA_demo.html#save-the-tibble",
    "href": "432_projectA_demo.html#save-the-tibble",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "4.3 Save The Tibble",
    "text": "4.3 Save The Tibble\nI’ve already done this so I won’t do it again, but this is the code I would use to save the file.\n\n\n\n\n\n\nNote\n\n\n\nNote that I have set this code chunk to eval: false so that it will not actually do anything. I’m just showing you what I would do to save the Rds file to the data sub-directory of our R project. You should not use eval: false anywhere in your Project A.\n\n\n\n\nCode\nwrite_rds(nh_demo, \"data/nh_demo.Rds\")"
  },
  {
    "objectID": "432_projectA_demo.html#five-key-statements",
    "href": "432_projectA_demo.html#five-key-statements",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "5.1 Five Key Statements",
    "text": "5.1 Five Key Statements\nI suggest you begin your Project A codebook with five statements (like the ones shown below) to fix ideas about the sample size, the missingness, the outcomes and a statement about the roles for the other variables.\n\n\n\n\n\n\nNote\n\n\n\nIf you look at the Quarto code that generated this document, you will see that I have used in-line coding to fill in the counts of subjects in statements 1 and 2. You should, too.\n\n\n\nSample Size The data in our complete nh_demo sample consist of 999 subjects from NHANES 2015-16 and NHANES 2017-18 between the ages of 40 and 79 in whom our outcome variables (HDL and HDL_RISK) were measured.\nMissingness Of the 999 subjects, 912 have complete data on all variables listed below.\nOur outcome variables are HDL, which is the subject’s serum HDL cholesterol measured in mg/dl for the linear regression, and HDL_RISK, which indicates whether or not the subject’s HDL cholesterol is at risk (too low) given their sex, for the logistic regression. There are no missing data in either of these outcomes.\nCandidate predictors for my models include AGE, WAIST, RACE_ETH and SROH, (for both models) as well as SEX for the linear model.\nThe other variables contained in my tidy tibble are SEQN which is the subject identifying code, and CYCLE which identifies whether this subject was part of the 2015-16 or 2017-18 NHANES reporting cycle."
  },
  {
    "objectID": "432_projectA_demo.html#describing-the-variables-5-tabs",
    "href": "432_projectA_demo.html#describing-the-variables-5-tabs",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "5.2 Describing the Variables (5 Tabs)",
    "text": "5.2 Describing the Variables (5 Tabs)\nVariables included in the nh_demo data are summarized and described in five different ways in the panel below. Click on the tab that interests you to see results.\n\n\n\n\n\n\nTip\n\n\n\nPlease provide, at minimum, the variable descriptions list and the data_codebook() results here. You can include the others if you like.\n\n\n\nDefinitionsdata_codebook()describe()tbl_summary()Missingness\n\n\nIn addition to the brief descriptions (including units of measurement, where appropriate) in the table below, descriptions of the original NHANES variables which led to these are also discussed in Section 3.1.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nSEQN\nNHANES subject identifying code\n\n\nHDL\nHDL cholesterol in mg/dl (outcome for our linear model)\n\n\nHDL_RISK\nDoes HDL put subject at risk? 1 (yes) if HDL &lt; 40 for males or if HDL &lt; 50 for females, otherwise 0 (outcome for logistic model)\n\n\nAGE\nAge in years, restricted to 40-79 here\n\n\nRACE_ETH\nRace-Ethnicity: five categories (NH_white, NH_Black, NH_Asian, Hispanic, Other)\n\n\nWAIST\nWaist circumference, in cm\n\n\nSROH\nSelf-Reported Overall Health: five categories (Excellent, Very_Good, Good, Fair, Poor)\n\n\nSEX\nBiological Sex: either F or M\n\n\nCYCLE\nNHANES reporting cycle: either 2015-16 or 2017-18\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe data_codebook() function comes from the datawizard package, which is part of the easystats ecosystem.\n\n\n\n\nCode\ndata_codebook(nh_demo)\n\n\nnh_demo (999 rows and 9 variables, 9 shown)\n\nID | Name     | Type        |  Missings |        Values |           N\n---+----------+-------------+-----------+---------------+------------\n1  | SEQN     | character   |  0 (0.0%) |        100012 |   1 ( 0.1%)\n   |          |             |           |        100067 |   1 ( 0.1%)\n   |          |             |           |        100112 |   1 ( 0.1%)\n   |          |             |           |        100142 |   1 ( 0.1%)\n   |          |             |           |        100145 |   1 ( 0.1%)\n   |          |             |           |        100150 |   1 ( 0.1%)\n   |          |             |           |        100167 |   1 ( 0.1%)\n   |          |             |           |        100178 |   1 ( 0.1%)\n   |          |             |           |        100191 |   1 ( 0.1%)\n   |          |             |           |        100210 |   1 ( 0.1%)\n   |          |             |           |         (...) |            \n---+----------+-------------+-----------+---------------+------------\n2  | HDL      | numeric     |  0 (0.0%) |     [22, 149] |         999\n---+----------+-------------+-----------+---------------+------------\n3  | HDL_RISK | numeric     |  0 (0.0%) |             0 | 666 (66.7%)\n   |          |             |           |             1 | 333 (33.3%)\n---+----------+-------------+-----------+---------------+------------\n4  | AGE      | numeric     |  0 (0.0%) |      [40, 79] |         999\n---+----------+-------------+-----------+---------------+------------\n5  | RACE_ETH | categorical |  0 (0.0%) |      NH_White | 333 (33.3%)\n   |          |             |           |      Hispanic | 255 (25.5%)\n   |          |             |           |      NH_Black | 229 (22.9%)\n   |          |             |           |      NH_Asian | 145 (14.5%)\n   |          |             |           |         Other |  37 ( 3.7%)\n---+----------+-------------+-----------+---------------+------------\n6  | WAIST    | numeric     | 44 (4.4%) | [66.5, 159.2] |         955\n---+----------+-------------+-----------+---------------+------------\n7  | SROH     | categorical | 56 (5.6%) |     Excellent |  84 ( 8.9%)\n   |          |             |           |     Very_Good | 223 (23.6%)\n   |          |             |           |          Good | 369 (39.1%)\n   |          |             |           |          Fair | 231 (24.5%)\n   |          |             |           |          Poor |  36 ( 3.8%)\n---+----------+-------------+-----------+---------------+------------\n8  | SEX      | character   |  0 (0.0%) |             F | 540 (54.1%)\n   |          |             |           |             M | 459 (45.9%)\n---+----------+-------------+-----------+---------------+------------\n9  | CYCLE    | character   |  0 (0.0%) |       2015-16 | 475 (47.5%)\n   |          |             |           |       2017-18 | 524 (52.5%)\n---------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe describe() function (and the html() function) used here come from the Hmisc package, developed by Frank Harrell and his colleagues.\n\n\n\n\nCode\ndescribe(nh_demo) |&gt; html()\n\n\n\n\n\n\nnh_demo Descriptives\nnh_demo  9  Variables   999  Observations\n\nSEQN\n\n \n nmissingdistinct\n 9990999\n \n\nlowest : 100012 100067 100112 100142 100145 ,  highest: 99937  99945  99966  99969  99997 \n\nHDL\n\n \n nmissingdistinctInfoMeanpMedianGmd.05.10.25.50.75.90.95\n 9990860.99953.652.518.331354151637685\n \n\nlowest :  22  23  24  25  26 ,  highest: 108 111 112 137 149\n\nHDL_RISK\n\n \n nmissingdistinctInfoSumMean\n 999020.6673330.3333\n \n\n\nAGE\n\n \n nmissingdistinctInfoMeanpMedianGmd.05.10.25.50.75.90.95\n 9990400.99958.015812.0342445058667375\n \n\nlowest : 40 41 42 43 44 ,  highest: 75 76 77 78 79\n\nRACE_ETH\n\n \n nmissingdistinct\n 99905\n \n\n Value      NH_White Hispanic NH_Black NH_Asian    Other\n Frequency       333      255      229      145       37\n Proportion    0.333    0.255    0.229    0.145    0.037 \n\n\nWAIST\n        n  missing distinct     Info     Mean  pMedian      Gmd      .05      .10 \n      955       44      482        1    102.7    102.1    18.05    77.27    82.40 \n      .25      .50      .75      .90      .95 \n    92.05   101.20   112.60   123.50   131.65  \n\nlowest : 66.5  67.9  68.7  69    70.3  ,  highest: 150.5 151   151.3 153.5 159.2\n\nSROH\n\n \n nmissingdistinct\n 943565\n \n\n Value      Excellent Very_Good      Good      Fair      Poor\n Frequency         84       223       369       231        36\n Proportion     0.089     0.236     0.391     0.245     0.038 \n\n\nSEX\n\n \n nmissingdistinct\n 99902\n \n\n Value          F     M\n Frequency    540   459\n Proportion 0.541 0.459 \n\n\nCYCLE\n\n \n nmissingdistinct\n 99902\n \n\n Value      2015-16 2017-18\n Frequency      475     524\n Proportion   0.475   0.525 \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe tbl_summary() function used here comes from the gtsummary package. Note that missing values are listed as “Unknown” by default.\n\n\n\n\nCode\ntbl_summary(select(nh_demo, -SEQN),\n        label = list(\n            HDL = \"HDL (HDL Cholesterol in mg/dl)\",\n            HDL_RISK = \"HDL_RISK (HDL &lt; 40 if Male, &lt; 50 if Female)?\",\n            AGE = \"AGE (in years)\",\n            WAIST = \"WAIST (circumference in cm)\",\n            CYCLE = \"CYCLE (NHANES reporting)\",\n            RACE_ETH = \"RACE_ETH (Race/Ethnicity)\",\n            SROH = \"SROH (Self-Reported Overall Health)\"),\n        stat = list( all_continuous() ~ \n                \"{median} [{min} to {max}]\" ))\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 9991\n\n\n\n\nHDL (HDL Cholesterol in mg/dl)\n51 [22 to 149]\n\n\nHDL_RISK (HDL &lt; 40 if Male, &lt; 50 if Female)?\n333 (33%)\n\n\nAGE (in years)\n58 [40 to 79]\n\n\nRACE_ETH (Race/Ethnicity)\n\n\n\n\n    NH_White\n333 (33%)\n\n\n    Hispanic\n255 (26%)\n\n\n    NH_Black\n229 (23%)\n\n\n    NH_Asian\n145 (15%)\n\n\n    Other\n37 (3.7%)\n\n\nWAIST (circumference in cm)\n101 [67 to 159]\n\n\n    Unknown\n44\n\n\nSROH (Self-Reported Overall Health)\n\n\n\n\n    Excellent\n84 (8.9%)\n\n\n    Very_Good\n223 (24%)\n\n\n    Good\n369 (39%)\n\n\n    Fair\n231 (24%)\n\n\n    Poor\n36 (3.8%)\n\n\n    Unknown\n56\n\n\nSEX\n\n\n\n\n    F\n540 (54%)\n\n\n    M\n459 (46%)\n\n\nCYCLE (NHANES reporting)\n\n\n\n\n    2015-16\n475 (48%)\n\n\n    2017-18\n524 (52%)\n\n\n\n1 Median [Min to Max]; n (%)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese summaries come from the naniar package.\n\n\n\n\nCode\ngg_miss_var(nh_demo)\n\n\n\n\n\n\n\n\n\nCode\nmiss_var_summary(nh_demo)\n\n\n# A tibble: 9 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 SROH         56     5.61\n2 WAIST        44     4.40\n3 SEQN          0     0   \n4 HDL           0     0   \n5 HDL_RISK      0     0   \n6 AGE           0     0   \n7 RACE_ETH      0     0   \n8 SEX           0     0   \n9 CYCLE         0     0   \n\n\nCode\nmiss_case_table(nh_demo)\n\n\n# A tibble: 3 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0     912     91.3 \n2              1      74      7.41\n3              2      13      1.30"
  },
  {
    "objectID": "432_projectA_demo.html#my-first-research-question",
    "href": "432_projectA_demo.html#my-first-research-question",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "6.1 My First Research Question",
    "text": "6.1 My First Research Question\nHow effectively can we predict HDL cholesterol levels using age, sex, race/ethnicity, waist circumference and self-reported overall health, in a sample of 999 NHANES participants ages 40-79?"
  },
  {
    "objectID": "432_projectA_demo.html#my-quantitative-outcome",
    "href": "432_projectA_demo.html#my-quantitative-outcome",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "6.2 My Quantitative Outcome",
    "text": "6.2 My Quantitative Outcome\n\nMy quantitative outcome is HDL, and I am interested in predicting this value using several more easily gathered characteristics of a subject.\nI have complete HDL data for all 999 subjects in my nh_demo tibble.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that if for some reason I did not have complete data in my main tibble on this quantitative outcome, I would filter my data here to include only those cases with complete data on this linear model outcome, before preceding with the rest of section 6.\n\n\n\nMy HDL data includes 86 different values, all measured in mg/dl.\nThe distribution of HDL across the 999 subjects in my nh_demo data (shown in the Figure below) is a bit right skewed, with a median of 51, and ranging from 22 mg/dl to 149 mg/dl.\n\n\n\nCode\np1 &lt;- ggplot(nh_demo, aes(sample = HDL)) +\n  geom_qq(col = \"navy\") + geom_qq_line(col = \"red\") + \n  labs(title = \"Normal Q-Q plot of HDL\", x = \"\",\n       y = \"HDL Cholesterol Level (mg/dl)\")\n\np2 &lt;- ggplot(nh_demo, aes(x = HDL)) +\n  geom_histogram(binwidth = 5, col = \"white\", fill = \"navy\") +\n  labs(title = \"Histogram of HDL\", x = \"HDL Cholesterol Level (mg/dl)\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n6.2.1 Numerical Summary of my linear outcome\n\n\n\n\n\n\nA new request\n\n\n\nThis request is new. I want to see the following results for your outcome here, and if your main tibble includes some missing values of your linear outcome, again these should be filtered away in Section 6.\n\nThe Hmisc::describe() results provide the number of observations, missing values (which should be 0 here) and the number of distinct values.\nThe mosaic::favstats() results include the standard deviation, which is not in Hmisc::describe().\nThe tabyl() results show the five most common HDL values, so that you can check that none of them occur more than 10% of the time.\n\nDr. Love will let you use an outcome where one value exceeds 10% of your data, but only if it doesn’t exceed 20% of your data, and the second most common value doesn’t exceed 10% of your data.\n\n\n\n\nHere are brief numerical summaries of my outcome.\n\n\nCode\nHmisc::describe(nh_demo$HDL)\n\n\nnh_demo$HDL \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     999        0       86    0.999     53.6     52.5     18.3       31 \n     .10      .25      .50      .75      .90      .95 \n      35       41       51       63       76       85 \n\nlowest :  22  23  24  25  26, highest: 108 111 112 137 149\n\n\nCode\nmosaic::favstats(nh_demo$HDL)\n\n\n min Q1 median Q3 max    mean       sd   n missing\n  22 41     51 63 149 53.6046 16.69348 999       0\n\n\nCode\nnh_demo |&gt; tabyl(HDL) |&gt; adorn_pct_formatting() |&gt; arrange(desc(n)) |&gt; head(5)\n\n\n HDL  n percent\n  41 48    4.8%\n  48 37    3.7%\n  43 30    3.0%\n  45 30    3.0%\n  49 29    2.9%\n\n\n\n\n6.2.2 Summary Statements about my outcome, HDL.\nMy outcome is HDL.\n\nI have 999 observations and no missing values on HDL.\nI have 86 distinct HDL values.\nThe range is 22 to 149, with mean 53.6 and standard deviation 16.7 mg/dl.\nThe most common value for HDL is 41, which occurs in 4.8% of my subjects."
  },
  {
    "objectID": "432_projectA_demo.html#my-planned-predictors-linear-model",
    "href": "432_projectA_demo.html#my-planned-predictors-linear-model",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "6.3 My Planned Predictors (Linear Model)",
    "text": "6.3 My Planned Predictors (Linear Model)\nThe predictors I intend to use in my linear model are AGE and WAIST, which are quantitative, SEX, which is a binary categorical variable, and RACE_ETH and SROH, each of which are 5-category variables treated as factors in my nh_demo tibble.\n\nAGE has 40 distinct values, and is measured in years.\nWAIST has 483 distinct values, and is measured in centimeters.\nSEX has two levels, with nrow(nh_demo |&gt; filter(SEX == \"F\")) female and nrow(nh_demo |&gt; filter(SEX == \"M\")) male subjects.\nRACE_ETH’s five categories each have at least 30 observations in each level (actually they each have 37 or more), and\nSROH’s five categories also have at least 30 observations (actually 36 or more) in each level, as we can see in the table below. SROH also has 56 missing values.\n\n\n\nCode\nnh_demo |&gt; tabyl(RACE_ETH, SROH) |&gt; adorn_totals(where = c(\"row\", \"col\"))\n\n\n RACE_ETH Excellent Very_Good Good Fair Poor NA_ Total\n NH_White        25       101  121   62   14  10   333\n Hispanic        19        38   87   90    8  13   255\n NH_Black        13        46   95   58    7  10   229\n NH_Asian        25        32   48   16    3  21   145\n    Other         2         6   18    5    4   2    37\n    Total        84       223  369  231   36  56   999\n\n\nThese five predictors are fewer than the allowed maximum of \\(4 + (999 - 100)/100 = 12.999\\), rounding to 12 predictors as prescribed by the Project A instructions, given our 999 observations in nh_demo.\n\n6.3.1 Anticipated Direction of Effects\nI expect Higher HDL to be associated with lower age, with lower waist circumference, with being female, with reporting better self-reported overall health, and with non-hispanic white ethnicity.\n\n\n6.3.2 Missingness Summary\n\n\n\n\n\n\nA new request\n\n\n\nThis request is also new. I want to see the following summary of missing values in your covariates here, after filtering to complete cases on your quantitative outcome variable.\nI’m hoping that you’ll have complete data for all predictors on more than 60% of your observations, and that you won’t be missing more than 20% of any individual predictor.\n\n\nHere is a report on missing data in my predictors for the linear model.\n\n\nCode\nlinear_model_predictors &lt;- nh_demo |&gt; select(AGE, WAIST, SEX, RACE_ETH, SROH)\n\nmiss_var_summary(linear_model_predictors) |&gt; filter(n_miss &gt; 0)\n\n\n# A tibble: 2 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 SROH         56     5.61\n2 WAIST        44     4.40\n\n\nCode\nmiss_case_table(linear_model_predictors)\n\n\n# A tibble: 3 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0     912     91.3 \n2              1      74      7.41\n3              2      13      1.30\n\n\n\nI have complete data for all linear model predictors in 912 (91.3%) of the 999 rows in my data.\nI am missing 56 values (5.6%) for SROH, and 44 values (4.4%) for WAIST."
  },
  {
    "objectID": "432_projectA_demo.html#my-second-research-question",
    "href": "432_projectA_demo.html#my-second-research-question",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "7.1 My Second Research Question",
    "text": "7.1 My Second Research Question\nHow effectively can we predict whether or not a subject’s HDL cholesterol level is at risk (too low) using age, race/ethnicity, waist circumference and self-reported overall health, in a sample of 999 NHANES participants ages 40-79?"
  },
  {
    "objectID": "432_projectA_demo.html#my-binary-outcome",
    "href": "432_projectA_demo.html#my-binary-outcome",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "7.2 My Binary Outcome",
    "text": "7.2 My Binary Outcome\n\nMy binary outcome is HDL_RISK, and I am interested in predicting this value using several more easily gathered characteristics of a subject.\nI have complete HDL_RISK data for all 999 subjects in my nh_demo tibble.\n\n\n\nCode\nnh_demo |&gt; tabyl(HDL_RISK) |&gt; adorn_pct_formatting()\n\n\n HDL_RISK   n percent\n        0 666   66.7%\n        1 333   33.3%\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that if for some reason I did not have complete data in my main tibble on this binary outcome, I would filter my data here to include only those cases with complete data on this logistic model outcome, before preceding with the rest of section 7. Then I would display the tabyl() above for my binary outcome."
  },
  {
    "objectID": "432_projectA_demo.html#my-planned-predictors-logistic-model",
    "href": "432_projectA_demo.html#my-planned-predictors-logistic-model",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "7.3 My Planned Predictors (Logistic Model)",
    "text": "7.3 My Planned Predictors (Logistic Model)\nI am again using four of the five predictors I used in my linear model, specifically age, waist circumference, self-reported overall health and race/ethnicity.\n\nNote that I am not including SEX in this model (although I did in my linear model) because my binary outcome variable’s cutoff values are stratified by SEX.\n\nI have 333 observations in my smaller outcome group (those at risk), and so I am permitted to have up to \\(4 + (333-100)/100 = 6.33\\), rounded to 6 predictors, so I’m within the acceptable limit prescribed by the Project A instructions.\n\n7.3.1 Anticipated Direction of Effects\nI expect higher rates of HDL risk to be associated with higher age, with higher waist circumference, with reporting worse self-reported overall health, and with Hispanic or non-Hispanic Black ethnicity.\n\n\n7.3.2 Missingness Summary\n\n\n\n\n\n\nA new request\n\n\n\nThis request is also new. I want to see the following summary of missing values in your covariates here, after filtering to complete cases on your binary outcome variable.\nI’m hoping that you’ll have complete data for all predictors on more than 60% of your observations, and that you won’t be missing more than 20% of any individual predictor.\n\n\nHere is a report on missing data in my predictors for the logistic model.\n\n\nCode\nlogistic_model_predictors &lt;- nh_demo |&gt; select(AGE, WAIST, RACE_ETH, SROH)\n\nmiss_var_summary(logistic_model_predictors) |&gt; filter(n_miss &gt; 0)\n\n\n# A tibble: 2 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 SROH         56     5.61\n2 WAIST        44     4.40\n\n\nCode\nmiss_case_table(logistic_model_predictors)\n\n\n# A tibble: 3 × 3\n  n_miss_in_case n_cases pct_cases\n           &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1              0     912     91.3 \n2              1      74      7.41\n3              2      13      1.30\n\n\n\nI have complete data for all of the logistic model predictors in 912 (91.3%) of the 999 rows in my data.\nI am missing 56 values (5.6%) for SROH, and 44 values (4.4%) for WAIST."
  },
  {
    "objectID": "432_projectA_demo.html#missingness-1",
    "href": "432_projectA_demo.html#missingness-1",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.1 Missingness",
    "text": "8.1 Missingness\nI will assume missing values are missing at random (MAR) in this work, and use single imputation for my analyses.\nHere’s a table of missingness before imputation. I have missing data on two of my predictors, SROH and WAIST.\n\n\nCode\nmiss_var_summary(nh_demo) |&gt; filter(n_miss &gt; 0)\n\n\n# A tibble: 2 × 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 SROH         56     5.61\n2 WAIST        44     4.40\n\n\n\n8.1.1 Single Imputation Approach\nI’ll use the mice package to do my single imputation (m = 1), with a seed set to 432432.\n\n\nCode\nnh_demo_i &lt;- \n  mice(nh_demo, m = 1, seed = 432432, print = FALSE) |&gt;\n  complete() |&gt;\n  tibble()\n\n\nWarning: Number of logged events: 3\n\n\nCode\nn_miss(nh_demo_i)\n\n\n[1] 0\n\n\nWe needn’t fear logged events in this context."
  },
  {
    "objectID": "432_projectA_demo.html#outcome-transformation",
    "href": "432_projectA_demo.html#outcome-transformation",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.2 Outcome Transformation",
    "text": "8.2 Outcome Transformation\nLet’s look at the Box-Cox results from the car package.\n\n\nCode\nmod_temp &lt;- lm(HDL ~ AGE + SEX + RACE_ETH + WAIST + SROH, data = nh_demo_i)\n\nboxCox(mod_temp)\n\n\n\n\n\n\n\n\n\nThe Box-Cox plot suggests that we take the logarithm of HDL before fitting our model, and so we shall. Here’s a plot of the distribution of the natural logarithm of HDL.\n\n\nCode\nnh_demo_i &lt;- nh_demo_i |&gt;\n  mutate(logHDL = log(HDL))\n\np1 &lt;- ggplot(nh_demo_i, aes(sample = logHDL)) +\n  geom_qq(col = \"navy\") + geom_qq_line(col = \"red\") + \n  labs(title = \"Normal Q-Q plot of log(HDL)\", x = \"\",\n       y = \"Log of HDL Cholesterol Level (mg/dl)\")\n\np2 &lt;- ggplot(nh_demo_i, aes(x = logHDL)) +\n  geom_histogram(bins = 20, col = \"white\", fill = \"navy\") +\n  labs(title = \"Histogram of log(HDL)\", x = \"Log of HDL Cholesterol Level (mg/dl)\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nIt does seem that this transformed outcome follows something closer to a Normal distribution."
  },
  {
    "objectID": "432_projectA_demo.html#scatterplot-matrix-and-collinearity",
    "href": "432_projectA_demo.html#scatterplot-matrix-and-collinearity",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.3 Scatterplot Matrix and Collinearity",
    "text": "8.3 Scatterplot Matrix and Collinearity\nHere’s a scatterplot matrix of our outcome and predictors after transformation and imputation.\n\n\nCode\nggpairs(nh_demo_i, columns = c(\"AGE\", \"WAIST\", \"SEX\", \n                               \"RACE_ETH\", \"SROH\", \"logHDL\"))\n\n\n\n\n\n\n\n\n\nTo check collinearity in more detail, we can estimate variance inflation factors for our predictors.\n\n\nCode\nmod_A &lt;- lm(logHDL ~ AGE + SEX + RACE_ETH + WAIST + SROH, data = nh_demo_i)\n\ncar::vif(mod_A)\n\n\n             GVIF Df GVIF^(1/(2*Df))\nAGE      1.024684  1        1.012267\nSEX      1.025596  1        1.012717\nRACE_ETH 1.248667  4        1.028148\nWAIST    1.233811  1        1.110770\nSROH     1.182393  4        1.021163\n\n\nWe don’t have any large generalized VIF results here, so we have no meaningful concerns regarding collinearity."
  },
  {
    "objectID": "432_projectA_demo.html#model-a",
    "href": "432_projectA_demo.html#model-a",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.4 Model A",
    "text": "8.4 Model A\n\n8.4.1 Fitting Model A\nHere is the fit using the lm() function.\n\n\nCode\nmod_A &lt;- lm(logHDL ~ AGE + SEX + RACE_ETH + WAIST + SROH, data = nh_demo_i)\n\n\nWe’ll also fit model A with the ols() function from the rms package, even though we won’t actually use this fit for a while.\n\n\nCode\ndd &lt;- datadist(nh_demo_i)\noptions(datadist = \"dd\")\n\nmod_A_ols &lt;- ols(logHDL ~ AGE + SEX + RACE_ETH + WAIST + SROH,\n                 data = nh_demo_i, x = TRUE, y = TRUE)\n\n\n\n\n8.4.2 Coefficient Estimates\n\n\n\n\n\n\nNote\n\n\n\nI’ll show two different approaches (model_parameters() from the easystats ecosystem, and tidy() from the broom package) to summarize the coefficient estimates from mod_A. Either is sufficient for Project A, and there’s no need to show both approaches.\n\n\n\nModel Parameters (Model A)Tidied Coefficient Estimates (Model A)\n\n\n\n\nCode\nmodel_parameters(mod_A, ci = 0.90) |&gt; print_md(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n90% CI\nt(987)\np\n\n\n\n\n(Intercept)\n4.616\n0.078\n(4.489, 4.744)\n59.436\n&lt; .001\n\n\nAGE\n0.003\n7.938e-04\n(0.001, 0.004)\n3.510\n&lt; .001\n\n\nSEX (M)\n-0.166\n0.017\n(-0.194, -0.139)\n-10.010\n&lt; .001\n\n\nRACE ETH (Hispanic)\n-0.042\n0.022\n(-0.078, -0.005)\n-1.873\n0.061\n\n\nRACE ETH (NH_Black)\n0.090\n0.022\n(0.053, 0.127)\n4.025\n&lt; .001\n\n\nRACE ETH (NH_Asian)\n-0.068\n0.028\n(-0.114, -0.023)\n-2.479\n0.013\n\n\nRACE ETH (Other)\n0.013\n0.045\n(-0.062, 0.088)\n0.289\n0.773\n\n\nWAIST\n-0.007\n5.665e-04\n(-0.008, -0.006)\n-12.257\n&lt; .001\n\n\nSROH (Very_Good)\n-0.008\n0.032\n(-0.061, 0.045)\n-0.243\n0.808\n\n\nSROH (Good)\n-0.074\n0.031\n(-0.125, -0.023)\n-2.392\n0.017\n\n\nSROH (Fair)\n-0.088\n0.033\n(-0.143, -0.033)\n-2.640\n0.008\n\n\nSROH (Poor)\n-0.023\n0.050\n(-0.106, 0.059)\n-0.466\n0.642\n\n\n\n\n\n\n\n\n\nCode\ntidy(mod_A, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, low90 = conf.low, \n         high90 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n4.616\n0.078\n4.489\n4.744\n0.000\n\n\nAGE\n0.003\n0.001\n0.001\n0.004\n0.000\n\n\nSEXM\n-0.166\n0.017\n-0.194\n-0.139\n0.000\n\n\nRACE_ETHHispanic\n-0.042\n0.022\n-0.078\n-0.005\n0.061\n\n\nRACE_ETHNH_Black\n0.090\n0.022\n0.053\n0.127\n0.000\n\n\nRACE_ETHNH_Asian\n-0.068\n0.028\n-0.114\n-0.023\n0.013\n\n\nRACE_ETHOther\n0.013\n0.045\n-0.062\n0.088\n0.773\n\n\nWAIST\n-0.007\n0.001\n-0.008\n-0.006\n0.000\n\n\nSROHVery_Good\n-0.008\n0.032\n-0.061\n0.045\n0.808\n\n\nSROHGood\n-0.074\n0.031\n-0.125\n-0.023\n0.017\n\n\nSROHFair\n-0.088\n0.033\n-0.143\n-0.033\n0.008\n\n\nSROHPoor\n-0.023\n0.050\n-0.106\n0.059\n0.642\n\n\n\n\n\n\n\n\n\n\n8.4.3 Model A Effects\nSince we’ve built the mod_A_ols model using the rms package, it can be helpful to look at the effects using its plot and associated table, as shown below.\n\n\n\n\n\n\nNote\n\n\n\nIf you look at the Quarto code, you’ll see that I’ve used warning: false in this code chunk to suppress some un-interesting warnings here, and in several other code chunks in this document. I encourage you to do the same in your Project A, for the code chunks that I’ve used warning: false in this demo.\n\nDon’t use warning: false otherwise in Project A, though.\n\n\n\n\n\nCode\nplot(summary(mod_A_ols, conf.int = 0.90))\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod_A_ols, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nAGE\n50\n66.0\n16.0\n0.045\n0.013\n0.024\n0.065\n1\n\n\nWAIST\n92\n112.6\n20.6\n-0.143\n0.012\n-0.162\n-0.124\n1\n\n\nSEX - M:F\n1\n2.0\nNA\n-0.166\n0.017\n-0.194\n-0.139\n1\n\n\nRACE_ETH - Hispanic:NH_White\n1\n2.0\nNA\n-0.042\n0.022\n-0.078\n-0.005\n1\n\n\nRACE_ETH - NH_Black:NH_White\n1\n3.0\nNA\n0.090\n0.022\n0.053\n0.127\n1\n\n\nRACE_ETH - NH_Asian:NH_White\n1\n4.0\nNA\n-0.068\n0.028\n-0.114\n-0.023\n1\n\n\nRACE_ETH - Other:NH_White\n1\n5.0\nNA\n0.013\n0.045\n-0.062\n0.088\n1\n\n\nSROH - Excellent:Good\n3\n1.0\nNA\n0.074\n0.031\n0.023\n0.125\n1\n\n\nSROH - Very_Good:Good\n3\n2.0\nNA\n0.066\n0.022\n0.030\n0.102\n1\n\n\nSROH - Fair:Good\n3\n4.0\nNA\n-0.014\n0.022\n-0.050\n0.021\n1\n\n\nSROH - Poor:Good\n3\n5.0\nNA\n0.050\n0.043\n-0.021\n0.122\n1\n\n\n\n\n\n\n\n8.4.4 Quality of Fit Summaries\n\n\n\n\n\n\nNote\n\n\n\nI’ll show two different approaches (model_performance() from the easystats ecosystem, and glance() from the broom package) to summarize the quality of fit measures describing mod_A. I actually like being able to see both of these approaches, since each provides some unique information, so I would encourage you, too, to include both in your Project A.\n\n\n\nModel A PerformanceModel A at a glance()\n\n\n\n\nCode\nmodel_performance(mod_A) |&gt; print_md(digits = 3)\n\n\n\nIndices of model performance\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n146.355\n146.724\n210.143\n0.273\n0.265\n0.257\n0.259\n\n\n\n\n\n\n\nIn our Model A, we use 11 degrees of freedom, and obtain an \\(R^2\\) value of 0.273.\n\n\nCode\nglance(mod_A) |&gt;\n  select(r2 = r.squared, adjr2 = adj.r.squared, sigma, \n         AIC, BIC, nobs, df, df.residual) |&gt;\n  kable(digits = c(3, 3, 2, 1, 1, 0, 0, 0))\n\n\n\n\n\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.residual\n\n\n\n\n0.273\n0.265\n0.26\n146.4\n210.1\n999\n11\n987\n\n\n\n\n\n\n\n\n\n\n8.4.5 Regression Diagnostics for Model A\n\n\n\n\n\n\nNote\n\n\n\nNote in the Quarto code for the next chunk that I have set the fig-height to 9, so that the plots are easier to read. Please do that, too, whenever you use check_model() from the easystats ecosystem.\n\nI also prefer that you run check_model() with detrend = FALSE for the Normal Q-Q plot of residuals, as done here.\n\n\n\n\n\nCode\ncheck_model(mod_A, detrend = FALSE)\n\n\n\n\n\n\n\n\n\nFor the most part, these plots look very reasonable. I see no clear problems with the assumptions of linearity, normality or constant variance evident in any of these results.\nThe main issue is the posterior predictive check, where our predictions are missing near the center of the distribution a bit, with more predicted values of log(HDL) in the 3.75 to 4.25 range than we see in the original data."
  },
  {
    "objectID": "432_projectA_demo.html#non-linearity-and-spearman-rho2",
    "href": "432_projectA_demo.html#non-linearity-and-spearman-rho2",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.5 Non-Linearity and Spearman \\(\\rho^2\\)",
    "text": "8.5 Non-Linearity and Spearman \\(\\rho^2\\)\nHere’s the relevant Spearman \\(\\rho^2\\) plot, as a place to look for sensible places to consider a non-linear term or terms.\n\n\nCode\nplot(spearman2(logHDL ~ AGE + WAIST + SEX + RACE_ETH + SROH, \n               data = nh_demo_i))\n\n\n\n\n\n\n\n\n\nOur Spearman \\(\\rho^2\\) plot first suggests the use of a non-linear term in WAIST, so we’ll add a restricted cubic spline in WAIST using 5 knots, which should add 3 degrees of freedom to our initial model.\nNext, the SEX variable also seems to be a good choice, so we’ll add an interaction between SEX and the main effect of WAIST, which will add one more degree of freedom to our model A."
  },
  {
    "objectID": "432_projectA_demo.html#model-b",
    "href": "432_projectA_demo.html#model-b",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.6 Model B",
    "text": "8.6 Model B\nOur Model B will add two non-linear terms, summing up to 4 additional degrees of freedom, to our Model A.\n\n8.6.1 Fitting Model B\n\n\nCode\nmod_B &lt;- lm(logHDL ~ AGE + rcs(WAIST, 5) + SEX + \n              WAIST %ia% SEX + RACE_ETH + SROH,\n            data = nh_demo_i)\n\n\nWe’ll also fit model B with the ols() function from the rms package.\n\n\nCode\ndd &lt;- datadist(nh_demo_i)\noptions(datadist = \"dd\")\n\nmod_B_ols &lt;- ols(logHDL ~ AGE + rcs(WAIST, 5) + SEX + \n              WAIST %ia% SEX + RACE_ETH + SROH,\n            data = nh_demo_i, x = TRUE, y = TRUE)\n\n\n\n\n8.6.2 Coefficient Estimates\n\n\n\n\n\n\nNote\n\n\n\nI’ll show two different approaches (model_parameters() from the easystats ecosystem, and tidy() from the broom package) to summarize the coefficient estimates from mod_B. Either is sufficient for Project A, and there’s no need to show both approaches. The tidy() approach handles the non-linear terms a bit better in my view, so I’d probably go with that.\n\n\n\nModel Parameters (Model B)Tidied Coefficient Estimates (Model B)\n\n\n\n\nCode\nmodel_parameters(mod_B, ci = 0.90) |&gt; print_md(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n90% CI\nt(983)\np\n\n\n\n\n(Intercept)\n5.436\n0.252\n(5.021, 5.852)\n21.549\n&lt; .001\n\n\nAGE\n0.003\n7.920e-04\n(0.002, 0.005)\n4.096\n&lt; .001\n\n\nrcs(WAIST ( degree)\n-0.017\n0.003\n(-0.022, -0.012)\n-5.617\n&lt; .001\n\n\nrcs(WAIST ( degree)\n0.039\n0.018\n(0.009, 0.068)\n2.151\n0.032\n\n\nrcs(WAIST ( degree)\n-0.158\n0.095\n(-0.314, -0.001)\n-1.661\n0.097\n\n\nrcs(WAIST ( degree)\n0.188\n0.132\n(-0.030, 0.406)\n1.422\n0.155\n\n\nSEX (M)\n-0.107\n0.111\n(-0.291, 0.076)\n-0.961\n0.337\n\n\nWAIST %ia% SEX\n-5.032e-04\n0.001\n(-0.002, 0.001)\n-0.473\n0.637\n\n\nRACE ETH (Hispanic)\n-0.034\n0.022\n(-0.070, 0.002)\n-1.548\n0.122\n\n\nRACE ETH (NH_Black)\n0.088\n0.022\n(0.051, 0.125)\n3.911\n&lt; .001\n\n\nRACE ETH (NH_Asian)\n-0.075\n0.028\n(-0.121, -0.030)\n-2.735\n0.006\n\n\nRACE ETH (Other)\n0.010\n0.045\n(-0.064, 0.084)\n0.223\n0.823\n\n\nSROH (Very_Good)\n-0.001\n0.032\n(-0.055, 0.052)\n-0.045\n0.964\n\n\nSROH (Good)\n-0.063\n0.031\n(-0.114, -0.013)\n-2.068\n0.039\n\n\nSROH (Fair)\n-0.087\n0.033\n(-0.142, -0.033)\n-2.638\n0.008\n\n\nSROH (Poor)\n-0.031\n0.050\n(-0.113, 0.051)\n-0.627\n0.531\n\n\n\n\n\n\n\n\n\nCode\ntidy(mod_B, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, \n         p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n5.436\n0.252\n5.021\n5.852\n0.000\n\n\nAGE\n0.003\n0.001\n0.002\n0.005\n0.000\n\n\nrcs(WAIST, 5)WAIST\n-0.017\n0.003\n-0.022\n-0.012\n0.000\n\n\nrcs(WAIST, 5)WAIST’\n0.039\n0.018\n0.009\n0.068\n0.032\n\n\nrcs(WAIST, 5)WAIST’’\n-0.158\n0.095\n-0.314\n-0.001\n0.097\n\n\nrcs(WAIST, 5)WAIST’’’\n0.188\n0.132\n-0.030\n0.406\n0.155\n\n\nSEXM\n-0.107\n0.111\n-0.291\n0.076\n0.337\n\n\nWAIST %ia% SEX\n-0.001\n0.001\n-0.002\n0.001\n0.637\n\n\nRACE_ETHHispanic\n-0.034\n0.022\n-0.070\n0.002\n0.122\n\n\nRACE_ETHNH_Black\n0.088\n0.022\n0.051\n0.125\n0.000\n\n\nRACE_ETHNH_Asian\n-0.075\n0.028\n-0.121\n-0.030\n0.006\n\n\nRACE_ETHOther\n0.010\n0.045\n-0.064\n0.084\n0.823\n\n\nSROHVery_Good\n-0.001\n0.032\n-0.055\n0.052\n0.964\n\n\nSROHGood\n-0.063\n0.031\n-0.114\n-0.013\n0.039\n\n\nSROHFair\n-0.087\n0.033\n-0.142\n-0.033\n0.008\n\n\nSROHPoor\n-0.031\n0.050\n-0.113\n0.051\n0.531\n\n\n\n\n\n\n\n\n\n\n8.6.3 Model B Effects\nHere, we use the mod_B_ols model to look at the effects using its plot and associated table, which may be especially helpful when we include non-linear terms.\n\n\n\n\n\n\nNote\n\n\n\nI’ve used warning: false in this code chunk to suppress some un-interesting warnings. You can (for this type of plot) too, if you need to, in your Project A.\n\n\n\n\nCode\nplot(summary(mod_B_ols, conf.int = 0.90))\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod_B_ols, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nAGE\n50\n66.0\n16.0\n0.052\n0.013\n0.031\n0.073\n1\n\n\nWAIST\n92\n112.6\n20.6\n-0.133\n0.026\n-0.175\n-0.091\n1\n\n\nSEX - M:F\n1\n2.0\nNA\n-0.158\n0.017\n-0.186\n-0.131\n1\n\n\nRACE_ETH - Hispanic:NH_White\n1\n2.0\nNA\n-0.034\n0.022\n-0.070\n0.002\n1\n\n\nRACE_ETH - NH_Black:NH_White\n1\n3.0\nNA\n0.088\n0.022\n0.051\n0.125\n1\n\n\nRACE_ETH - NH_Asian:NH_White\n1\n4.0\nNA\n-0.075\n0.028\n-0.121\n-0.030\n1\n\n\nRACE_ETH - Other:NH_White\n1\n5.0\nNA\n0.010\n0.045\n-0.064\n0.084\n1\n\n\nSROH - Excellent:Good\n3\n1.0\nNA\n0.063\n0.031\n0.013\n0.114\n1\n\n\nSROH - Very_Good:Good\n3\n2.0\nNA\n0.062\n0.022\n0.027\n0.097\n1\n\n\nSROH - Fair:Good\n3\n4.0\nNA\n-0.024\n0.022\n-0.059\n0.012\n1\n\n\nSROH - Poor:Good\n3\n5.0\nNA\n0.032\n0.043\n-0.039\n0.103\n1\n\n\n\n\n\n\n\n8.6.4 Quality of Fit Summaries\n\nModel B PerformanceModel B at a glance()\n\n\n\n\nCode\nmodel_performance(mod_B) |&gt; print_md(digits = 3)\n\n\n\nIndices of model performance\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n130.829\n131.453\n214.244\n0.290\n0.279\n0.254\n0.256\n\n\n\n\n\n\n\nIn our Model B, we use 15 degrees of freedom, and obtain an \\(R^2\\) value of 0.29.\n\n\nCode\nglance(mod_B) |&gt;\n  select(r2 = r.squared, adjr2 = adj.r.squared, sigma, \n         AIC, BIC, nobs, df, df.residual) |&gt;\n  kable(digits = c(3, 3, 2, 1, 1, 0, 0, 0))\n\n\n\n\n\nr2\nadjr2\nsigma\nAIC\nBIC\nnobs\ndf\ndf.residual\n\n\n\n\n0.29\n0.279\n0.26\n130.8\n214.2\n999\n15\n983\n\n\n\n\n\n\n\n\n\n\n8.6.5 Regression Diagnostics for Model B\n\n\nCode\ncheck_model(mod_B, detrend = FALSE)\n\n\n\n\n\n\n\n\n\nThese residual plots also look pretty reasonable, too. Again, I see no clear problems with the assumptions of linearity, normality or constant variance evident in these results. The posterior predictive check is a little better than Model A, but not much. The collinearity we’ve introduced here is due to the interaction terms, so that’s not a concern for us."
  },
  {
    "objectID": "432_projectA_demo.html#validating-models-a-and-b",
    "href": "432_projectA_demo.html#validating-models-a-and-b",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.7 Validating Models A and B",
    "text": "8.7 Validating Models A and B\nWe will use the validate() function from the rms package to validate our ols fits.\n\n\nCode\nset.seed(4321); (valA &lt;- validate(mod_A_ols))\n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.2729   0.2800 0.2630   0.0170          0.2558 40\nMSE           0.0660   0.0656 0.0669  -0.0014          0.0674 40\ng             0.1787   0.1811 0.1759   0.0052          0.1736 40\nIntercept     0.0000   0.0000 0.1209  -0.1209          0.1209 40\nSlope         1.0000   1.0000 0.9689   0.0311          0.9689 40\n\n\nCode\nset.seed(4322); (valB &lt;- validate(mod_B_ols))\n\n\n          index.orig training   test optimism index.corrected  n\nR-square      0.2898   0.3056 0.2790   0.0267          0.2631 40\nMSE           0.0645   0.0626 0.0655  -0.0029          0.0674 40\ng             0.1827   0.1867 0.1796   0.0072          0.1755 40\nIntercept     0.0000   0.0000 0.1501  -0.1501          0.1501 40\nSlope         1.0000   1.0000 0.9620   0.0380          0.9620 40\n\n\n\n8.7.1 Validated \\(R^2\\), and MSE as well as IC statistics\n\n\n\n\n\n\nNote\n\n\n\nIf you inspect the Quarto code for the table below, you’ll see that I have used in-line coding to specify the values of each element. That’s definitely worth considering in building your work, although it takes some care.\n\n\n\n\n\nModel\nValidated \\(R^2\\)\nValidated MSE\nAIC\nBIC\ndf\n\n\n\n\nA\n0.256\n0.0674\n146.4\n210.1\n11\n\n\nB\n0.263\n0.0674\n130.8\n214.2\n15"
  },
  {
    "objectID": "432_projectA_demo.html#final-linear-regression-model",
    "href": "432_projectA_demo.html#final-linear-regression-model",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "8.8 Final Linear Regression Model",
    "text": "8.8 Final Linear Regression Model\nWe’ll choose Model B here.\nWe see a small improvement for Model B in terms of validated \\(R^2\\) and AIC. We also note that BIC is a little lower (thus better) for Model A, and there’s no material difference in validated mean squared error. Each of the models matches the assumptions of linear regression well, so there’s not much to choose from in that regard. Really, we could pick either model here, but I wanted to pick a model with non-linear terms so that I could display and discuss them in what follows.\n\n8.8.1 Winning Model’s OLS summary\n\n\nCode\nmod_B_ols\n\n\nLinear Regression Model\n\nols(formula = logHDL ~ AGE + rcs(WAIST, 5) + SEX + WAIST %ia% \n    SEX + RACE_ETH + SROH, data = nh_demo_i, x = TRUE, y = TRUE)\n\n                Model Likelihood    Discrimination    \n                      Ratio Test           Indexes    \nObs     999    LR chi2    341.85    R2       0.290    \nsigma0.2560    d.f.           15    R2 adj   0.279    \nd.f.    983    Pr(&gt; chi2) 0.0000    g        0.183    \n\nResiduals\n\n      Min        1Q    Median        3Q       Max \n-0.763472 -0.183023 -0.002842  0.171374  0.916543 \n\n                  Coef    S.E.   t     Pr(&gt;|t|)\nIntercept          5.4362 0.2523 21.55 &lt;0.0001 \nAGE                0.0032 0.0008  4.10 &lt;0.0001 \nWAIST             -0.0170 0.0030 -5.62 &lt;0.0001 \nWAIST'             0.0387 0.0180  2.15 0.0317  \nWAIST''           -0.1579 0.0951 -1.66 0.0971  \nWAIST'''           0.1881 0.1323  1.42 0.1552  \nSEX=M             -0.1071 0.1115 -0.96 0.3370  \nWAIST * SEX=M     -0.0005 0.0011 -0.47 0.6367  \nRACE_ETH=Hispanic -0.0341 0.0220 -1.55 0.1219  \nRACE_ETH=NH_Black  0.0877 0.0224  3.91 &lt;0.0001 \nRACE_ETH=NH_Asian -0.0754 0.0276 -2.73 0.0064  \nRACE_ETH=Other     0.0100 0.0449  0.22 0.8234  \nSROH=Very_Good    -0.0014 0.0323 -0.04 0.9645  \nSROH=Good         -0.0635 0.0307 -2.07 0.0389  \nSROH=Fair         -0.0873 0.0331 -2.64 0.0085  \nSROH=Poor         -0.0312 0.0498 -0.63 0.5310  \n\n\nAs a reminder, we displayed the validated \\(R^2\\) value ( = 0.263) for our Model B back in Section 8.7.1.\n\n\n8.8.2 Effects Plot for Winning Model\n\n\nCode\nplot(summary(mod_B_ols, conf.int = 0.90))\n\n\n\n\n\n\n\n\n\n\n\n8.8.3 Numerical Description of Effect Sizes\n\n\nCode\nsummary(mod_B_ols, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nAGE\n50\n66.0\n16.0\n0.052\n0.013\n0.031\n0.073\n1\n\n\nWAIST\n92\n112.6\n20.6\n-0.133\n0.026\n-0.175\n-0.091\n1\n\n\nSEX - M:F\n1\n2.0\nNA\n-0.158\n0.017\n-0.186\n-0.131\n1\n\n\nRACE_ETH - Hispanic:NH_White\n1\n2.0\nNA\n-0.034\n0.022\n-0.070\n0.002\n1\n\n\nRACE_ETH - NH_Black:NH_White\n1\n3.0\nNA\n0.088\n0.022\n0.051\n0.125\n1\n\n\nRACE_ETH - NH_Asian:NH_White\n1\n4.0\nNA\n-0.075\n0.028\n-0.121\n-0.030\n1\n\n\nRACE_ETH - Other:NH_White\n1\n5.0\nNA\n0.010\n0.045\n-0.064\n0.084\n1\n\n\nSROH - Excellent:Good\n3\n1.0\nNA\n0.063\n0.031\n0.013\n0.114\n1\n\n\nSROH - Very_Good:Good\n3\n2.0\nNA\n0.062\n0.022\n0.027\n0.097\n1\n\n\nSROH - Fair:Good\n3\n4.0\nNA\n-0.024\n0.022\n-0.059\n0.012\n1\n\n\nSROH - Poor:Good\n3\n5.0\nNA\n0.032\n0.043\n-0.039\n0.103\n1\n\n\n\n\n\n\n\n8.8.4 Effect Size Description(s)\nIn your work, we would only need to see one of the following effect size descriptions.\nWAIST description: If we have two female subjects of the same age, race/ethnicity and self-reported overall health, then if subject 1 has a waist circumference of 92 cm and subject 2 has a waist circumference of 112.6 cm, then our model estimates that subject 1 will have a log(HDL) that is 0.133 higher than subject 2. The 90% confidence interval around that estimated effect on log(HDL) ranges from (0.091, 0.175).\nSEX description: If we have two subjects, each with the same age, race/ethnicity and self-reported overall health and with a waist circumference of 101.7 cm, then if subject 1 is male and subject 2 is female, our model estimates that the female subject will have a log(HDL) that is 0.158 higher (with 90% CI: (0.131, 0.186)) than the male subject.\nAGE description: If we have two subjects of the same sex, waist circumference, race/ethnicity and self-reported overall health, then if subject 1 is age 50 and subject 2 is age 66, our model predicts that subject 1’s log(HDL) will be 0.052 larger than subject 2’s log(HDL), with 90% confidence interval (0.031, 0.073).\nSROH description: If we have two subjects of the same age, sex, waist circumference and race/ethnicity. then if subject 1 reports Excellent overall health while subject 2 reports only Good overall health, our model predicts that subject 1’s log(HDL) will be 0.063 higher than subject 2’s log(HDL), with 90% CI (0.013, 0.114).\n\n\n8.8.5 Prediction Plot for Winning Model\nHere’s the set of prediction plots to describe the impact of the coefficients on log(HDL).\n\n\nCode\nggplot(Predict(mod_B_ols))\n\n\n\n\n\n\n\n\n\n\n\n8.8.6 Nomogram of Winning Model\n\n\nCode\nplot(nomogram(mod_B_ols, fun = exp, funlabel = \"HDL\"))\n\n\n\n\n\n\n\n\n\n\n\n8.8.7 Prediction for a New Subject\nI will create a predicted HDL for a new female Non-Hispanic White subject who is 60 years old, has an 85 cm waist circumference, and rates their overall health as Fair.\n\n\n\n\n\n\nNote\n\n\n\nI can do this using either the ols() or the lm() fit to our model. Either is sufficient for Project A, and there’s no need to show both approaches.\n\n\n\nUsing the lm() fitUsing the ols() fit\n\n\nHere, I’ll actually run two predictions, one for a Male and one for a Female subject with the same values of AGE, WAIST, RACE_ETH and SROH.\n\n\nCode\nnew_subjects &lt;- \n  data.frame(AGE = c(60,60), WAIST = c(85, 85), SEX = c(\"M\", \"F\"),\n             RACE_ETH = c(\"NH_White\", \"NH_White\"), SROH = c(\"Fair\", \"Fair\"))\n\npreds1 &lt;- predict.lm(mod_B, newdata = new_subjects, \n                     interval = \"prediction\", level = 0.90)\n\nexp(preds1)\n\n\n       fit      lwr      upr\n1 52.08776 34.05169 79.67694\n2 60.50899 39.58373 92.49604\n\n\nWe then exponentiate these results to obtain the estimate and 90% prediction interval on the original scale of HDL cholesterol, as shown in the table below.\n\n\n\n\n\n\n\n\nPredictor Values\nPredicted HDL\n90% Prediction Interval\n\n\n\n\nAGE = 60, WAIST = 85, SEX = M, RACE_ETH = NH_White, SROH = FAIR\n52.09 mg/dl\n(34.05, 79.68) mg/dl\n\n\nAGE = 60, WAIST = 85, SEX = F, RACE_ETH = NH_White, SROH = FAIR\n60.51 mg/dl\n(39.58, 92.5) mg/dl\n\n\n\n\n\nAlternatively, I could have used our ols() fit. Here, I’ll just fit the results for the female subject.\n\n\nCode\nnew_subject &lt;- tibble(  AGE = 60, WAIST = 85, SEX = \"F\", \n                       RACE_ETH = \"NH_White\", SROH = \"Fair\" ) \n\npreds2 &lt;- predict(mod_B_ols, newdata = new_subject, \n                 conf.int = 0.90, conf.type = \"individual\")\n\npreds2\n\n\n$linear.predictors\n       1 \n4.102792 \n\n$lower\n       1 \n3.678418 \n\n$upper\n       1 \n4.527166 \n\n\nWe then exponentiate these results to obtain the estimate and 90% prediction interval on the original scale of HDL cholesterol, as shown in the table below.\n\n\n\n\n\n\n\n\nPredictor Values\nPredicted HDL\n90% Prediction Interval\n\n\n\n\nAGE = 60, WAIST = 85, SEX = F, RACE_ETH = NH_White, SROH = FAIR\n60.51 mg/dl\n(39.58, 92.5) mg/dl\n\n\n\nNaturally, the two fitting approaches (lm() and ols()) produce the same model, so they produce the same predictions."
  },
  {
    "objectID": "432_projectA_demo.html#missingness-2",
    "href": "432_projectA_demo.html#missingness-2",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "9.1 Missingness",
    "text": "9.1 Missingness\nAgain, we’ll assume missing values are MAR, and use the single imputation approach developed previously in Section 8.1.1."
  },
  {
    "objectID": "432_projectA_demo.html#model-y",
    "href": "432_projectA_demo.html#model-y",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "9.2 Model Y",
    "text": "9.2 Model Y\nWe’ll predict Pr(HDL_RISK = 1), the probably of having a low enough HDL to put the subject at risk, as a function of AGE, WAIST, RACE_ETH and SROH.\n\n9.2.1 Fitting Model Y\nWe’ll fit our logistic regression model using both glm() and lrm().\n\n\nCode\nmod_Y &lt;- glm(HDL_RISK ~ AGE + WAIST + RACE_ETH + SROH,\n            data = nh_demo_i, family = binomial())\n\nddd &lt;- datadist(nh_demo_i)\noptions(datadist = \"ddd\")\n\nmod_Y_lrm &lt;- lrm(HDL_RISK ~ AGE + WAIST + RACE_ETH + SROH,\n                data = nh_demo_i, x = TRUE, y = TRUE)\n\n\n\n\n9.2.2 Coefficient Estimates\n\n\n\n\n\n\nNote\n\n\n\nI’ll show two different approaches (model_parameters() from the easystats ecosystem, and tidy() from the broom package) to summarize the coefficient estimates from mod_Y. Either is sufficient for Project A, and there’s no need to show both approaches.\n\n\n\nTidied Odds Ratio Estimates (Model Y)Model Y Parameters\n\n\n\n\nCode\ntidy(mod_Y, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n0.011\n0.722\n0.003\n0.036\n0.000\n\n\nAGE\n0.986\n0.007\n0.975\n0.998\n0.048\n\n\nWAIST\n1.041\n0.005\n1.032\n1.050\n0.000\n\n\nRACE_ETHHispanic\n1.004\n0.189\n0.736\n1.369\n0.983\n\n\nRACE_ETHNH_Black\n0.531\n0.203\n0.379\n0.740\n0.002\n\n\nRACE_ETHNH_Asian\n1.607\n0.239\n1.083\n2.380\n0.047\n\n\nRACE_ETHOther\n0.536\n0.420\n0.261\n1.049\n0.138\n\n\nSROHVery_Good\n1.196\n0.318\n0.717\n2.044\n0.574\n\n\nSROHGood\n1.677\n0.300\n1.037\n2.792\n0.085\n\n\nSROHFair\n2.405\n0.315\n1.448\n4.100\n0.005\n\n\nSROHPoor\n1.993\n0.450\n0.949\n4.193\n0.126\n\n\n\n\n\n\n\n\n\nCode\nmodel_parameters(mod_Y, ci = 0.90, exponentiate = TRUE) |&gt;\n  print_md(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n90% CI\nz\np\n\n\n\n\n(Intercept)\n0.011\n0.008\n(0.003, 0.036)\n-6.243\n&lt; .001\n\n\nAGE\n0.986\n0.007\n(0.975, 0.998)\n-1.979\n0.048\n\n\nWAIST\n1.041\n0.005\n(1.032, 1.050)\n7.779\n&lt; .001\n\n\nRACE ETH (Hispanic)\n1.004\n0.189\n(0.736, 1.369)\n0.022\n0.983\n\n\nRACE ETH (NH_Black)\n0.531\n0.108\n(0.379, 0.740)\n-3.114\n0.002\n\n\nRACE ETH (NH_Asian)\n1.607\n0.384\n(1.083, 2.380)\n1.985\n0.047\n\n\nRACE ETH (Other)\n0.536\n0.225\n(0.261, 1.049)\n-1.483\n0.138\n\n\nSROH (Very_Good)\n1.196\n0.380\n(0.717, 2.044)\n0.562\n0.574\n\n\nSROH (Good)\n1.677\n0.503\n(1.037, 2.792)\n1.722\n0.085\n\n\nSROH (Fair)\n2.405\n0.759\n(1.448, 4.100)\n2.782\n0.005\n\n\nSROH (Poor)\n1.993\n0.897\n(0.949, 4.193)\n1.531\n0.126\n\n\n\n\n\n\n\n\n\n\n9.2.3 Model Y Effects\n\n\nCode\nplot(summary(mod_Y_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod_Y_lrm, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nAGE\n50\n66.0\n16.0\n-0.221\n0.112\n-0.404\n-0.037\n1\n\n\nOdds Ratio\n50\n66.0\n16.0\n0.802\nNA\n0.667\n0.963\n2\n\n\nWAIST\n92\n112.6\n20.6\n0.831\n0.107\n0.655\n1.006\n1\n\n\nOdds Ratio\n92\n112.6\n20.6\n2.295\nNA\n1.925\n2.735\n2\n\n\nRACE_ETH - Hispanic:NH_White\n1\n2.0\nNA\n0.004\n0.189\n-0.306\n0.314\n1\n\n\nOdds Ratio\n1\n2.0\nNA\n1.004\nNA\n0.736\n1.369\n2\n\n\nRACE_ETH - NH_Black:NH_White\n1\n3.0\nNA\n-0.633\n0.203\n-0.967\n-0.299\n1\n\n\nOdds Ratio\n1\n3.0\nNA\n0.531\nNA\n0.380\n0.742\n2\n\n\nRACE_ETH - NH_Asian:NH_White\n1\n4.0\nNA\n0.474\n0.239\n0.081\n0.867\n1\n\n\nOdds Ratio\n1\n4.0\nNA\n1.607\nNA\n1.085\n2.381\n2\n\n\nRACE_ETH - Other:NH_White\n1\n5.0\nNA\n-0.624\n0.420\n-1.315\n0.068\n1\n\n\nOdds Ratio\n1\n5.0\nNA\n0.536\nNA\n0.268\n1.070\n2\n\n\nSROH - Excellent:Good\n3\n1.0\nNA\n-0.517\n0.300\n-1.010\n-0.023\n1\n\n\nOdds Ratio\n3\n1.0\nNA\n0.596\nNA\n0.364\n0.977\n2\n\n\nSROH - Very_Good:Good\n3\n2.0\nNA\n-0.338\n0.193\n-0.656\n-0.020\n1\n\n\nOdds Ratio\n3\n2.0\nNA\n0.713\nNA\n0.519\n0.980\n2\n\n\nSROH - Fair:Good\n3\n4.0\nNA\n0.361\n0.180\n0.065\n0.656\n1\n\n\nOdds Ratio\n3\n4.0\nNA\n1.434\nNA\n1.067\n1.928\n2\n\n\nSROH - Poor:Good\n3\n5.0\nNA\n0.173\n0.368\n-0.433\n0.778\n1\n\n\nOdds Ratio\n3\n5.0\nNA\n1.188\nNA\n0.649\n2.177\n2\n\n\n\n\n\n\n\n9.2.4 Quality of Fit Summaries\n\n\n\n\n\n\nNote\n\n\n\nHere, we have three available approaches to summarize the fit of Model Y, thanks to our glm() and lrm() fits of the same model. I’ll show all three, but in practice (and in your Project A) I wouldn’t include the model_performance() results.\n\n\n\nlrm() for Model Yglance at Model YModel Y Performance\n\n\nOur Nagelkerke \\(R^2\\) estimate for Model Y is 0.152, and our C statistic is estimated to be 0.701.\n\n\nCode\nmod_Y_lrm\n\n\nLogistic Regression Model\n\nlrm(formula = HDL_RISK ~ AGE + WAIST + RACE_ETH + SROH, data = nh_demo_i, \n    x = TRUE, y = TRUE)\n\n                       Model Likelihood     Discrimination    Rank Discrim.    \n                             Ratio Test            Indexes          Indexes    \nObs           999    LR chi2     115.50     R2       0.152    C       0.701    \n 0            666    d.f.            10    R2(10,999)0.100    Dxy     0.402    \n 1            333    Pr(&gt; chi2) &lt;0.0001    R2(10,666)0.146    gamma   0.402    \nmax |deriv| 1e-06                           Brier    0.198    tau-a   0.179    \n\n                  Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept         -4.5080 0.7221 -6.24  &lt;0.0001 \nAGE               -0.0138 0.0070 -1.98  0.0478  \nWAIST              0.0403 0.0052  7.78  &lt;0.0001 \nRACE_ETH=Hispanic  0.0041 0.1886  0.02  0.9826  \nRACE_ETH=NH_Black -0.6327 0.2032 -3.11  0.0018  \nRACE_ETH=NH_Asian  0.4743 0.2390  1.98  0.0472  \nRACE_ETH=Other    -0.6235 0.4205 -1.48  0.1381  \nSROH=Very_Good     0.1786 0.3175  0.56  0.5739  \nSROH=Good          0.5168 0.3001  1.72  0.0850  \nSROH=Fair          0.8776 0.3154  2.78  0.0054  \nSROH=Poor          0.6895 0.4503  1.53  0.1258  \n\n\n\n\nThe glance() results below give us our AIC and BIC values, as well as the degrees of freedom used by Model Y.\n\n\nCode\nglance(mod_Y) |&gt;\n  mutate(df = nobs - df.residual - 1) |&gt;\n  select(AIC, BIC, df, df.residual, nobs) |&gt;\n  kable(digits = 1)\n\n\n\n\n\nAIC\nBIC\ndf\ndf.residual\nnobs\n\n\n\n\n1178.3\n1232.2\n10\n988\n999\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese summaries aren’t as appealing to me as those in the other two tabs here.\n\n\n\n\nCode\nmodel_performance(mod_Y) |&gt; print_md(digits = 2)\n\n\n\nIndices of model performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nTjur’s R2\nRMSE\nSigma\nLog_loss\nScore_log\nScore_spherical\nPCP\n\n\n\n\n1178.26\n1178.53\n1232.23\n0.11\n0.44\n1.00\n0.58\n-145.30\n1.81e-03\n0.60\n\n\n\n\n\n\n\n\n\n\n9.2.5 Confusion Matrix (Model Y)\nFirst, we augment our nh_demo_i data to include predicted probabilities of (HDL_RISK = 1) from Model Y.\n\n\nCode\nresY_aug &lt;- augment(mod_Y, type.predict = \"response\")\n\n\nMy prediction rule for this confusion matrix is that the fitted value of Pr(HDL_RISK = 1) needs to be greater than or equal to 0.5 for me to predict HDL_RISK is 1, and otherwise I predict 0.\n\n\nCode\ncm_Y &lt;- caret::confusionMatrix(\n  data = factor(resY_aug$.fitted &gt;= 0.5),\n  reference = factor(resY_aug$HDL_RISK == 1),\n  positive = \"TRUE\")\n\ncm_Y\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   603  241\n     TRUE     63   92\n                                          \n               Accuracy : 0.6957          \n                 95% CI : (0.6661, 0.7241)\n    No Information Rate : 0.6667          \n    P-Value [Acc &gt; NIR] : 0.02722         \n                                          \n                  Kappa : 0.2097          \n                                          \n Mcnemar's Test P-Value : &lt; 2e-16         \n                                          \n            Sensitivity : 0.27628         \n            Specificity : 0.90541         \n         Pos Pred Value : 0.59355         \n         Neg Pred Value : 0.71445         \n             Prevalence : 0.33333         \n         Detection Rate : 0.09209         \n   Detection Prevalence : 0.15516         \n      Balanced Accuracy : 0.59084         \n                                          \n       'Positive' Class : TRUE            \n                                          \n\n\nHere are our results, tabulated nicely.\n\n\n\n\n\n\n\n\n\n\nModel\nClassification Rule\nSensitivity\nSpecificity\nPos. Pred. Value\n\n\n\n\nY\nPredicted Pr(HDL_RISK = 1) &gt;= 0.5\n0.276\n0.905\n0.594"
  },
  {
    "objectID": "432_projectA_demo.html#non-linearity-and-spearman-rho2-plot",
    "href": "432_projectA_demo.html#non-linearity-and-spearman-rho2-plot",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "9.3 Non-Linearity and Spearman \\(\\rho^2\\) plot",
    "text": "9.3 Non-Linearity and Spearman \\(\\rho^2\\) plot\nHere’s the relevant Spearman \\(\\rho^2\\) plot.\n\n\nCode\nplot(spearman2(HDL_RISK ~ AGE + WAIST + SEX + RACE_ETH + SROH, \n               data = nh_demo_i))\n\n\n\n\n\n\n\n\n\nOur Spearman \\(\\rho^2\\) plot first suggests the use of a non-linear term in WAIST, so we’ll add a restricted cubic spline in WAIST using 4 knots, which should add 2 degrees of freedom to our initial model.\nNext, the SROH variable seems to be a good choice, so we’ll add an interaction between SROH and the main effect of WAIST, which will add four more degrees of freedom to our model Y."
  },
  {
    "objectID": "432_projectA_demo.html#model-z",
    "href": "432_projectA_demo.html#model-z",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "9.4 Model Z",
    "text": "9.4 Model Z\nAs mentioned, our model Z will add 6 degrees of freedom through two non-linear terms, to model Y.\n\n9.4.1 Fitting Model Z\nWe’ll fit Model Z with both glm() and lrm().\n\n\nCode\nmod_Z &lt;- glm(HDL_RISK ~ AGE + rcs(WAIST, 4) + RACE_ETH + \n               SROH + WAIST %ia% SROH,\n            data = nh_demo_i, family = binomial())\n\nddd &lt;- datadist(nh_demo_i)\noptions(datadist = \"ddd\")\n\nmod_Z_lrm &lt;- lrm(HDL_RISK ~ AGE + rcs(WAIST, 4) + RACE_ETH + \n                   SROH + WAIST %ia% SROH,\n                 data = nh_demo_i, x = TRUE, y = TRUE)\n\n\n\n\n9.4.2 Coefficient Estimates\n\n\n\n\n\n\nNote\n\n\n\nI’ll show two different approaches (model_parameters() from the easystats ecosystem, and tidy() from the broom package) to summarize the coefficient estimates from mod_Y. Either is sufficient for Project A, and there’s no need to show both approaches. The tidy() approach handles the non-linear terms a bit better in my view, so I’d probably go with that.\n\n\n\nTidied Odds Ratio Estimates (Model Z)Model Z Parameters\n\n\n\n\nCode\ntidy(mod_Z, exponentiate = TRUE, conf.int = TRUE, conf.level = 0.90) |&gt;\n  select(term, estimate, se = std.error, \n         low90 = conf.low, high90 = conf.high, p = p.value) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nse\nlow90\nhigh90\np\n\n\n\n\n(Intercept)\n0.000\n2.905\n0.000\n0.018\n0.003\n\n\nAGE\n0.985\n0.007\n0.973\n0.996\n0.032\n\n\nrcs(WAIST, 4)WAIST\n1.093\n0.032\n1.039\n1.154\n0.005\n\n\nrcs(WAIST, 4)WAIST’\n0.893\n0.074\n0.787\n1.006\n0.128\n\n\nrcs(WAIST, 4)WAIST’’\n1.295\n0.231\n0.890\n1.909\n0.263\n\n\nRACE_ETHHispanic\n0.971\n0.190\n0.710\n1.326\n0.875\n\n\nRACE_ETHNH_Black\n0.525\n0.203\n0.375\n0.732\n0.002\n\n\nRACE_ETHNH_Asian\n1.739\n0.248\n1.157\n2.615\n0.025\n\n\nRACE_ETHOther\n0.541\n0.419\n0.264\n1.056\n0.142\n\n\nSROHVery_Good\n0.630\n2.352\n0.013\n31.942\n0.845\n\n\nSROHGood\n1.151\n2.158\n0.032\n43.328\n0.948\n\n\nSROHFair\n0.437\n2.246\n0.010\n18.809\n0.713\n\n\nSROHPoor\n1.828\n2.837\n0.015\n187.479\n0.832\n\n\nWAIST %ia% SROHWAIST * SROH=Very_Good\n1.006\n0.024\n0.968\n1.047\n0.791\n\n\nWAIST %ia% SROHWAIST * SROH=Good\n1.004\n0.022\n0.968\n1.041\n0.860\n\n\nWAIST %ia% SROHWAIST * SROH=Fair\n1.017\n0.022\n0.979\n1.056\n0.462\n\n\nWAIST %ia% SROHWAIST * SROH=Poor\n1.002\n0.027\n0.959\n1.049\n0.936\n\n\n\n\n\n\n\n\n\nCode\nmodel_parameters(mod_Z, ci = 0.90, exponentiate = TRUE) |&gt;\n  print_md(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n90% CI\nz\np\n\n\n\n\n(Intercept)\n1.765e-04\n5.127e-04\n(1.213e-06, 0.018)\n-2.975\n0.003\n\n\nAGE\n0.985\n0.007\n(0.973, 0.996)\n-2.150\n0.032\n\n\nrcs(WAIST ( degree)\n1.093\n0.035\n(1.039, 1.154)\n2.791\n0.005\n\n\nrcs(WAIST ( degree)\n0.893\n0.066\n(0.787, 1.006)\n-1.523\n0.128\n\n\nrcs(WAIST ( degree)\n1.295\n0.300\n(0.890, 1.909)\n1.118\n0.263\n\n\nRACE ETH (Hispanic)\n0.971\n0.184\n(0.710, 1.326)\n-0.157\n0.875\n\n\nRACE ETH (NH_Black)\n0.525\n0.107\n(0.375, 0.732)\n-3.164\n0.002\n\n\nRACE ETH (NH_Asian)\n1.739\n0.431\n(1.157, 2.615)\n2.236\n0.025\n\n\nRACE ETH (Other)\n0.541\n0.227\n(0.264, 1.056)\n-1.467\n0.142\n\n\nSROH (Very_Good)\n0.630\n1.483\n(0.013, 31.942)\n-0.196\n0.845\n\n\nSROH (Good)\n1.151\n2.485\n(0.032, 43.328)\n0.065\n0.948\n\n\nSROH (Fair)\n0.437\n0.982\n(0.010, 18.809)\n-0.368\n0.713\n\n\nSROH (Poor)\n1.828\n5.185\n(0.015, 187.479)\n0.213\n0.832\n\n\nWAIST %ia% SROHWAIST * SROH=Very Good\n1.006\n0.024\n(0.968, 1.047)\n0.265\n0.791\n\n\nWAIST %ia% SROHWAIST * SROH=Good\n1.004\n0.022\n(0.968, 1.041)\n0.176\n0.860\n\n\nWAIST %ia% SROHWAIST * SROH=Fair\n1.017\n0.023\n(0.979, 1.056)\n0.735\n0.462\n\n\nWAIST %ia% SROHWAIST * SROH=Poor\n1.002\n0.027\n(0.959, 1.049)\n0.081\n0.936\n\n\n\n\n\n\n\n\n\n\n9.4.3 Model Z Effects\n\n\nCode\nplot(summary(mod_Z_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod_Z_lrm, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nAGE\n50\n66.0\n16.0\n-0.243\n0.113\n-0.429\n-0.057\n1\n\n\nOdds Ratio\n50\n66.0\n16.0\n0.784\nNA\n0.651\n0.944\n2\n\n\nWAIST\n92\n112.6\n20.6\n0.769\n0.228\n0.395\n1.144\n1\n\n\nOdds Ratio\n92\n112.6\n20.6\n2.158\nNA\n1.484\n3.140\n2\n\n\nRACE_ETH - Hispanic:NH_White\n1\n2.0\nNA\n-0.030\n0.190\n-0.342\n0.283\n1\n\n\nOdds Ratio\n1\n2.0\nNA\n0.971\nNA\n0.710\n1.327\n2\n\n\nRACE_ETH - NH_Black:NH_White\n1\n3.0\nNA\n-0.644\n0.203\n-0.979\n-0.309\n1\n\n\nOdds Ratio\n1\n3.0\nNA\n0.525\nNA\n0.376\n0.734\n2\n\n\nRACE_ETH - NH_Asian:NH_White\n1\n4.0\nNA\n0.553\n0.248\n0.146\n0.961\n1\n\n\nOdds Ratio\n1\n4.0\nNA\n1.739\nNA\n1.157\n2.613\n2\n\n\nRACE_ETH - Other:NH_White\n1\n5.0\nNA\n-0.615\n0.419\n-1.304\n0.074\n1\n\n\nOdds Ratio\n1\n5.0\nNA\n0.541\nNA\n0.271\n1.077\n2\n\n\nSROH - Excellent:Good\n3\n1.0\nNA\n-0.529\n0.311\n-1.041\n-0.017\n1\n\n\nOdds Ratio\n3\n1.0\nNA\n0.589\nNA\n0.353\n0.983\n2\n\n\nSROH - Very_Good:Good\n3\n2.0\nNA\n-0.355\n0.197\n-0.680\n-0.031\n1\n\n\nOdds Ratio\n3\n2.0\nNA\n0.701\nNA\n0.507\n0.969\n2\n\n\nSROH - Fair:Good\n3\n4.0\nNA\n0.306\n0.198\n-0.019\n0.632\n1\n\n\nOdds Ratio\n3\n4.0\nNA\n1.359\nNA\n0.981\n1.882\n2\n\n\nSROH - Poor:Good\n3\n5.0\nNA\n0.295\n0.402\n-0.366\n0.956\n1\n\n\nOdds Ratio\n3\n5.0\nNA\n1.344\nNA\n0.694\n2.602\n2\n\n\n\n\n\n\n\n9.4.4 Quality of Fit Summaries\n\n\n\n\n\n\nNote\n\n\n\nAgain, we have three available approaches to summarize the fit of Model Z, thanks to our glm() and lrm() fits of the same model. I’ll show all three, but in practice (and in your Project A) I wouldn’t include the model_performance() results.\n\n\n\nlrm() for Model Zglance at Model ZModel Z Performance\n\n\nOur Nagelkerke \\(R^2\\) estimate for Model Z is 0.162, and our C statistic is estimated to be 0.702.\n\n\nCode\nmod_Z_lrm\n\n\nLogistic Regression Model\n\nlrm(formula = HDL_RISK ~ AGE + rcs(WAIST, 4) + RACE_ETH + SROH + \n    WAIST %ia% SROH, data = nh_demo_i, x = TRUE, y = TRUE)\n\n                       Model Likelihood     Discrimination    Rank Discrim.    \n                             Ratio Test            Indexes          Indexes    \nObs           999    LR chi2     123.92     R2       0.162    C       0.702    \n 0            666    d.f.            16    R2(16,999)0.102    Dxy     0.404    \n 1            333    Pr(&gt; chi2) &lt;0.0001    R2(16,666)0.150    gamma   0.404    \nmax |deriv| 3e-08                           Brier    0.197    tau-a   0.180    \n\n                       Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept              -8.6421 2.9045 -2.98  0.0029  \nAGE                    -0.0152 0.0071 -2.15  0.0315  \nWAIST                   0.0887 0.0318  2.79  0.0053  \nWAIST'                 -0.1134 0.0745 -1.52  0.1277  \nWAIST''                 0.2588 0.2314  1.12  0.2634  \nRACE_ETH=Hispanic      -0.0299 0.1900 -0.16  0.8751  \nRACE_ETH=NH_Black      -0.6438 0.2035 -3.16  0.0016  \nRACE_ETH=NH_Asian       0.5535 0.2476  2.24  0.0254  \nRACE_ETH=Other         -0.6148 0.4189 -1.47  0.1422  \nSROH=Very_Good         -0.4613 2.3523 -0.20  0.8445  \nSROH=Good               0.1409 2.1582  0.07  0.9480  \nSROH=Fair              -0.8270 2.2460 -0.37  0.7127  \nSROH=Poor               0.6031 2.8370  0.21  0.8317  \nWAIST * SROH=Very_Good  0.0063 0.0236  0.27  0.7908  \nWAIST * SROH=Good       0.0038 0.0217  0.18  0.8602  \nWAIST * SROH=Fair       0.0164 0.0223  0.73  0.4625  \nWAIST * SROH=Poor       0.0022 0.0271  0.08  0.9356  \n\n\n\n\nThe glance() results below give us our AIC and BIC values, as well as the degrees of freedom used by Model Z.\n\n\nCode\nglance(mod_Z) |&gt;\n  mutate(df = nobs - df.residual - 1) |&gt;\n  select(AIC, BIC, df, df.residual, nobs) |&gt;\n  kable(digits = 1)\n\n\n\n\n\nAIC\nBIC\ndf\ndf.residual\nnobs\n\n\n\n\n1181.8\n1265.3\n16\n982\n999\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs mentioned previously, these summaries aren’t as appealing to me as those in the other two tabs here.\n\n\n\n\nCode\nmodel_performance(mod_Z) |&gt; print_md(digits = 2)\n\n\n\nIndices of model performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nTjur’s R2\nRMSE\nSigma\nLog_loss\nScore_log\nScore_spherical\nPCP\n\n\n\n\n1181.84\n1182.46\n1265.25\n0.12\n0.44\n1.00\n0.57\n-144.81\n1.60e-03\n0.61\n\n\n\n\n\n\n\n\n\n\n9.4.5 Confusion Matrix (Model Z)\nAs in Model Y, my prediction rule for my Model Z confusion matrix is that the fitted value of Pr(HDL_RISK = 1) needs to be greater than or equal to 0.5 for me to predict HDL_RISK is 1, and otherwise I predict 0.\nAgain, we augment our nh_demo_i data to include predicted probabilities of (HDL_RISK = 1) from Model Z.\n\n\nCode\nresZ_aug &lt;- augment(mod_Z, type.predict = \"response\")\n\n\nApplying my prediction rule, we obtain:\n\n\nCode\ncm_Z &lt;- caret::confusionMatrix(\n  data = factor(resZ_aug$.fitted &gt;= 0.5),\n  reference = factor(resZ_aug$HDL_RISK == 1),\n  positive = \"TRUE\")\n\ncm_Z\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   608  241\n     TRUE     58   92\n                                         \n               Accuracy : 0.7007         \n                 95% CI : (0.6712, 0.729)\n    No Information Rate : 0.6667         \n    P-Value [Acc &gt; NIR] : 0.01177        \n                                         \n                  Kappa : 0.2193         \n                                         \n Mcnemar's Test P-Value : &lt; 2e-16        \n                                         \n            Sensitivity : 0.27628        \n            Specificity : 0.91291        \n         Pos Pred Value : 0.61333        \n         Neg Pred Value : 0.71614        \n             Prevalence : 0.33333        \n         Detection Rate : 0.09209        \n   Detection Prevalence : 0.15015        \n      Balanced Accuracy : 0.59459        \n                                         \n       'Positive' Class : TRUE           \n                                         \n\n\nHere are our results comparing classification performance by models Y and Z.\n\n\n\n\n\n\n\n\n\n\nModel\nClassification Rule\nSensitivity\nSpecificity\nPos. Pred. Value\n\n\n\n\nY\nPredicted Pr(HDL_RISK = 1) &gt;= 0.5\n0.276\n0.905\n0.594\n\n\nZ\nPredicted Pr(HDL_RISK = 1) &gt;= 0.5\n0.276\n0.913\n0.613"
  },
  {
    "objectID": "432_projectA_demo.html#validating-models-y-and-z",
    "href": "432_projectA_demo.html#validating-models-y-and-z",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "9.5 Validating Models Y and Z",
    "text": "9.5 Validating Models Y and Z\nWe will use the validate() function from the rms package to validate our lrm fits.\n\n\nCode\nset.seed(4323); (valY &lt;- validate(mod_Y_lrm))\n\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.4017   0.4207  0.3843   0.0364          0.3652 40\nR2            0.1516   0.1668  0.1406   0.0262          0.1254 40\nIntercept     0.0000   0.0000 -0.0602   0.0602         -0.0602 40\nSlope         1.0000   1.0000  0.9034   0.0966          0.9034 40\nEmax          0.0000   0.0000  0.0329   0.0329          0.0329 40\nD             0.1146   0.1272  0.1057   0.0215          0.0932 40\nU            -0.0020  -0.0020  0.0010  -0.0030          0.0010 40\nQ             0.1166   0.1292  0.1047   0.0245          0.0921 40\nB             0.1979   0.1956  0.2003  -0.0047          0.2026 40\ng             0.8757   0.9354  0.8396   0.0959          0.7798 40\ngp            0.1767   0.1855  0.1701   0.0154          0.1612 40\n\n\nCode\nset.seed(4324); (valZ &lt;- validate(mod_Z_lrm))\n\n\n          index.orig training    test optimism index.corrected  n\nDxy           0.4036   0.4267  0.3867   0.0400          0.3636 40\nR2            0.1620   0.1806  0.1461   0.0345          0.1275 40\nIntercept     0.0000   0.0000 -0.0896   0.0896         -0.0896 40\nSlope         1.0000   1.0000  0.8674   0.1326          0.8674 40\nEmax          0.0000   0.0000  0.0474   0.0474          0.0474 40\nD             0.1230   0.1386  0.1102   0.0284          0.0946 40\nU            -0.0020  -0.0020  0.0021  -0.0041          0.0021 40\nQ             0.1250   0.1406  0.1081   0.0325          0.0925 40\nB             0.1968   0.1938  0.2000  -0.0062          0.2030 40\ng             0.9605   1.0350  0.8957   0.1393          0.8212 40\ngp            0.1825   0.1924  0.1724   0.0200          0.1626 40\n\n\n\n9.5.1 Validated Nagelkerke \\(R^2\\), and C, as well as IC statistics\n\n\n\nModel\nValidated \\(R^2\\)\nValidated C\nAIC\nBIC\ndf\n\n\n\n\nY\n0.1254\n0.6826\n1178.3\n1232.2\n10\n\n\nZ\n0.1275\n0.6818\n1181.8\n1265.3\n16"
  },
  {
    "objectID": "432_projectA_demo.html#final-logistic-regression-model",
    "href": "432_projectA_demo.html#final-logistic-regression-model",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "9.6 Final Logistic Regression Model",
    "text": "9.6 Final Logistic Regression Model\nI prefer Model Y, because of its slightly better AIC, BIC and validated C statistic, despite the fact that Model Z has a slightly higher validated \\(R^2\\) and that Model Z also has a slightly higher positive predictive value. It’s pretty close, though.\n\n9.6.1 Winning Model’s Parameter Estimates\n\n\nCode\nmod_Y_lrm\n\n\nLogistic Regression Model\n\nlrm(formula = HDL_RISK ~ AGE + WAIST + RACE_ETH + SROH, data = nh_demo_i, \n    x = TRUE, y = TRUE)\n\n                       Model Likelihood     Discrimination    Rank Discrim.    \n                             Ratio Test            Indexes          Indexes    \nObs           999    LR chi2     115.50     R2       0.152    C       0.701    \n 0            666    d.f.            10    R2(10,999)0.100    Dxy     0.402    \n 1            333    Pr(&gt; chi2) &lt;0.0001    R2(10,666)0.146    gamma   0.402    \nmax |deriv| 1e-06                           Brier    0.198    tau-a   0.179    \n\n                  Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept         -4.5080 0.7221 -6.24  &lt;0.0001 \nAGE               -0.0138 0.0070 -1.98  0.0478  \nWAIST              0.0403 0.0052  7.78  &lt;0.0001 \nRACE_ETH=Hispanic  0.0041 0.1886  0.02  0.9826  \nRACE_ETH=NH_Black -0.6327 0.2032 -3.11  0.0018  \nRACE_ETH=NH_Asian  0.4743 0.2390  1.98  0.0472  \nRACE_ETH=Other    -0.6235 0.4205 -1.48  0.1381  \nSROH=Very_Good     0.1786 0.3175  0.56  0.5739  \nSROH=Good          0.5168 0.3001  1.72  0.0850  \nSROH=Fair          0.8776 0.3154  2.78  0.0054  \nSROH=Poor          0.6895 0.4503  1.53  0.1258  \n\n\n\n\n9.6.2 Winning Model Effect Sizes\n\n\nCode\nplot(summary(mod_Y_lrm, conf.int = 0.90))\n\n\n\n\n\n\n\n\n\n\n\n9.6.3 Numerical Description of Effect Sizes\n\n\nCode\nsummary(mod_Y_lrm, conf.int = 0.90) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow\nHigh\nDiff.\nEffect\nS.E.\nLower 0.9\nUpper 0.9\nType\n\n\n\n\nAGE\n50\n66.0\n16.0\n-0.221\n0.112\n-0.404\n-0.037\n1\n\n\nOdds Ratio\n50\n66.0\n16.0\n0.802\nNA\n0.667\n0.963\n2\n\n\nWAIST\n92\n112.6\n20.6\n0.831\n0.107\n0.655\n1.006\n1\n\n\nOdds Ratio\n92\n112.6\n20.6\n2.295\nNA\n1.925\n2.735\n2\n\n\nRACE_ETH - Hispanic:NH_White\n1\n2.0\nNA\n0.004\n0.189\n-0.306\n0.314\n1\n\n\nOdds Ratio\n1\n2.0\nNA\n1.004\nNA\n0.736\n1.369\n2\n\n\nRACE_ETH - NH_Black:NH_White\n1\n3.0\nNA\n-0.633\n0.203\n-0.967\n-0.299\n1\n\n\nOdds Ratio\n1\n3.0\nNA\n0.531\nNA\n0.380\n0.742\n2\n\n\nRACE_ETH - NH_Asian:NH_White\n1\n4.0\nNA\n0.474\n0.239\n0.081\n0.867\n1\n\n\nOdds Ratio\n1\n4.0\nNA\n1.607\nNA\n1.085\n2.381\n2\n\n\nRACE_ETH - Other:NH_White\n1\n5.0\nNA\n-0.624\n0.420\n-1.315\n0.068\n1\n\n\nOdds Ratio\n1\n5.0\nNA\n0.536\nNA\n0.268\n1.070\n2\n\n\nSROH - Excellent:Good\n3\n1.0\nNA\n-0.517\n0.300\n-1.010\n-0.023\n1\n\n\nOdds Ratio\n3\n1.0\nNA\n0.596\nNA\n0.364\n0.977\n2\n\n\nSROH - Very_Good:Good\n3\n2.0\nNA\n-0.338\n0.193\n-0.656\n-0.020\n1\n\n\nOdds Ratio\n3\n2.0\nNA\n0.713\nNA\n0.519\n0.980\n2\n\n\nSROH - Fair:Good\n3\n4.0\nNA\n0.361\n0.180\n0.065\n0.656\n1\n\n\nOdds Ratio\n3\n4.0\nNA\n1.434\nNA\n1.067\n1.928\n2\n\n\nSROH - Poor:Good\n3\n5.0\nNA\n0.173\n0.368\n-0.433\n0.778\n1\n\n\nOdds Ratio\n3\n5.0\nNA\n1.188\nNA\n0.649\n2.177\n2\n\n\n\n\n\n\n\n9.6.4 Effect Size Description(s)\nIn your work, we would only need to see one of the following effect size descriptions.\nWAIST description: If we have two subjects of the same age, race/ethnicity and self-reported overall health, then if subject 1 has a waist circumference of 92 cm and subject 2 has a waist circumference of 112.6 cm, then our model estimates that subject 2 will have 2.295 times the odds (90% CI: 1.93, 2.74) that subject 1 has of having a HDL that is at risk.\nAGE description: If we have two subjects of the same waist circumference, race/ethnicity and self-reported overall health, then if subject 1 is age 50 and subject 2 is age 66, our model predicts that subject 2 has 80.2% of the odds of an at-risk HDL that subject 1 has. The 90% CI for the odds ratio comparing subject 1 to subject 2 is (0.667, 0.963).\nSROH description: If we have two subjects of the same age, waist circumference and race/ethnicity. then if subject 1 reports Excellent overall health while subject 2 reports only Good overall health, our model predicts that the odds ratio comparing subject 1’s odds of an at-risk HDL to those of subject 2 will be 0.596, with a 90% CI of (0.364, 0.977).\n\n\n9.6.5 Validated \\(R^2\\) and \\(C\\) statistic for Winning Model\nAs we saw in Section 9.5.1,\n\nthe validated \\(R^2\\) statistic for Model Y is 0.1254, and\nthe validated \\(C\\) statistic for Model Y is 0.6826.\n\n\n\n9.6.6 Nomogram of Winning Model\n\n\nCode\nplot(nomogram(mod_Y_lrm, fun = plogis, \n              funlabel = \"Pr(HDL_RISK = 1)\"))\n\n\n\n\n\n\n\n\n\n\n\n9.6.7 Predictions for Two New Subjects\nI will create a predicted Pr(HDL_RISK = 1) for two new Non-Hispanic White subjects who are 60 years old, and who rate their overall health as Fair. The first will have a waist circumference of 80 cm, and the second will have a waist circumference of 100 cm.\n\n\n\n\n\n\nNote\n\n\n\nI can do this using either the glm() or the lrm() fit to our model. Either is sufficient for Project A, and there’s no need to show both approaches.\n\n\n\nUsing the glm() fitUsing the lrm() fit\n\n\n\n\nCode\nnew_subj &lt;- \n  data.frame(AGE = c(60,60), WAIST = c(80, 100), \n             RACE_ETH = c(\"NH_White\", \"NH_White\"), SROH = c(\"Fair\", \"Fair\"))\n\npreds3 &lt;- predict(mod_Y, newdata = new_subj, type = \"response\")\n\npreds3\n\n\n        1         2 \n0.2256724 0.3949595 \n\n\n\nOur prediction for subject 1’s Pr(HDL_RISK = 1) is 0.226.\nOur prediction for subject 2’s Pr(HDL_RISK = 1) is 0.395.\n\n\n\nAlternatively, I could have used our lrm() fit.\n\n\nCode\nnew_subj &lt;- \n  data.frame(AGE = c(60,60), WAIST = c(80, 100), \n             RACE_ETH = c(\"NH_White\", \"NH_White\"), SROH = c(\"Fair\", \"Fair\"))\n\npreds4 &lt;- predict(mod_Y_lrm, newdata = new_subj, type = \"fitted\")\n\npreds4\n\n\n        1         2 \n0.2256724 0.3949595 \n\n\nAs before, the two fitting approaches (glm() and lrm()) produce the same model, so they produce the same predictions."
  },
  {
    "objectID": "432_projectA_demo.html#answering-my-research-questions",
    "href": "432_projectA_demo.html#answering-my-research-questions",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "10.1 Answering My Research Questions",
    "text": "10.1 Answering My Research Questions\n\n10.1.1 Question 1 (with Answer)\nI’ll leave the job of restating and drawing conclusions about the research question under study in our linear regression analyses for you to answer.\n\n\n10.1.2 Question 2 (with Answer)\nI’ll leave the job of restating and drawing conclusions about the research question under study in our logistic regression analyses for you to answer."
  },
  {
    "objectID": "432_projectA_demo.html#thoughts-on-project-a",
    "href": "432_projectA_demo.html#thoughts-on-project-a",
    "title": "Predicting High-Density Lipoprotein Cholesterol Levels",
    "section": "10.2 Thoughts on Project A",
    "text": "10.2 Thoughts on Project A\nNote that we expect you to answer exactly two of the following four questions in your discussion.\n\n10.2.1 Question 1\n\nWhat was substantially harder or easier than you expected, and why?\n\nIf you chose to answer Question 1, your answer would go here.\n\n\n10.2.2 Question 2\n\nWhat do you wish you’d known at the start of this process that you know now, and why?\n\nIf you chose to answer Question 2, your answer would go here.\n\n\n10.2.3 Question 3\n\nWhat was the most confusing part of doing the project, and how did you get past it?\n\nIf you chose to answer Question 3, your answer would go here.\n\n\n10.2.4 Question 4\n\nWhat was the most useful thing you learned while doing the project, and why?\n\nIf you chose to answer Question 4, your answer would go here."
  },
  {
    "objectID": "chr_codebook.html",
    "href": "chr_codebook.html",
    "title": "County Health Rankings Codebook",
    "section": "",
    "text": "library(gt)\nlibrary(naniar)\nlibrary(easystats)\nlibrary(tidyverse)\n\nlovedist &lt;- function(x) {\n  tibble::tibble(\n    n = length(x),\n    miss = sum(is.na(x)),\n    mean = mean(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE),\n    med = median(x, na.rm = TRUE),\n    mad = mad(x, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    q25 = quantile(x, 0.25, na.rm = TRUE),\n    q75 = quantile(x, 0.75, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n  )\n}\n\n\nchr2015 &lt;- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/refs/heads/master/data/chr_2015.csv\", show_col_types = FALSE)\nchr2024 &lt;- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/refs/heads/master/data/chr_2024.csv\", show_col_types = FALSE)\n\n\n\nThe data come from County Health Rankings, specifically the 2024 and 2015 reports."
  },
  {
    "objectID": "chr_codebook.html#r-packages-and-data-ingest",
    "href": "chr_codebook.html#r-packages-and-data-ingest",
    "title": "County Health Rankings Codebook",
    "section": "",
    "text": "library(gt)\nlibrary(naniar)\nlibrary(easystats)\nlibrary(tidyverse)\n\nlovedist &lt;- function(x) {\n  tibble::tibble(\n    n = length(x),\n    miss = sum(is.na(x)),\n    mean = mean(x, na.rm = TRUE),\n    sd = sd(x, na.rm = TRUE),\n    med = median(x, na.rm = TRUE),\n    mad = mad(x, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    q25 = quantile(x, 0.25, na.rm = TRUE),\n    q75 = quantile(x, 0.75, na.rm = TRUE),\n    max = max(x, na.rm = TRUE),\n  )\n}\n\n\nchr2015 &lt;- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/refs/heads/master/data/chr_2015.csv\", show_col_types = FALSE)\nchr2024 &lt;- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/refs/heads/master/data/chr_2024.csv\", show_col_types = FALSE)\n\n\n\nThe data come from County Health Rankings, specifically the 2024 and 2015 reports."
  },
  {
    "objectID": "chr_codebook.html#variable-descriptions",
    "href": "chr_codebook.html#variable-descriptions",
    "title": "County Health Rankings Codebook",
    "section": "Variable Descriptions",
    "text": "Variable Descriptions\n\n\n\nVariable\nNHANES\nSummary\nDefinition\nCHR 2024 describes\nCHR 2015 describes\n\n\n\n\nFIPS_code\nfipscode with C-\nfive-digit FIPS code\nunique five-digit FIPS code preceded by C-\n3054 values\n3054 values\n\n\nstate\nstate\nUSPS state abbreviation\n50 states + District of Columbia (DC)\n51 values\n51 values\n\n\ncounty\ncounty\ncounty name\nchr_2024 and chr_2015 contain the same counties\n3054 counties\n3054 counties\n\n\nrelease\nyear\nRelease Year\nDate for CHR Data Release\n2024\n2015\n\n\nprem_death\nv001_rawvalue x 0.01\nPremature death\nYears of potential life lost before age 75 per 1,000 population (age-adjusted)\n2019-2021\n2010-2012\n\n\npf_health\nv002_rawvalue x 100\nPoor or fair health\nPercentage of adults reporting fair or poor health (age-adjusted)\n2021\n2006-2012\n\n\npoor_phys\nv036_rawvalue\nPoor physical health days\nAverage number of physically unhealthy days reported in past 30 days (age-adjusted).\n2021\n2006-2012\n\n\npoor_ment\nv042_rawvalue\nPoor mental health days\nAverage number of mentally unhealthy days reported in past 30 days (age-adjusted).\n2021\n2006-2012\n\n\nlow_bwt\nv037_rawvalue x 100\nLow birthweight\nPercentage of live births with low birthweight (&lt; 2,500 grams)\n2016-2022\n2006-2012\n\n\nsmoking\nv009_rawvalue x 100\nAdult smoking\nPercentage of adults who are current smokers (age-adjusted)\n2021\n2006-2012\n\n\nobesity\nv011_rawvalue\nAdult obesity\nPercentage of the adult population (age 18 and older) that reports a body mass index (BMI) greater than or equal to 30 kg/m2 (age-adjusted)\n2021\n2011\n\n\nfood_env\nv133_rawvalue\nFood environment index\nIndex of factors that contribute to a healthy food environment, from 0 (worst) to 10 (best)\n2019 & 2021\n2012\n\n\ndrinking\nv049_rawvalue x 100\nExcessive drinking\nPercentage of adults reporting binge or heavy drinking (age-adjusted)\n2021\n2006-2012\n\n\nsti_rate\nv045_rawvalue\nSexually transmitted infections\nNumber of newly diagnosed chlamydia cases per 100,000 population\n2021\n2012\n\n\nunins\nv085_rawvalue x 100\nUninsured\nPercentage of population under age 65 without health insurance\n2021\n2012\n\n\npcp_rate\n1 / v004_rawvalue\nPrimary care physicians\nRatio of population to primary care physicians\n2021\n2012\n\n\nmammog\nv050_rawvalue x 100\nMammography screening\nPercentage of female Medicare enrollees ages 65-74 who received an annual mammography screening\n2021\n2012\n\n\nhsgrad\nv021_rawvalue x 100\nHigh school graduation\nPercentage of ninth-grade cohort that graduates in four years\n2020-2021\n2011-2012\n\n\nunemp\nv023_rawvalue\nUnemployment x 100\nPercentage of population ages 16 and older unemployed but seeking work\n2022\n2013\n\n\ninc_ineq\nv044_rawvalue\nIncome inequality\nRatio of household income at the 80th percentile to income at the 20th percentile\n2018-2022\n2009-2013\n\n\nsocial\nv140_rawvalue\nSocial associations\nNumber of membership associations per 10,000 population\n2021\n2012\n\n\nmed_inc\nv063_rawvalue x 0.001\nMedian household income\nThe income (in thousands of dollars) where half of households in a county earn more and half of households earn less\n2018-2022\n2013\n\n\nsev_hous\nv136_rawvalue x 100\nSevere housing problems\nPercentage of households with at least 1 of 4 housing problems: overcrowding, high housing costs, lack of kitchen facilities, or lack of plumbing facilities\n2016-2020\n2007-2011\n\n\ncommute\nv137_rawvalue x 100\nLong commute - driving alone\nAmong workers who commute in their car alone, percentage that commute more than 30 minutes\n2018-2022\n2009-2013\n\n\npopn\nv051_rawvalue\nPopulation\nResident population (number of residents)\n2022\n2013\n\n\nage65up\nv053_rawvalue x 100\nAge 65 and older\nPercentage of population ages 65 and older\n2022\n2013\n\n\nnh_white\nv126_rawvalue x 100\nNon-Hispanic white\nPercentage of population identifying as non-Hispanic white\n2022\n2013\n\n\nnon_eng\nv059_rawvalue x 100\nNot proficient in English\nPercentage of population aged 5 and over who reported speaking English less than well\n2018-2022\n2009-2013\n\n\nfemale\nv057_rawvalue x 100\nFemale\nPercentage of population identifying as female\n2022\n2013\n\n\nrural\nv058_rawvalue x 100\nRural\nPercentage of population living in a census-defined rural area\n2020\n2010"
  },
  {
    "objectID": "chr_codebook.html#numerical-summaries",
    "href": "chr_codebook.html#numerical-summaries",
    "title": "County Health Rankings Codebook",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nPremature Death Rate\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(prem_death)),\n  chr2024 |&gt;\n    reframe(lovedist(prem_death))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"prem_death\", \"prem_death\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nprem_death\n3054\n0\n79.63296\n24.01601\n76.745\n22.48363\n21.01\n62.9625\n93.2600\n238.80\n\n\n2024\nprem_death\n3054\n0\n98.32840\n34.26893\n93.700\n31.11977\n24.18\n74.4100\n116.9675\n411.52\n\n\n\n\n\n\n\n\n\nPoor or Fair Health\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(pf_health)),\n  chr2024 |&gt;\n    reframe(lovedist(pf_health))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"pf_health\", \"pf_health\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\npf_health\n3054\n347\n17.32837\n6.107532\n16.6\n6.07866\n4.5\n12.7\n20.95\n50.8\n\n\n2024\npf_health\n3054\n0\n17.78048\n4.526644\n17.0\n4.59606\n8.4\n14.3\n20.90\n38.0\n\n\n\n\n\n\n\n\n\nPoor physical health days\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(poor_phys)),\n  chr2024 |&gt;\n    reframe(lovedist(poor_phys))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"poor_phys\", \"poor_phys\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\npoor_phys\n3054\n285\n3.840014\n1.1382737\n3.7\n1.03782\n1.2\n3.1\n4.4\n10.0\n\n\n2024\npoor_phys\n3054\n0\n3.904486\n0.6452397\n3.9\n0.74130\n2.3\n3.4\n4.3\n6.4\n\n\n\n\n\n\n\n\n\nPoor mental health days\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(poor_ment)),\n  chr2024 |&gt;\n    reframe(lovedist(poor_ment))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"poor_ment\", \"poor_ment\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\npoor_ment\n3054\n490\n3.575156\n1.0266347\n3.5\n0.88956\n1.0\n2.9\n4.1\n10.1\n\n\n2024\npoor_ment\n3054\n0\n5.230354\n0.6084771\n5.2\n0.59304\n3.2\n4.8\n5.6\n7.4\n\n\n\n\n\n\n\n\n\nLow birthweight\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(low_bwt)),\n  chr2024 |&gt;\n    reframe(lovedist(low_bwt))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"low_bwt\", \"low_bwt\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nlow_bwt\n3054\n29\n8.202347\n2.083931\n7.9\n1.77912\n2.9\n6.8\n9.2\n18.0\n\n\n2024\nlow_bwt\n3054\n29\n8.361917\n2.095554\n8.1\n1.77912\n3.1\n6.9\n9.3\n19.2\n\n\n\n\n\n\n\n\n\nAdult smoking\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(smoking)),\n  chr2024 |&gt;\n    reframe(lovedist(smoking))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"smoking\", \"smoking\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nsmoking\n3054\n365\n21.28442\n6.287341\n20.8\n5.78214\n3.1\n17.0\n24.9\n51.1\n\n\n2024\nsmoking\n3054\n0\n19.08504\n4.065791\n18.8\n3.70650\n7.0\n16.4\n21.6\n43.0\n\n\n\n\n\n\n\n\n\nAdult obesity\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(obesity)),\n  chr2024 |&gt;\n    reframe(lovedist(obesity))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"obesity\", \"obesity\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nobesity\n3054\n0\n30.74096\n4.352451\n31.0\n3.70650\n12.0\n28.5\n33.4\n48.1\n\n\n2024\nobesity\n3054\n0\n37.39800\n4.556928\n37.7\n3.85476\n17.4\n35.1\n40.3\n52.5\n\n\n\n\n\n\n\n\n\nFood environment index\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(food_env)),\n  chr2024 |&gt;\n    reframe(lovedist(food_env))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"food_env\", \"food_env\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nfood_env\n3054\n0\n7.110642\n1.223480\n7.3\n1.03782\n0\n6.5\n7.9\n10\n\n\n2024\nfood_env\n3054\n30\n7.575595\n1.161079\n7.7\n1.03782\n0\n6.9\n8.4\n10\n\n\n\n\n\n\n\n\n\nExcessive drinking\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(drinking)),\n  chr2024 |&gt;\n    reframe(lovedist(drinking))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"drinking\", \"drinking\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\ndrinking\n3054\n859\n16.42579\n5.098082\n16.2\n5.04084\n3.2\n12.8\n19.5\n56.2\n\n\n2024\ndrinking\n3054\n0\n16.84790\n2.630048\n16.9\n2.52042\n9.0\n15.0\n18.5\n26.8\n\n\n\n\n\n\n\n\n\nSexually transmitted infections\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(sti_rate)),\n  chr2024 |&gt;\n    reframe(lovedist(sti_rate))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"sti_rate\", \"sti_rate\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nsti_rate\n3054\n109\n365.5041\n263.4221\n290.30\n167.0890\n39.2\n197.500\n449.700\n2854.3\n\n\n2024\nsti_rate\n3054\n106\n390.3388\n262.2673\n313.55\n178.2085\n26.0\n217.075\n497.125\n3467.6\n\n\n\n\n\n\n\n\n\nUninsured\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(unins)),\n  chr2024 |&gt;\n    reframe(lovedist(unins))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"unins\", \"unins\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nunins\n3054\n0\n17.45337\n5.296112\n17.3\n5.48562\n2.9\n13.5\n20.8\n38.6\n\n\n2024\nunins\n3054\n0\n11.44751\n5.192413\n10.3\n4.89258\n2.4\n7.4\n14.5\n38.7\n\n\n\n\n\n\n\n\n\nPrimary care physicians\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(pcp_rate)),\n  chr2024 |&gt;\n    reframe(lovedist(pcp_rate))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"pcp_rate\", \"pcp_rate\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\npcp_rate\n3054\n143\n2580.205\n2129.174\n1989\n1036.337\n210\n1381\n2922.5\n20936\n\n\n2024\npcp_rate\n3054\n167\n2820.442\n2463.584\n2107\n1217.215\n172\n1404\n3266.0\n29081\n\n\n\n\n\n\n\n\n\nMammography screening\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(mammog)),\n  chr2024 |&gt;\n    reframe(lovedist(mammog))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"mammog\", \"mammog\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nmammog\n3054\n37\n60.74147\n8.150897\n61\n7.413\n24\n56\n66\n84\n\n\n2024\nmammog\n3054\n8\n41.73539\n7.991924\n42\n7.413\n6\n37\n47\n64\n\n\n\n\n\n\n\n\n\nHigh school graduation\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(hsgrad)),\n  chr2024 |&gt;\n    reframe(lovedist(hsgrad))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"hsgrad\", \"hsgrad\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nhsgrad\n3054\n389\n83.15872\n9.321982\n85\n8.8956\n20\n78\n90\n100\n\n\n2024\nhsgrad\n3054\n591\n88.53187\n6.707152\n90\n4.4478\n22\n85\n93\n100\n\n\n\n\n\n\n\n\n\nUnemployment\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(unemp)),\n  chr2024 |&gt;\n    reframe(lovedist(unemp))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"unemp\", \"unemp\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nunemp\n3054\n0\n7.320563\n2.627749\n7\n2.9652\n1\n6\n9\n28\n\n\n2024\nunemp\n3054\n0\n3.603143\n1.248518\n3\n1.4826\n1\n3\n4\n15\n\n\n\n\n\n\n\n\n\nIncome inequality\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(inc_ineq)),\n  chr2024 |&gt;\n    reframe(lovedist(inc_ineq))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"inc_ineq\", \"inc_ineq\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\ninc_ineq\n3054\n0\n4.490596\n0.7020046\n4.39\n0.607866\n2.63\n4.02\n4.84\n9.65\n\n\n2024\ninc_ineq\n3054\n0\n4.535301\n0.8488547\n4.43\n0.681996\n0.00\n4.02\n4.94\n9.51\n\n\n\n\n\n\n\n\n\nSocial associations\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(social)),\n  chr2024 |&gt;\n    reframe(lovedist(social))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"social\", \"social\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nsocial\n3054\n0\n13.74706\n6.571020\n12.585\n5.070492\n0\n9.5150\n16.5400\n59.27\n\n\n2024\nsocial\n3054\n0\n11.43965\n5.515214\n10.840\n4.447800\n0\n8.0425\n14.1275\n48.22\n\n\n\n\n\n\n\n\n\nMedian household income\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(med_inc)),\n  chr2024 |&gt;\n    reframe(lovedist(med_inc))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"med_inc\", \"med_inc\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nmed_inc\n3054\n0\n45.97718\n11.71013\n44.0\n9.93342\n21.6\n38.1\n51.475\n117.7\n\n\n2024\nmed_inc\n3054\n0\n63.26847\n16.35805\n60.8\n13.34340\n29.0\n52.4\n70.500\n167.6\n\n\n\n\n\n\n\n\n\nSevere housing problems\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(sev_hous)),\n  chr2024 |&gt;\n    reframe(lovedist(sev_hous))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"sev_hous\", \"sev_hous\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nsev_hous\n3054\n0\n14.39659\n4.648617\n13.8\n3.85476\n2.5\n11.3\n16.7\n62.4\n\n\n2024\nsev_hous\n3054\n0\n12.84420\n4.213573\n12.2\n3.26172\n2.0\n10.2\n14.8\n57.7\n\n\n\n\n\n\n\n\n\nLong commute - driving alone\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(commute)),\n  chr2024 |&gt;\n    reframe(lovedist(commute))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"commute\", \"commute\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\ncommute\n3054\n0\n30.00891\n11.99093\n29.2\n12.75036\n0\n20.9\n38.400\n70.5\n\n\n2024\ncommute\n3054\n0\n33.22642\n12.61378\n32.7\n13.49166\n0\n23.7\n41.975\n73.3\n\n\n\n\n\n\n\n\n\nPopulation\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(popn)),\n  chr2024 |&gt;\n    reframe(lovedist(popn))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"popn\", \"popn\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\npopn\n3054\n0\n103463.3\n326547.2\n27150.0\n27851.38\n1179\n12126.50\n69767.25\n10017068\n\n\n2024\npopn\n3054\n0\n109078.6\n336911.3\n27033.5\n28471.11\n1057\n11846.25\n72145.00\n9721138\n\n\n\n\n\n\n\n\n\nAge 65 and older\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(age65up)),\n  chr2024 |&gt;\n    reframe(lovedist(age65up))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"age65up\", \"age65up\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nage65up\n3054\n0\n17.05972\n4.284420\n16.8\n3.85476\n3.8\n14.3\n19.4\n51.6\n\n\n2024\nage65up\n3054\n0\n20.35498\n4.738572\n20.0\n4.15128\n5.4\n17.3\n22.9\n57.5\n\n\n\n\n\n\n\n\n\nNon-Hispanic white\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(nh_white)),\n  chr2024 |&gt;\n    reframe(lovedist(nh_white))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"nh_white\", \"nh_white\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nnh_white\n3054\n0\n77.33101\n19.75793\n84.75\n15.34491\n3.2\n65.8\n93.3\n98.2\n\n\n2024\nnh_white\n3054\n0\n74.77685\n20.21175\n81.80\n16.90164\n2.8\n63.0\n91.2\n97.5\n\n\n\n\n\n\n\n\n\nNot proficient in English\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(non_eng)),\n  chr2024 |&gt;\n    reframe(lovedist(non_eng))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"non_eng\", \"non_eng\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nnon_eng\n3054\n0\n1.810544\n2.913281\n0.8\n0.88956\n0\n0.3\n2.0\n29.7\n\n\n2024\nnon_eng\n3054\n0\n1.574263\n2.536232\n0.7\n0.74130\n0\n0.3\n1.7\n32.1\n\n\n\n\n\n\n\n\n\nFemale\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(female)),\n  chr2024 |&gt;\n    reframe(lovedist(female))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"female\", \"female\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nfemale\n3054\n0\n49.97472\n2.214339\n50.4\n1.03782\n29.9\n49.6\n51.0\n57.0\n\n\n2024\nfemale\n3054\n0\n49.55688\n2.257926\n49.9\n1.18608\n27.0\n49.0\n50.7\n57.8\n\n\n\n\n\n\n\n\n\nRural\n\nbind_rows(\n  chr2015 |&gt;\n    reframe(lovedist(rural)),\n  chr2024 |&gt;\n    reframe(lovedist(rural))) |&gt;\n  mutate(CHR = c(2015, 2024),\n         var = c(\"rural\", \"rural\")) |&gt;\n  relocate(CHR, var, everything()) |&gt; gt()\n\n\n\n\n\n\n\nCHR\nvar\nn\nmiss\nmean\nsd\nmed\nmad\nmin\nq25\nq75\nmax\n\n\n\n\n2015\nrural\n3054\n0\n57.48415\n31.15573\n58.3\n37.80630\n0\n32.525\n83.4\n100\n\n\n2024\nrural\n3054\n0\n63.03124\n33.51602\n64.7\n52.33578\n0\n34.800\n100.0\n100"
  },
  {
    "objectID": "hbp_codebook.html",
    "href": "hbp_codebook.html",
    "title": "HBP 3024 Codebook",
    "section": "",
    "text": "The hbp3024.xlsx file is available for download on the 432 data page.\nThe (simulated) data in the hbp3024.xlsx file describe 3024 adults living with hypertension (high blood pressure) diagnoses who receive primary care in one of seven practices.\n\nIn each of the seven practices, 432 (different) individuals (who I’ll call subjects in what follows) were sampled at random from all eligible subjects.\nThe data are based on real electronic health record (EHR) data, but with some noise added.\n\nThe practices are named after streets that appear in The Simpsons.\n\n\n\n\nThe data are cross-sectional and describe results from a one-year reporting window. To be eligible for the study, a subject had to meet all of the following criteria:\n\nhave an EHR-documented hypertension diagnosis which applied during the one-year reporting window,\ncared for at one of the seven practices in this study, and by one of the 55 participating providers in this study\nage 25 or older at the start of the one-year reporting period (note that all subjects with ages 80 and higher are listed as age 80 in the data)\nbetween 1 and 12 primary care office visits in the one-year reporting period\nbetween 2 and 24 primary care office visits combined across the reporting period and the previous year\nfall into one of two biological sex categories (female or male)\nfall into one of four primary insurance categories, specifically Medicare, Commercial, Medicaid or Uninsured.\nhave a most recent systolic BP between 80 and 220 mm Hg and most recent diastolic BP between 40 and 140 mm Hg, where the systolic BP is at least 15 and no more than 130 mm Hg larger than the diastolic BP.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrecord\nunique code for each subject (six digits, first digit is 9, last indicates practice)\n\n\npractice\nprimary care practice, of which there are seven in the data\n\n\nprovider\nprimary care provider (each practice has multiple providers)\n\n\nage\nsubject’s age as of the start of the reporting period\n\n\nrace\nsubject’s race (4 levels: Asian, AA_Black, White, Other)\n\n\neth_hisp\nis subject of Hispanic/Latino ethnicity? Yes or No\n\n\nsex\nsubject’s sex (F or M)\n\n\ninsurance\nsubject’s primary insurance (Medicare, Commercial, Medicaid, Uninsured)\n\n\nincome\nestimated median income of subject’s home neighborhood (via American Community Survey, to nearest $100)\n\n\nhsgrad\nestimated percentage of adults living in the subject’s home neighborhood who have graduated from high school (via American Community Survey, to the nearest tenth of a percent)\n\n\ntobacco\ntobacco use status (Current, Former, or Never)\n\n\ndepr_diag\ndoes subject have depression diagnosis? Yes or No\n\n\nheight\nsubject’s height in meters, rounded to two decimal places\n\n\nweight\nsubject’s weight in kilograms, rounded to one decimal place\n\n\nldl\nsubject’s LDL cholesterol level, in mg/dl\n\n\nstatin\ndoes subject have a current prescription for a statin medication? Yes or No\n\n\nbp_med\ndoes subject have a current prescription for a blood pressure control medication? Yes or No\n\n\nsbp\nsubject’s most recently obtained systolic blood pressure, in mm Hg\n\n\ndbp\nsubject’s most recently obtained diastolic blood pressure, in mm Hg\n\n\nvisits_1\nsubject’s number of visits for primary care in reporting period (one year)\n\n\nvisits_2\nsubject’s visits for primary care in the past two years\n\n\nacearb\ndoes subject have a current prescription for an ACE-inhibitor or ARB? Yes or No\n\n\nbetab\ndoes subject have a current prescription for a beta-blocker? Yes or No\n\n\n\n\n\n\n\nThe list of medications included in bp_med is: ACE-inhibitor, ARB, Diuretic, Calcium-Channel Blocker, Beta-Blocker, Alpha-1 Blocker, Centrally acting Alpha-2 Agonist, Vasodilator or other antihypertensive agents. A subject with a current prescription for any of these will have a Yes in bp_med.\nFor the acearb, betab, bpmed, statin and depr_diag variables, a No response includes all subjects where there’s no evidence in the EHR of meeting the Yes criterion, so that there are no missing values (a missing value is interpreted there as No.)\nFor the height, weight and ldl results, implausible values were treated as missing in preparing the data for you.\nThe race and eth_hisp values are self-reported, and some subjects refused to answer one or both of the relevant questions.\nThe income and hsgrad values are imputed from the subject’s home address, usually at the census block level, but occasionally at the level of the zip code.\n\nWhen a subject’s home address could not be geocoded, these values are noted as missing.\nGeocoded estimates of income below 6500 are reported as 6500, and estimates above 130000 are reported as 130000.\nFor hsgrad, geocoded estimates below 40 are reported as 40, and estimates above 99.9 are reported as 99.9."
  },
  {
    "objectID": "hbp_codebook.html#the-hbp3024-data",
    "href": "hbp_codebook.html#the-hbp3024-data",
    "title": "HBP 3024 Codebook",
    "section": "",
    "text": "The hbp3024.xlsx file is available for download on the 432 data page.\nThe (simulated) data in the hbp3024.xlsx file describe 3024 adults living with hypertension (high blood pressure) diagnoses who receive primary care in one of seven practices.\n\nIn each of the seven practices, 432 (different) individuals (who I’ll call subjects in what follows) were sampled at random from all eligible subjects.\nThe data are based on real electronic health record (EHR) data, but with some noise added.\n\nThe practices are named after streets that appear in The Simpsons.\n\n\n\n\nThe data are cross-sectional and describe results from a one-year reporting window. To be eligible for the study, a subject had to meet all of the following criteria:\n\nhave an EHR-documented hypertension diagnosis which applied during the one-year reporting window,\ncared for at one of the seven practices in this study, and by one of the 55 participating providers in this study\nage 25 or older at the start of the one-year reporting period (note that all subjects with ages 80 and higher are listed as age 80 in the data)\nbetween 1 and 12 primary care office visits in the one-year reporting period\nbetween 2 and 24 primary care office visits combined across the reporting period and the previous year\nfall into one of two biological sex categories (female or male)\nfall into one of four primary insurance categories, specifically Medicare, Commercial, Medicaid or Uninsured.\nhave a most recent systolic BP between 80 and 220 mm Hg and most recent diastolic BP between 40 and 140 mm Hg, where the systolic BP is at least 15 and no more than 130 mm Hg larger than the diastolic BP.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrecord\nunique code for each subject (six digits, first digit is 9, last indicates practice)\n\n\npractice\nprimary care practice, of which there are seven in the data\n\n\nprovider\nprimary care provider (each practice has multiple providers)\n\n\nage\nsubject’s age as of the start of the reporting period\n\n\nrace\nsubject’s race (4 levels: Asian, AA_Black, White, Other)\n\n\neth_hisp\nis subject of Hispanic/Latino ethnicity? Yes or No\n\n\nsex\nsubject’s sex (F or M)\n\n\ninsurance\nsubject’s primary insurance (Medicare, Commercial, Medicaid, Uninsured)\n\n\nincome\nestimated median income of subject’s home neighborhood (via American Community Survey, to nearest $100)\n\n\nhsgrad\nestimated percentage of adults living in the subject’s home neighborhood who have graduated from high school (via American Community Survey, to the nearest tenth of a percent)\n\n\ntobacco\ntobacco use status (Current, Former, or Never)\n\n\ndepr_diag\ndoes subject have depression diagnosis? Yes or No\n\n\nheight\nsubject’s height in meters, rounded to two decimal places\n\n\nweight\nsubject’s weight in kilograms, rounded to one decimal place\n\n\nldl\nsubject’s LDL cholesterol level, in mg/dl\n\n\nstatin\ndoes subject have a current prescription for a statin medication? Yes or No\n\n\nbp_med\ndoes subject have a current prescription for a blood pressure control medication? Yes or No\n\n\nsbp\nsubject’s most recently obtained systolic blood pressure, in mm Hg\n\n\ndbp\nsubject’s most recently obtained diastolic blood pressure, in mm Hg\n\n\nvisits_1\nsubject’s number of visits for primary care in reporting period (one year)\n\n\nvisits_2\nsubject’s visits for primary care in the past two years\n\n\nacearb\ndoes subject have a current prescription for an ACE-inhibitor or ARB? Yes or No\n\n\nbetab\ndoes subject have a current prescription for a beta-blocker? Yes or No\n\n\n\n\n\n\n\nThe list of medications included in bp_med is: ACE-inhibitor, ARB, Diuretic, Calcium-Channel Blocker, Beta-Blocker, Alpha-1 Blocker, Centrally acting Alpha-2 Agonist, Vasodilator or other antihypertensive agents. A subject with a current prescription for any of these will have a Yes in bp_med.\nFor the acearb, betab, bpmed, statin and depr_diag variables, a No response includes all subjects where there’s no evidence in the EHR of meeting the Yes criterion, so that there are no missing values (a missing value is interpreted there as No.)\nFor the height, weight and ldl results, implausible values were treated as missing in preparing the data for you.\nThe race and eth_hisp values are self-reported, and some subjects refused to answer one or both of the relevant questions.\nThe income and hsgrad values are imputed from the subject’s home address, usually at the census block level, but occasionally at the level of the zip code.\n\nWhen a subject’s home address could not be geocoded, these values are noted as missing.\nGeocoded estimates of income below 6500 are reported as 6500, and estimates above 130000 are reported as 130000.\nFor hsgrad, geocoded estimates below 40 are reported as 40, and estimates above 99.9 are reported as 99.9."
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Course Calendar.\n\nWe charge a 5 point penalty for a lab that is 1-48 hours late.\nWe do not grade work that is more than 48 hours late.\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7. If you decide to skip a lab, please submit a note to Canvas by the deadline saying that you are skipping the lab.\n\n\n\n\nThere is a Lab 1 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 1, as it will make things easier for you and for the people grading your work.\n\nIn the Lab 1 template, we use the flatly theme. If you’d like to use a different theme, the available list is here.\n\n\n\n\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output.\n\n\n\nThe chr_2024.csv data set we have provided describes a series of 30 variables, pulled from the data for 3054 counties in the County Health Rankings report for 2024.\n\nThe chr_2024.csv file is available for download on the 432 data page.\nA detailed codebook for all of the data in the chr_2024 file is available here."
  },
  {
    "objectID": "lab1.html#template",
    "href": "lab1.html#template",
    "title": "Lab 1",
    "section": "",
    "text": "There is a Lab 1 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 1, as it will make things easier for you and for the people grading your work.\n\nIn the Lab 1 template, we use the flatly theme. If you’d like to use a different theme, the available list is here."
  },
  {
    "objectID": "lab1.html#our-best-advice",
    "href": "lab1.html#our-best-advice",
    "title": "Lab 1",
    "section": "",
    "text": "Review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "lab1.html#the-data",
    "href": "lab1.html#the-data",
    "title": "Lab 1",
    "section": "",
    "text": "The chr_2024.csv data set we have provided describes a series of 30 variables, pulled from the data for 3054 counties in the County Health Rankings report for 2024.\n\nThe chr_2024.csv file is available for download on the 432 data page.\nA detailed codebook for all of the data in the chr_2024 file is available here."
  },
  {
    "objectID": "lab1.html#be-sure-to-include-session-information",
    "href": "lab1.html#be-sure-to-include-session-information",
    "title": "Lab 1",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "lab1.html#after-the-lab",
    "href": "lab1.html#after-the-lab",
    "title": "Lab 1",
    "section": "After the Lab",
    "text": "After the Lab\n\nWe will post an answer sketch to our Shared Google Drive 48 hours after the Lab is due.\nWe will post grades to our Grading Roster on our Shared Google Drive one week after the Lab is due.\nSee the Lab Appeal Policy in our Syllabus if you are interested in having your Lab grade reviewed, and use the Lab Regrade Request form specified there to complete the task. Thank you."
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Course Calendar.\n\nWe charge a 5 point penalty for a lab that is 1-48 hours late.\nWe do not grade work that is more than 48 hours late.\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7. If you decide to skip a lab, please submit a note to Canvas by the deadline saying that you are skipping the lab.\n\n\n\n\nThere is a Lab 3 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 3, as it will make things easier for you and for the people grading your work.\n\nIn the Lab 3 template, we use the united theme. If you’d like to use a different theme, the available list is here.\n\n\n\n\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output.\n\n\n\n\nThe nh_1500 R data set is available for download on the 432 data page.\nA detailed description of each variable in the nh_1500 (and also the nh_3143) data is available here."
  },
  {
    "objectID": "lab3.html#template",
    "href": "lab3.html#template",
    "title": "Lab 3",
    "section": "",
    "text": "There is a Lab 3 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 3, as it will make things easier for you and for the people grading your work.\n\nIn the Lab 3 template, we use the united theme. If you’d like to use a different theme, the available list is here."
  },
  {
    "objectID": "lab3.html#our-best-advice",
    "href": "lab3.html#our-best-advice",
    "title": "Lab 3",
    "section": "",
    "text": "Review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "lab3.html#the-data",
    "href": "lab3.html#the-data",
    "title": "Lab 3",
    "section": "",
    "text": "The nh_1500 R data set is available for download on the 432 data page.\nA detailed description of each variable in the nh_1500 (and also the nh_3143) data is available here."
  },
  {
    "objectID": "lab3.html#be-sure-to-include-session-information",
    "href": "lab3.html#be-sure-to-include-session-information",
    "title": "Lab 3",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "lab3.html#after-the-lab",
    "href": "lab3.html#after-the-lab",
    "title": "Lab 3",
    "section": "After the Lab",
    "text": "After the Lab\n\nWe will post an answer sketch to our Shared Google Drive 48 hours after the Lab is due.\nWe will post grades to our Grading Roster on our Shared Google Drive one week after the Lab is due.\nSee the Lab Appeal Policy in our Syllabus if you are interested in having your Lab grade reviewed, and use the Lab Regrade Request form specified there to complete the task. Thank you."
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "Lab 5",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Course Calendar.\n\nWe charge a 5 point penalty for a lab that is 1-48 hours late.\nWe do not grade work that is more than 48 hours late.\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7. If you decide to skip a lab, please submit a note to Canvas by the deadline saying that you are skipping the lab.\n\n\n\n\nYou should be able to modify any of the first three Lab templates available on our 432-data page to help you do this Lab. Feel encouraged to try a different HTML theme if you like, maybe yeti or spacelab or materia.\n\n\n\n\n\n\nTip\n\n\n\nIn my answer sketch for Lab 5, I used the following R packages:\n\njanitor, naniar\nreadxl, survey, tableone\neasystats and tidyverse\n\nin case that is useful for you to know.\n\n\n\n\n\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output.\n\n\n\n\nThe hbp3024.xlsx Excel file (first introduced in Lab 2) and the nh_3143.Rds R data set (introduced in Lab 3) are available for download on the 432 data page.\nA detailed description of each variable in the hbp3024 data is available here.\nA detailed description of each variable in the nh_3143 data is available here."
  },
  {
    "objectID": "lab5.html#template",
    "href": "lab5.html#template",
    "title": "Lab 5",
    "section": "",
    "text": "You should be able to modify any of the first three Lab templates available on our 432-data page to help you do this Lab. Feel encouraged to try a different HTML theme if you like, maybe yeti or spacelab or materia.\n\n\n\n\n\n\nTip\n\n\n\nIn my answer sketch for Lab 5, I used the following R packages:\n\njanitor, naniar\nreadxl, survey, tableone\neasystats and tidyverse\n\nin case that is useful for you to know."
  },
  {
    "objectID": "lab5.html#our-best-advice",
    "href": "lab5.html#our-best-advice",
    "title": "Lab 5",
    "section": "",
    "text": "Review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "lab5.html#the-data",
    "href": "lab5.html#the-data",
    "title": "Lab 5",
    "section": "",
    "text": "The hbp3024.xlsx Excel file (first introduced in Lab 2) and the nh_3143.Rds R data set (introduced in Lab 3) are available for download on the 432 data page.\nA detailed description of each variable in the hbp3024 data is available here.\nA detailed description of each variable in the nh_3143 data is available here."
  },
  {
    "objectID": "lab5.html#hints-for-question-1",
    "href": "lab5.html#hints-for-question-1",
    "title": "Lab 5",
    "section": "Hints for Question 1",
    "text": "Hints for Question 1\n\nBe sure that your table specifies the number of subjects in each practice. Note that you’ll have to do something so that your work focuses on the comparison of Elm to King, leaving out the other practices.\nYou’ll have to deal with some missing values in the data. All missing values are indicated in the .xlsx file with NA, and you’ll need to specify that as you ingest the data in order to have them show up properly in R. It’s not usually appropriate to report results that include imputation in a Table 1, so build a note specifying the amount of missing data in a footnote to the table. An appropriate approach would be to produce a list just below your Table. Do not impute.\nSome variables will present as characters in the data, but you’d instead prefer them to appear as factors. Be sure to include code in your response to make these changes (the forcats package is your friend here) and then (perhaps using the fct_relevel function in the forcats package) be sure to move the levels of those factors into an order that facilitates interpretation.\nBe sure, too, to make reasoned choices about whether means and standard deviations or instead medians and quartiles are more appropriate displays for the quantitative variables. Include your reasons in a list displayed at the end of your table. Note that the record information is just a code (even though it is numerical) and should be treated as a character variable in using these data.\nNote that body mass index (BMI) and BMI category are not supplied in the data, although you do have height and weight. So, you’ll have to calculate the BMI and add it to the data set. If you don’t know the formula for BMI, you have Google to help you figure it out.\nFor BMI categories, use the four groups specified in the How is BMI interpreted for Adults section of this description of Adult BMI by the Centers for Disease Control. Again, you’ll need to use your calculated BMI values and then create the categories in your data set, and you’ll need to figure out a way to accurately get each subject into the correct category.\nDo not include R output without complete sentences describing what you are doing in each step, and what you conclude from that work."
  },
  {
    "objectID": "lab5.html#be-sure-to-include-session-information",
    "href": "lab5.html#be-sure-to-include-session-information",
    "title": "Lab 5",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "lab5.html#after-the-lab",
    "href": "lab5.html#after-the-lab",
    "title": "Lab 5",
    "section": "After the Lab",
    "text": "After the Lab\n\nWe will post an answer sketch to our Shared Google Drive 48 hours after the Lab is due.\nWe will post grades to our Grading Roster on our Shared Google Drive one week after the Lab is due.\nSee the Lab Appeal Policy in our Syllabus if you are interested in having your Lab grade reviewed, and use the Lab Regrade Request form specified there to complete the task. Thank you."
  },
  {
    "objectID": "lab7.html",
    "href": "lab7.html",
    "title": "Lab 7",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Course Calendar.\n\nWe charge a 5 point penalty for a lab that is 1-48 hours late.\nWe do not grade work that is more than 48 hours late.\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can skip exactly one of Labs 1-5 without penalty, but all students must complete both Lab 6 and Lab 7.\n\n\n\n\nYou should be able to modify the Lab 3 Quarto template available on our 432-data page to help you do this Lab.\n\n\n\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output.\n\n\n\n\nThe chr_2015.csv csv file (from Lab 1), hbp3024.xlsx Excel file (from Lab 2), nh_1500.Rds R data set (from Lab 3) and the remit48.sav SPSS file all appear on the 432 data page.\nA detailed codebook for all of the data in the chr_2024 file is available here.\nA detailed description of each variable in the hbp3024 data is available here.\nA detailed description of each variable in the nh_1500 data is available here.\nThe variables included in the remit48 data are described in Question 4, below."
  },
  {
    "objectID": "lab7.html#template",
    "href": "lab7.html#template",
    "title": "Lab 7",
    "section": "",
    "text": "You should be able to modify the Lab 3 Quarto template available on our 432-data page to help you do this Lab."
  },
  {
    "objectID": "lab7.html#our-best-advice",
    "href": "lab7.html#our-best-advice",
    "title": "Lab 7",
    "section": "",
    "text": "Review your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "lab7.html#the-data",
    "href": "lab7.html#the-data",
    "title": "Lab 7",
    "section": "",
    "text": "The chr_2015.csv csv file (from Lab 1), hbp3024.xlsx Excel file (from Lab 2), nh_1500.Rds R data set (from Lab 3) and the remit48.sav SPSS file all appear on the 432 data page.\nA detailed codebook for all of the data in the chr_2024 file is available here.\nA detailed description of each variable in the hbp3024 data is available here.\nA detailed description of each variable in the nh_1500 data is available here.\nThe variables included in the remit48 data are described in Question 4, below."
  },
  {
    "objectID": "lab7.html#be-sure-to-include-session-information",
    "href": "lab7.html#be-sure-to-include-session-information",
    "title": "Lab 7",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.4.2 (2024-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.9.0       cachem_1.1.0      cli_3.6.4        \n  compiler_4.4.2    digest_0.6.37     evaluate_1.0.3    fastmap_1.2.0    \n  fontawesome_0.5.3 fs_1.6.5          glue_1.8.0        graphics_4.4.2   \n  grDevices_4.4.2   highr_0.11        htmltools_0.5.8.1 htmlwidgets_1.6.4\n  jquerylib_0.1.4   jsonlite_1.9.0    knitr_1.49        lifecycle_1.0.4  \n  memoise_2.0.1     methods_4.4.2     mime_0.12         R6_2.6.1         \n  rappdirs_0.3.3    rlang_1.1.5       rmarkdown_2.29    rstudioapi_0.17.1\n  sass_0.4.9        stats_4.4.2       tinytex_0.55      tools_4.4.2      \n  utils_4.4.2       xfun_0.51         yaml_2.3.10"
  },
  {
    "objectID": "lab7.html#after-the-lab",
    "href": "lab7.html#after-the-lab",
    "title": "Lab 7",
    "section": "After the Lab",
    "text": "After the Lab\n\nWe will post an answer sketch to our Shared Google Drive 48 hours after the Lab is due.\nWe will post grades to our Grading Roster on our Shared Google Drive one week after the Lab is due.\nSee the Lab Appeal Policy in our Syllabus if you are interested in having your Lab grade reviewed, and use the Lab Regrade Request form specified there to complete the task. Thank you."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "432 Class Notes",
    "section": "",
    "text": "The 432 Class Notes were last updated on 2024-04-29 and are posted here."
  },
  {
    "objectID": "notes.html#errors-in-the-432-class-notes",
    "href": "notes.html#errors-in-the-432-class-notes",
    "title": "432 Class Notes",
    "section": "Errors in the 432 Class Notes",
    "text": "Errors in the 432 Class Notes\nIf you find any errors in the 432 notes, please submit them to Campuswire in the “Notes edits” folder.\n\nChapter 1\nIn section 1.12.1, below the miss_case_table() results, there is a bullet list describing the PHQ9_CAT which has a problem with the quotation marks in the first bullet, which should read:\n\nPHQ9 &gt; 20 means PHQ9_CAT is “severe”,\n\n\n\nChapter 4\nUnder the histogram in section 4.3.1, replace SED_F with SED_f in the sentence below so it reads:\n\nNow, we want to compare the Lowest SEDATE group (SED_f = [2, 200)) to the Highest SEDATE group (SED_f = [420, 1320]).\n\n\n\nChapter 7\nIn section 7.8, there is a typo (the word mode instead of more) in the opening paragraph. It should read:\n\nMultiple imputation, where NA values are repeatedly estimated/replaced with multiple data values, for the purpose of obtaining more complete samples and capturing details of the variation inherent in the fact that the data have missingness, so as to obtain more accurate estimates than are possible with single imputation.\n\nThank you for your patience."
  },
  {
    "objectID": "projB.html",
    "href": "projB.html",
    "title": "Project B",
    "section": "",
    "text": "In project B, you will create two research questions that can be addressed with data you can conveniently obtain that can be used to address each of your questions, with models that you learned in the 431-432 sequence.\nYou will fit one regression model for each research question.\n\nEach of your models must include 2-8 predictors.\nEach model must include at least one predictor that is not included in the other model.\nEach model will need to describe observations from the same pool of “subjects”, so that a single tibble services the whole Project.\n\nThe model for your first outcome must be either:\n\nA model for a multi-categorical outcome (with 3-7 levels)\nA model for a count outcome, or\nA Cox model for a time-to-event outcome with censoring\n\nThe model for your second outcome must use a different approach than you used for Outcome 1. For this model, you can use any of the three options above, or you may use:\n\nA linear or binary logistic model fit using a Bayesian engine, or\nA linear or binary logistic model fit by selecting candidate predictors using a best subsets procedure, or\nA linear regression model that uses survey weights.\n\nNote: Use at least 3 predictors (and still no more than 8) if you are fitting a linear or binary logistic model for your second outcome, please.\n\n\nThere are three deliverables in Project B: a Proposal Form, an in-person or Zoom Presentation, a Portfolio developed with Quarto (along with your Data).\n\nProposal Form By the deadline on the Calendar in early April, you will submit a Google Form that Dr. Love will make available at https://bit.ly/432-2025-projB-proposal.\n\nIn this form, you will specify your title, your research questions, and your data source, and whether or not you are working with a partner.\nYou will also provide a brief description of each of your outcomes, and the type of models you plan to fit.\nFinally, you will also specify the times that work for you (and your partner) to give your Project B presentation to Professor Love.\nWe will need to approve your Proposal Form before you proceed further, and this may require some revisions to your plan.\n\nPresentation After receiving our approval of your submitted proposal form, you will prepare a portfolio of your results with Quarto (a template is available for this), and then present your work to Professor Love. The schedule for these presentations will be determined after completion of the Project B Proposal Form.\nPortfolio and Data You will then make changes (as needed) to your Quarto document (portfolio) in response to Professor Love’s comments during your presentation, and submit the final version of the portfolio along with a version of your data (as described here) to Canvas by the deadline in the Calendar.\n\n\n\nYou can choose either to work alone, or with one other person, to complete Project B, unless Professor Love has specifically requested that you work alone. He will make those requests after Project A’s grading is complete.\nIf you are working with a partner…\n\nexactly one of you needs to complete the Project B Proposal Form (the other student should send an email to Dr. Love with the subject line 432 Project B Partnership confirming that you are working in a partnership and telling me the name of your partner), then\nyou will each participate in the Project B Presentation and the development of the Portfolio, and\n\nYour final Portfolio and Data will be submitted by exactly one of the two partners to Canvas while the non-reporting partner will submit a one-page note to Canvas indicating the members of the partnership and that the partner will submit the work.\n\n\n\n\nIf you decide to use some sort of AI to help you with any part of the Project, we ask that you place a note to that effect, describing what you used and how you used it, as a separate section called “Use of AI”. This should appear just before your section containing the Session Information. Thank you."
  },
  {
    "objectID": "projB.html#deliverables",
    "href": "projB.html#deliverables",
    "title": "Project B",
    "section": "",
    "text": "There are three deliverables in Project B: a Proposal Form, an in-person or Zoom Presentation, a Portfolio developed with Quarto (along with your Data).\n\nProposal Form By the deadline on the Calendar in early April, you will submit a Google Form that Dr. Love will make available at https://bit.ly/432-2025-projB-proposal.\n\nIn this form, you will specify your title, your research questions, and your data source, and whether or not you are working with a partner.\nYou will also provide a brief description of each of your outcomes, and the type of models you plan to fit.\nFinally, you will also specify the times that work for you (and your partner) to give your Project B presentation to Professor Love.\nWe will need to approve your Proposal Form before you proceed further, and this may require some revisions to your plan.\n\nPresentation After receiving our approval of your submitted proposal form, you will prepare a portfolio of your results with Quarto (a template is available for this), and then present your work to Professor Love. The schedule for these presentations will be determined after completion of the Project B Proposal Form.\nPortfolio and Data You will then make changes (as needed) to your Quarto document (portfolio) in response to Professor Love’s comments during your presentation, and submit the final version of the portfolio along with a version of your data (as described here) to Canvas by the deadline in the Calendar.\n\n\n\nYou can choose either to work alone, or with one other person, to complete Project B, unless Professor Love has specifically requested that you work alone. He will make those requests after Project A’s grading is complete.\nIf you are working with a partner…\n\nexactly one of you needs to complete the Project B Proposal Form (the other student should send an email to Dr. Love with the subject line 432 Project B Partnership confirming that you are working in a partnership and telling me the name of your partner), then\nyou will each participate in the Project B Presentation and the development of the Portfolio, and\n\nYour final Portfolio and Data will be submitted by exactly one of the two partners to Canvas while the non-reporting partner will submit a one-page note to Canvas indicating the members of the partnership and that the partner will submit the work."
  },
  {
    "objectID": "projB.html#use-of-ai",
    "href": "projB.html#use-of-ai",
    "title": "Project B",
    "section": "",
    "text": "If you decide to use some sort of AI to help you with any part of the Project, we ask that you place a note to that effect, describing what you used and how you used it, as a separate section called “Use of AI”. This should appear just before your section containing the Session Information. Thank you."
  },
  {
    "objectID": "projB.html#on-research-questions",
    "href": "projB.html#on-research-questions",
    "title": "Project B",
    "section": "On Research Questions",
    "text": "On Research Questions\nProject B requires you to answer two research questions with data you obtain. You can study any question you like, although I’d steer clear of anything that you think Professor Love might find inappropriate. All of the advice from Project A on this subject still holds. Each of your research questions needs to lead clearly to a modeling strategy, where you’ll fit an appropriate regression model."
  },
  {
    "objectID": "projB.html#on-selecting-data",
    "href": "projB.html#on-selecting-data",
    "title": "Project B",
    "section": "On Selecting Data",
    "text": "On Selecting Data\nWe hope that most people will find datasets of interest to them and use those. If you do not have any strong ideas for data to use, then we encourage you to consider the suggestions from 432 Project A. The best projects from my perspective use interesting and appropriate data that we have never seen before.\n\nYou must completely identify the source(s) of the data, so that Professor Love understands what data you are using very well, but you will not be required to share the data with him, or anyone else, if the data you use are not already available to the public. You must ensure that you have all necessary permission(s) to use the data for this course project.\nEach of your models must work with data drawn from the same tibble, and describe observations from the same pool of “subjects”.\nWe prefer that data sets contain between 200 and 2000 complete observations on each outcome, but you can use more than 2000 or as few as 150, if it’s important to do so. Projects with less than 150 subjects with complete data on all variables are not acceptable for Project B.\nYour tibble will need a subject identifying code, and each model must have a separate outcome, and at least one predictor not used in the other model and each model must have 2-8 predictors, we anticipate that your clean, tidied tibble will include between 6 and 19 variables.\nWe suggest that you collapse multiple categories as necessary to ensure that you have at least 25 observations in every category you plan to use for every predictor or outcome studied in Project B.\n\nUnlike 432 Project A, NHANES data are a completely acceptable choice for Project B, although we do require you to use the demographics file and at least two other NHANES files in developing your data set. If you use NHANES data or other large (weighted) survey data, you are welcome to do any or all of Project B without incorporating sampling weights.\nNote: We have had some difficulty identifying data set suggestions (outside of educational repositories like the UCI Machine Learning Repository) that are both messy enough to be interesting for this project and which also provide time-to-event outcomes with right censoring that are suitable for the survival analysis work we discuss in 432. If you have suggestions along these lines, we would be eager to hear them.\n\nThere are only a few restrictions on your choice of data.\n\nYou are not allowed to use data stored as a data set in any R package other than NHANES or Tidy Tuesday data.\nYou are not allowed to use data Professor Love or anyone else has provided to you for teaching purposes.\nYou cannot reuse the data you used in Project A for 432, although you can use a different data set to answer related questions. You are welcome to reuse data you used in your 431 project work if it is suitable and you haven’t used it in Project A for 432.\n\n\n\nNo multi-level (hierarchical) data\nWe want to powerfully discourage you from working with nested data that really require the use of multi-level models. The same rules set forward in Project A apply here, as well."
  },
  {
    "objectID": "projB.html#what-information-does-the-form-require",
    "href": "projB.html#what-information-does-the-form-require",
    "title": "Project B",
    "section": "What information does the form require?",
    "text": "What information does the form require?\n\nYou’ll need to assert that you have already successfully ingested the data into R.\nYou’ll need to tell us the two types of models you plan to fit.\nYou’ll need to describe the variables you intend to use as outcomes in your models.\n\nThese two outcomes need to be clearly linked to your research questions, and you’ll need to tell us something about each of their distributions.\n\nYou’ll need to specify how many cases are in your data with complete data on each of your study’s two outcomes.\nYou’ll need to specify the title of your project.\nYou’ll need to specify your research questions.\nYou’ll need to specify who your partner is, if you have one. If you are working with a partner, exactly one of you will fill out the form. The other student should send an email to Dr. Love with the subject line 432 Project B Partnership confirming that you are working in a partnership and telling me the name of your partner.\nYou’ll need to specify several dates and times on which you can give your presentation, from a list provided in the form. You’ll have the opportunity to tell us about special circumstances, and to express a “first choice” preference as part of the form."
  },
  {
    "objectID": "projB.html#scheduling-your-presentation",
    "href": "projB.html#scheduling-your-presentation",
    "title": "Project B",
    "section": "Scheduling Your Presentation",
    "text": "Scheduling Your Presentation\nOnce Dr. Love has received and approved all Project B Proposal Forms, he will schedule the presentations, based on your requests. All presentations will be scheduled within the following time windows:\n\nThursday 2025-04-17 between 3 and 6 PM (in person presentations)\nFriday 2025-04-18 between 10 AM and 2 PM (zoom presentations)\nMonday 2025-04-21 between 8:30 AM and Noon, as well as 1 - 6:30 PM (zoom presentations)\nTuesday 2025-04-22 between 8:30 AM and Noon, as well as 1 - 6:30 PM (zoom presentations) note that we don’t have class on 2025-04-22.\nWednesday 2025-04-23 between 8:30 AM and 11 AM, and 12:30 PM to 2 PM (zoom presentations)\n\nIn making your decisions about availability, please note the following:\n\nPassover is 2025-04-12 through 2025-04-20.\nEaster Sunday is Sunday 2025-04-20.\nMy Project B Ask Me Anything class (over Zoom) will be on Tuesday 2025-04-15.\n432 Quiz 2 will be made available to you on Friday 2025-04-18 and is due on Friday 2025-04-25 at Noon.\nWe won’t hold 432 class on Tuesday 2025-04-22.\nThe final 432 class will be held (in person) on Thursday 2025-04-24.\nAt CWRU, classes for Spring Semester end on Monday 2025-04-28, and the University Reading Days are 2025-04-29 and 2025-04-30."
  },
  {
    "objectID": "projB.html#how-will-we-respond-to-the-form",
    "href": "projB.html#how-will-we-respond-to-the-form",
    "title": "Project B",
    "section": "How will we respond to the form?",
    "text": "How will we respond to the form?\nOn the basis of your responses to the Proposal Form, we will either approve or reject your Project B plan, and once it is approved, you can proceed.\n\nIf we cannot approve your project, we’ll tell you why, and tell you how to try again. You’ll need to iterate until we approve your project, but we hope this won’t require more than two tries for anyone.\nThe main reason why we doesn’t approve projects is that we don’t understand the description of your data set or its outcomes, your planned modeling approaches, or how the research questions link to the available data, so focus on making those connections as clear as possible.\nTAs are available during their office hours to review your planned research questions, data set and planned variables with you to make suggestions.\n\nOnce all forms are in, Professor Love will also schedule the presentations, and this information will be announced in class and posted as quickly as possible."
  },
  {
    "objectID": "projB.html#making-changes-to-your-proposal",
    "href": "projB.html#making-changes-to-your-proposal",
    "title": "Project B",
    "section": "Making Changes to your Proposal",
    "text": "Making Changes to your Proposal\nNo plan survives first contact with the data unscathed. You will wind up making changes as your work on the Project progresses. Once we approve your proposed research questions, you should feel to free to make any change you like, so long as you are not materially changing the general topic of your work. If you feel that you need to change the title or one of the outcomes in your project, that’s the time to run the changes past Professor Love in an email, but if the original title and outcomes still work, you’ll be fine.\n\nIf you do need to change a title, request a re-approval via email by submitting both the initial title and research questions as well as the new ones you now propose, accompanied by a brief description of what needs to change and why you need to make the change.\nProfessor Love will consider changes of this sort via email he receives more than 72 hours prior to your scheduled project presentation. After that, your title is locked in.\nYou do not need permission to adjust your choice of variables, or your strategy for including subjects or anything else that wouldn’t change the main goals of your project or your title."
  },
  {
    "objectID": "projB.html#p-values-and-statistical-significance",
    "href": "projB.html#p-values-and-statistical-significance",
    "title": "Project B",
    "section": "P values and “statistical significance”",
    "text": "P values and “statistical significance”\nAs in Project A, you are welcome to include p values in your analyses in either the portfolio or the presentation, but you should demonstrate good statistical practice by not comparing them to \\(\\alpha\\) levels to declare things to be important, meaningful, or significant.\nYou should not use the words “statistically significant” or any synonyms (like statistically detectable) in any of your materials or in your presentation."
  },
  {
    "objectID": "projB.html#template-for-the-portfolio",
    "href": "projB.html#template-for-the-portfolio",
    "title": "Project B",
    "section": "Template for the Portfolio",
    "text": "Template for the Portfolio\nProfessor Love built a sample template for the Project B Portfolio, and we strongly encourage you to use it. This template is posted to our 432-data site.\nAny alternate template or formatting style is acceptable if it is built using Quarto and contains all of the section headings in Professor Love’s template. The list of HTML “themes” that are available in Quarto by changing the “theme” option in the start of your document can be found here and we encourage you to pick something you think looks nice."
  },
  {
    "objectID": "projB.html#what-are-the-main-section-headings-in-the-portfolio",
    "href": "projB.html#what-are-the-main-section-headings-in-the-portfolio",
    "title": "Project B",
    "section": "What are the main section headings in the Portfolio?",
    "text": "What are the main section headings in the Portfolio?\nThe Template begins with an unnumbered section for R Packages and Setup. The numbered sections we want you to use are:\n\nBackground\nResearch Questions\nData Ingest and Management\nCode Book and Description\nAnalyses\nConclusions and Discussion\nReferences and Acknowledgments\nSession Information\n\nMake your portfolio gorgeous, thoughtful and incredibly easy for Professor Love to use in evaluating your work.\n\nJeff Leek’s material in How To Be a Modern Scientist is very useful here, in particular the material on Scientific Talks and on Paper Writing. We especially like the advice to write clearly and simply.\nInclude all R code and output that you need to help Professor Love understand the important issues in your study.\n\nAdding more material, just for the sake of demonstrating that you can, is a good way for me to be unimpressed with your project. If you cannot think of anything to say about a piece of output easily, why are you including it?\n\nCleaning the data is a vitally important step. Professor Love will assume that you have done it perfectly. The TAs can help you, but this is your responsibility.\nYour cleaning should use tools from the tidyverse when possible, and you should do all analytic work on tibbles, whenever possible.\nDon’t include warnings or messages from R that we don’t need to know about. Use and trust your judgment.\nNever show long versions of output when short ones will do. A particularly good idea is to print a tibble rather than showing an entire data set."
  },
  {
    "objectID": "projB.html#our-best-advice",
    "href": "projB.html#our-best-advice",
    "title": "Project B",
    "section": "Our Best Advice",
    "text": "Our Best Advice\nReview your HTML output file carefully before submission for copy-editing issues (spelling, grammar and syntax.) Even with spell-check in RStudio (just hit F7), it’s hard to find errors with these issues in your Quarto file so long as it is running. You really need to look closely at the resulting HTML output."
  },
  {
    "objectID": "projB.html#submitting-your-data",
    "href": "projB.html#submitting-your-data",
    "title": "Project B",
    "section": "Submitting Your Data",
    "text": "Submitting Your Data\nIf your data are available to the public, including Professor Love, then submit (along with your final portfolio):\n\nwhatever Professor Love needs to provide to the Quarto file to ingest your data (this is most commonly a .csv file or series of them, but you might instead pull directly from the internet, which is also fine.)\na tidied version of the data set, saved as .Rds, and matching precisely what you describe in your codebook.\nif the data are available online, you must provide within your HTML file a working URL with instructions to access the data.\n\nIf your data are not available to the public, then submit (again, with your Portfolio):\n\nan .Rds or .csv file consisting of five rows of your data, with all variables included in your codebook provided, and with different values of each of the variables displayed.\nThe five rows can be five actual rows from your data set, or each row can take parts of several different actual rows in your data to hide details and prevent re-identification.\nNo protected health information may be included in the five rows of data you submit, nor can any protected information be contained in the actual analytic data set(s) you use in your work."
  },
  {
    "objectID": "projB.html#missing-data",
    "href": "projB.html#missing-data",
    "title": "Project B",
    "section": "Missing Data",
    "text": "Missing Data\n\nDrop all cases without complete outcome data, but otherwise, your goal should be to preserve as much of the data collection effort as possible.\nUse multiple imputation to deal with missing values in presenting a final model for either a linear or logistic regression whenever possible.\nBe sure to explicitly state the assumptions you make about the missing data mechanism.\nYou may use single imputation in the process of developing your models, in presenting residual plots for linear models, or in presenting final models for regression approaches for count, multi-categorical or time-to-event with censoring outcomes.\nDo not use a complete-case analysis or sampling strategy except to ensure that all of your cases have complete outcome data."
  },
  {
    "objectID": "projB.html#model-size",
    "href": "projB.html#model-size",
    "title": "Project B",
    "section": "Model Size",
    "text": "Model Size\n\nWhile you can fit a larger model, we strongly recommend you restrain yourself to no more than 8 predictors in a final model regardless of your sample size. Anything more than that will be difficult to interpret at best.\nIf your number of main effects (predictors) that you want to include in your final model exceeds the number of degrees of freedom specified below, then don’t add any non-linear terms. If you do decide to include non-linear terms as determined based on a Spearman rho-squared plot, then adhere closely to the maximum degrees of freedom specified in the table below. These df limits include the intercept term(s).\n\n\nIf you are fitting a regression to a quantitative or count outcome, let n = sample size. For this count and all of the counts here, do not include any data points where the outcome is missing.\nIf you are fitting a regression to a categorical outcome, let n = # of observations in the category with the smallest sample size.\nIf you are fitting a regression to a time-to-event outcome, let n = # of observations where the event occurred (was not censored).\n\n\n\n\nValue of n\n20-100\n101-250\n251-500\n501-999\n1000+\n\n\n\n\nMaximum df\n6\n9\n12\n16\n20\n\n\n\nFor Project B, don’t worry about penalizing yourself for “peeking” at the data by running automated selection procedures (although we don’t generally recommend these) or scatterplot matrices."
  },
  {
    "objectID": "projB.html#transformations",
    "href": "projB.html#transformations",
    "title": "Project B",
    "section": "Transformations",
    "text": "Transformations\nIn Project B, for outcome transformations related to quantitative outcomes (if you are fitting such a model), follow our advice from Project A."
  },
  {
    "objectID": "projB.html#advice-on-predictors",
    "href": "projB.html#advice-on-predictors",
    "title": "Project B",
    "section": "Advice on Predictors",
    "text": "Advice on Predictors\n\nCollapse multi-categorical predictors sufficiently that you don’t have problems fitting or interpreting the model. In most cases, it is very difficult to describe what’s happening with predictors containing more than five levels.\nIf you choose to use a spline or polynomial function with a quantitative predictor and want also to use an interaction term for that predictor, be sure to restrict the interaction to a linear term only with %ia%.\nMake sure that the coefficients and standard errors in your models don’t explode, which can happen when a predictor overwhelms the regression model. It’s your job to identify that there is a problem and do something to address it appropriately, rather than presenting a clearly inappropriate model."
  },
  {
    "objectID": "projB.html#on-validation",
    "href": "projB.html#on-validation",
    "title": "Project B",
    "section": "On Validation",
    "text": "On Validation\n\nWe expect a validation, properly executed, to be an important part of every Project B.\nThis will most commonly include a split into training and testing samples, and an evaluation of key results in a held-out sample. Alternatively, you might choose an evaluation based on cross-validation, in which case we recommend 5-fold cross validation for this project.\nBootstrap validation of summary statistics via the tools available in the rms package is also welcome, if you are fitting models that use those tools."
  },
  {
    "objectID": "projB.html#questions-about-your-project-b-work-in-general",
    "href": "projB.html#questions-about-your-project-b-work-in-general",
    "title": "Project B",
    "section": "8 questions about your Project B work in general",
    "text": "8 questions about your Project B work in general\n\nDoes the background motivate your research questions?\nAre you research questions clear and appropriately linked to the regression analyses you are doing?\nHow much data do you have, and what is the source of that data?\nHow did you deal with missing data, and what assumptions about missingness did you make?\nIs your portfolio well-labeled and easy to find things in?\nDoes your presentation focus on important issues? See the list in the instructions.\nHave you avoided discussing statistical significance or any synonyms for that problematic term?\nCan you answer questions effectively about the work you’re presenting?\n\nOf course, I’ll also want to know if you have questions for me about things related to the Project."
  },
  {
    "objectID": "projB.html#questions-about-a-model-for-a-time-to-event-outcome-with-censoring",
    "href": "projB.html#questions-about-a-model-for-a-time-to-event-outcome-with-censoring",
    "title": "Project B",
    "section": "8 questions about a model for a time-to-event outcome with censoring",
    "text": "8 questions about a model for a time-to-event outcome with censoring\n\nWhat is the outcome of interest, and what is the variable describing the time, and the variable describing the “event”?\nHow many subjects are in your data, and how many of them have the actual “event”?\nWhat does it mean for someone to be censored in your data?\nWhat does a Kaplan-Meier curve describing survival across all of your outcomes, or perhaps comparing two or three key groups, tell us about your research question?\nWhy did you select the combination of predictors that you did for this model?\nHave you provided an appropriate summary of the effects in your Cox model, and can you explain an effect to me that I select at random from your model appropriately without talking about statistical significance?\nOverall, how well does your model fit, as judged by the unvalidated concordance, C statistic, and Cox-Snell R-square measure?\nWhat does your assessment of the proportional hazards assumption suggest about your model?"
  },
  {
    "objectID": "projB.html#questions-about-a-model-for-a-count-outcome",
    "href": "projB.html#questions-about-a-model-for-a-count-outcome",
    "title": "Project B",
    "section": "8 questions about a model for a count outcome",
    "text": "8 questions about a model for a count outcome\n\nWhat is the outcome of interest, and is this in fact a count outcome?\nHow many subjects are in your data, and what does the distribution of your count outcome look like in those subjects?\nHow did you split the data into testing and training samples, which you should do with a count outcome, following Slides Set 15.\nWhy did you select the combination of predictors that you did for this model?\nWhich model(s) did you fit to your data? You should plan to fit at least two of a Poisson, a Negative Binomial and/or a zero-inflated model or hurdle model.\nWhy did you settle on the choice of count model you did? A comparison of rootograms is the most desirable way to show me how you made this choice.\nHave you provided an appropriate summary of the effects in your model, and can you explain an effect to me that I select at random from your model appropriately without talking about statistical significance?\nHow well does your model fit, as judged by training then test sample summaries of R-square and the root mean squared error? Note: I don’t care about Vuong’s test."
  },
  {
    "objectID": "projB.html#questions-about-a-model-for-a-multi-categorical-outcome-with-3-7-levels",
    "href": "projB.html#questions-about-a-model-for-a-multi-categorical-outcome-with-3-7-levels",
    "title": "Project B",
    "section": "8 questions about a model for a multi-categorical outcome with 3-7 levels",
    "text": "8 questions about a model for a multi-categorical outcome with 3-7 levels\n\nWhat is the outcome of interest, and is it an ordinal or nominal multi-categorical outcome? Did you do any collapsing of levels? Why? How many subjects are in your data overall, and within each level of your outcome?\nWhy did you select the combination of predictors that you did for this model?\nWhat do the predictions from your model look like, when tabulated (in the rows) against the true results from your outcome (in the columns)? What does that table tell us about the accuracy of your predictions?\nUse your model to make predictions for two new subjects, and use those predictions to discuss the impact of an interesting change in one of your more important predictors.\n\nand four additional questions, depending on whether you have an ordinal or nominal outcome…\nIf you have an ordinal outcome, then I’d additionally ask:\n\nHow are you making certain that R recognizes your outcome as an ordered factor?\nCan you display and explain the ggplot(Predict(yourmodel)) result from your POLR model (i.e. a lrm() model for a proportional outcome)?\nHow well does the POLR model fit, as judged by validated R-square and C statistics obtained through fitting with lrm()?\nWhich model did you wind up preferring for your data (POLR or multinomial) and why? (A likelihood ratio test along with AIC and BIC comparisons is a good option here.)\n\nIf you have a nominal outcome, then I would additionally ask:\n\nHow do you know that your multinomial model converged?\nHow many intercepts and how many coefficients (slopes) are in your model? Do these values make sense in light of your outcome and predictors?\nCan you display and explain the meaning of a plot of response probabilities describing the relationship between your outcome and one of your interesting categorical predictors?\nCan you compare your model (which should have at least 2 predictors) to another model for your outcome that includes only one of your predictors (it’s up to you to decide what one predictor would be most interesting to study) using AIC, BIC and effective degrees of freedom, so that you can then tell us which of those two models looks best by both AIC and BIC?"
  },
  {
    "objectID": "projB.html#questions-about-a-model-for-a-quantitative-outcome",
    "href": "projB.html#questions-about-a-model-for-a-quantitative-outcome",
    "title": "Project B",
    "section": "8 questions about a model for a quantitative outcome",
    "text": "8 questions about a model for a quantitative outcome\n\nHave you made a reasonable choice of strategy here? (Bayesian model, Selecting Candidates via best subsets, using survey weights?) Why did you select this approach - how does it tie in with your research question?\nCan you demonstrate that you have selected a transformation (or not) for the outcome wisely?\nCan you demonstrate that you have fit the model properly, and evaluated it effectively using the tools we have developed in class?\nCan you demonstrate whether your model fits well, effectively using the tools available for you for the type of model you’re developing?\nHave you provided an appropriate summary of the effects in your model, and can you explain an effect to me that I select at random from your model appropriately without talking about statistical significance?\nCan you make predictions for new data using your model?\nCan you compare the performance of your model in a sensible way to an ordinary least squares linear model?\nCan you evaluate the assumptions that are relevant to your model?"
  },
  {
    "objectID": "projB.html#questions-about-a-model-for-a-binary-outcome",
    "href": "projB.html#questions-about-a-model-for-a-binary-outcome",
    "title": "Project B",
    "section": "8 questions about a model for a binary outcome",
    "text": "8 questions about a model for a binary outcome\n\nHave you made a reasonable choice of strategy here? Why did you select this approach, and how does it tie in with your research question?\nHow did you make certain that your binary outcome was being modeled in the direction you intend it to be modeled (i.e. the Probability of A happening, rather than the Probability of A not happening?)\nCan you demonstrate that you have fit the model properly, and evaluated it effectively using the tools we have developed in class?\nCan you demonstrate whether your model fits well, effectively using the tools available for you for the type of model you’re developing?\nHave you provided an appropriate summary of the effects in your model, and can you explain an effect to me that I select at random from your model appropriately without talking about statistical significance?\nCan you make predictions for new data using your model?\nCan you compare the performance of your model in a sensible way to an ordinary logistic regression model?\nCan you evaluate the assumptions that are relevant to your model?"
  },
  {
    "objectID": "quiz2.html",
    "href": "quiz2.html",
    "title": "Quiz 2",
    "section": "",
    "text": "Quiz 2 covers materials from Classes 1-24 of the course, as well as Jeff Leek’s How to be a modern scientist, and other sources assigned during classes 1-24.\nBy the time specified on the Calendar, several elements related to Quiz 2 will be provided to you.\n\nThe main document (pdf) for Quiz 2, which will include both instructions and all of the questions.\nA link to the Google Form Answer Sheet for Quiz 2, where you will submit your responses.\nDownload information for the Data Sets we will provide for Quiz 2.\n\nQuiz 2 will be due when the Calendar says it is.\nIf you have a question prior to the release of Quiz 2 about the Quiz, send it by email to Dr. Love at thomas dot love at case dot edu."
  }
]